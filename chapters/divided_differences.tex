\chapter{Divided differences}

\section{Motivation}
Divided differences are derivatives without limits.

Consider a function $f : \R \to \R$. Its (first) divided difference is defined as
\begin{align*}
	[\cdot , \cdot]_{f} : \R^{2} \setminus \{x \neq y\} &\to \R \\
	[x, y]_{f} &= \frac{f(x) - f(y)}{x - y}.
\end{align*}

If $f$ is sufficiently smooth, we should also define $[x, x]_{f} = f'(x)$: if $f \in C^{1}(\R)$, this gives continuous extension to $[\cdot, \cdot]_{f}$. Much of the power of divided differences comes however from the fact that they conveniently carry same information even if we do not do such extension.

Consider the case of increasing $f$. This information is exactly carried by the inequality $[x, y]_{f} \geq 0$. Again, if $f$ is differentiable, this is equivalent to $f'(x) = [x, x]_{f} \geq 0$. There are many ways to see this fact, one of the more standard being the mean value theorem: If $x \neq y$, for some $\xi$ between $x$ and $y$ we have
\[
	\frac{f(x) - f(y)}{x - y} = f'(\xi).
\]
Now if the derivative is everywhere non-negative, so are divided differences. Also divided differences are sort of approximations for derivative.

We can push these ideas to higher orders. Second order divided differences should be something that captures second order behaviour of a function. In particular, if $f \in C^{2}(\R)$ has non-negative second derivative everywhere, i.e. it is convex, its second divided difference should be non-negative, and vice versa. Standard definition of convexity is almost what we are looking for: $f$ is convex if for any $x, y \in \R$ and $0 \leq t \leq 1$ we have $t f(x) + (1 - t) f(y) \geq f(t x + (1 - t)y)$. So if we define the mapping $[\cdot, \cdot, \cdot]_{f} : \R^{2} \times [0, 1]$ by $t f(x) + (1 - t) f(y) - f(t x + (1 - t)y)$, we have $[x, y, t]_{f} \geq 0$ for any $(x, y, t) \in \R^{2} \times [0, 1]$ if and only if $f^{2}(x) \geq 0$ for any $x \in \R$.

There is however much better version for the function. If we write $z = t x + (1 - t) y$, we can solve that $t = \frac{z - y}{x - y}$ and express
\begin{eqnarray*}
	[x, y, t]_{f} &=& t f(x) + (1 - t) f(y) - f(t x + (1 - t)y) \\
	&=& \frac{z - y}{x - y} f(x) + \frac{x - z}{x - y} f(y) - f(z) \\
	&=& -(z - y)(z - x) \left(\frac{f(x)}{(x - y)(x - z)} + \frac{f(y)}{(y - z)(y - x)} + \frac{f(z)}{(z - x)(z - y)} \right)
\end{eqnarray*}

If $t \notin \{0, 1\}$, $-(z - y)(z - x)$ is positive, so if $f$ is convex,
\[
	\frac{f(x)}{(x - y)(x - z)} + \frac{f(y)}{(y - z)(y - x)} + \frac{f(z)}{(z - x)(z - y)} \geq 0
\]
for any $x, y$ and $z$ such that $z$ is between $x$ and $y$. This is new expression is symmetric in its variables, so actually there's no need to assume anything on the fo $x, y$ and $z$, just that they're distinct. We can also easily carry this argument to the other direction. If this expression is non-negative any distinct $x, y$ and $z$, $f$ is convex. This motivates us to crap the previous definition and set instead
\[
	[x, y, z]_{f} := \frac{f(x)}{(x - y)(x - z)} + \frac{f(y)}{(y - z)(y - x)} + \frac{f(z)}{(z - x)(z - y)}.
\]

One would naturally except that by setting
\[ 
	[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \sum_{i = 0}^{n} \frac{f(x_{i})}{\prod_{j \neq i} (x_{i} - x_{j})},
\]
one obtains something that naturally generalizes divided differences for higher orders. This is indeed the case.

\section{Basic properties}

For $n \geq 1$ define $D_{n} = \{x\in \R^{n} | x_{i} = x_{j} \text{ for some $1 \leq i \leq n$}\}$.
\begin{maar}
Let $n \geq 0$. For any real function $f : (a, b) \to \R$ we define the corresponding $n$'th divided difference $[\cdots]_{f} : (a, b)^{n + 1} \setminus D_{n + 1}$ by setting
\[
	[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \sum_{i = 0}^{n} \frac{f(x_{i})}{\prod_{j \neq i} (x_{i} - x_{j})}.
\]
\end{maar}

As with the first order divided differences, if we can continuously extend divided differences to the set $D_{n + 1}$, we should do that, and we identify the resulting function with the original one.

Albeit a rather direct generalization for the cases $n = 1$ and $n = 2$, We defined divided differences only for real valued functions, but codomain could just as well any real or complex vector space. it's not very clear why such definition should correspond to anything useful. We have however the following important properties.

\begin{prop}
	Divided differences are symmetric in the variable and linear in the function, i.e. for any $\alpha, \beta \in \R$, $f, g : (a, b) \to \R$ and $0 < x_{0}, x_{1}, \ldots, x_{n} < b$ we have
	\[
		[x_{0}, x_{1}, \ldots, x_{n}]_{\alpha f + \beta g} = \alpha [x_{0}, x_{1}, \ldots, x_{n}]_{f} + \beta [x_{0}, x_{1}, \ldots, x_{n}]_{g}.
	\]
	In addition, divided differences can be calculated recursively as
	\[
		[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \frac{[x_{0}, x_{1}, \ldots, x_{n - 1}]_{f} - [x_{1}, \ldots, x_{n}]_{f}}{x_{0} - x_{n}}.
	\]
\end{prop}
\begin{proof}
	Easy to check.
\end{proof}

Also, we have following classic characterization.

\begin{prop}
We have $[x_{0}, x_{1}, \ldots, x_{n}]_{(x \mapsto x^{n})} = 1$ and $[x_{0}, x_{1}, \ldots, x_{n}]_{p} = 0$ for any polynomial of degree at most $n - 1$. In other words, $[x_{0}, x_{1}, \ldots, x_{n}]_{f}$ is the leading coefficient of the Lagrange interpolation polynomial on pairs $(x_{0}, f(x_{0})), (x_{1}, f(x_{1}), \ldots, (x_{n}, f(x_{n}))$.
\end{prop}
\begin{proof}
	Claims are easily derived from each other since if $f$ is itself a polynomial of degree at most $n$, its lagrange interpolation polynomial is $f$ itself. Recall that the Lagrange interpolation polynomial of a dataset $(x_{0}, y_{0}), (x_{1}, y_{1}), \ldots, (x_{n}, y_{n})$ is given by
	\[
		\sum_{i = 0}^{n} y_{i} \frac{\prod_{j \neq i}(x - x_{j})}{\prod_{j \neq i}(x_{i} - x_{j})},
	\]
	and in our case the leading coefficient of this polynomial leads exactly to our definition for the divided differences.
\end{proof}

These observation already partially justify the terminology: as higher order derivatives are defined recursively using (first order) derivatives, higher order divided differences can be calculated recursively using (the usual) divided differences.

The most important property of the divided differences is the following.

\begin{lause}[Mean value theorem for divided differences]
	Let $n \geq 1$ and $f \in C^{n}(a, b)$. Then for any $x_{0}, x_{1}, \ldots, x_{n}$ we have $\min_{0 \leq i \leq n}(x_{i}) \leq \xi \leq \max_{0 \leq i \leq n}(x_{i})$ such that
	\[
		[x_{0}, x_{1}, \ldots, x_{n}] = \frac{f^{(n)}(\xi)}{n!}.
	\]
\end{lause}
\begin{proof}
	We prove the statement assuming additionally that the divided differences define a continuous function on the whole set $(a, b)^{n + 1}$: this will proven later. Note that if one manages to prove the statement for distinct points, one may take sequence of tuples of distinct points, $((x_{i}^{(j)})_{i = 0}^{n})_{j = 1}^{\infty}$ converging to $(x_{i})_{i = 0}^{\infty}$. Now the left-hand side will converge to the respective divided difference (assuming the continuity), and by moving to a convergent subsequence, so will the $\xi_{n}$'s on the right-hand side. By the continuity of $f^{(n)}$ we are done.

	For the case of distinct $x_{i}$'s note that we have already proven the statement for polynomials of order at most $n$. By linearity it hence suffices to prove the statement for $C^{n}(a, b)$ functions vanishing on the set $\{x_{i} | 0 \leq i \leq n\}$. This we know already for $n = 1$ ; this is the mean value theorem. Let us prove the statement by induction on $n$. To simplify notation we may assume that $x_{0} < x_{1} < \ldots < x_{n}$. Note that by the mean value theorem, given that $f(x_{i}) = 0$ for any $0 \leq i \leq n$, we also have $f'(y_{i}) = 0$ for some $x_{i} < y_{i} < y_{i + 1}$, for $0 \leq i \leq n - 1$. By the induction hypothesis the $(n - 1)$:th derivative of $f'$, $f^{(n)}$ has a zero $\xi$ with $x_{0} \leq \xi \leq x_{n}$. But this is exactly what we wanted.

	TODO: figure of recursive procedure.
\end{proof}

We'll get back to smoothness in a minute. This is already a very precise sense in which divided differences work like derivatives, up to a constant. In some sense though $\frac{f^{(n)}}{n!}$, the Taylor coefficients are even more natural objects than the pure derivatives. They are the coefficients in the Taylor expansion, and they satisfy very natural Leibniz rule
\[
	\frac{(f g)^{(n)}(x)}{n!} = \sum_{k = 0}^{n} \left(\frac{f^{(k)}(x)}{k!}\right)\left(\frac{g^{(n - k)}(x)}{(n - k)!}\right),
\]
which is of course just a formula for polynomial convolution.

Divided differences enjoy similar Leibniz formula, which is related to a generalization of Taylor expansion, called Newton expansion. In Newton expansion we first fix a sequence of points $x_{0}, x_{1}, \ldots x_{n} \in (a, b)$, say pairwise distinct for starters. For $f : (a, b) \to \R$ and $x \in (a, b)$ we may start a process of rewriting
\begin{align*}
	f(x) &= f(x_0) + f(x) - f(x_{0}) \\
	&= [x_{0}]_{f} + [x, x_{0}]_{f} (x - x_{0}) \\
	&= [x_{0}]_{f} + ([x_{0}, x_{1}]_{f} + ([x, x_{0}]_{f}- [x_{0}, x_{1}]_{f}))(x - x_{0}) \\
	&= [x_{0}]_{f} + [x_{0}, x_{1}]_{f} (x - x_{0})  + [x, x_{0}, x_{1}]_{f} (x - x_{0}) (x - x_{1}) \\
	&= \ldots \\
	&= [x_{0}]_{f} + [x_{0}, x_{1}]_{f} (x - x_{0})  + [x_{0}, x_{1}, x_{2}]_{f} (x - x_{0}) (x - x_{1}) + \ldots \\
	& + [x_{0}, x_{1}, \ldots, x_{n}]_{f} (x - x_{0}) (x - x_{1}) \cdots (x - x_{n - 1}) \\
	& + [x, x_{0}, x_{1}, \ldots, x_{n}]_{f} (x - x_{0}) (x - x_{1}) \cdots (x - x_{n}).
\end{align*}

By taking first $1, 2, \ldots$ terms of the sum, one obtains Newton form of interpolating polynomial for the first $1, 2, \ldots, $ points of the sequence $x_{0}, x_{1}, \ldots$. If the points $x_{i}$ coincide, the previous coincides with the usual Taylor expansion and, as usual, the joy is that we can approximate $[x, x_{0}, x_{1}, \ldots, x_{n}]_{f}$ by a $n$'th derivative of $f$.

\begin{huom}
	As Taylor expansion lead to Taylor series, one might wonder under which conditions do Newton expansions lead to Newton series. That is, under which conditions for analytic $f : U \to \C$ and a sequence $z_{0}, z_{1}, \ldots, z_{n}, \ldots \in \C$ and $z \in \C$ the following converges
	\begin{align*}
		f(z) &= [z_{0}]_{f} \\
		&+ [z_{0}, z_{1}]_{f} (z - z_{0}) \\
		&+ [z_{0}, z_{1}, z_{2}]_{f} (z - z_{0}) (z - z_{1}) \\
		&+ \ldots \\
		&+ [z_{0}, z_{1}, z_{2}, \ldots, z_{n}]_{f} (z - z_{0}) (z - z_{1}) \cdots (z - z_{n - 1}) \\
		&+ \ldots
	\end{align*}
	If $f$ is entire and $Z = (z_{i})_{i \geq 0}$ is bounded, Newton series converges for every $z \in \C$, but if $Z$ is not bounded, series need not converge for any $z$ outside $Z$.

	For other domains the question is more subtle, and it's closely related to logarithmic potentials and subharmonic functions.
\end{huom}

TODO: Smoothness: Tohoku

\section{Cauchy's integral formula}

Complex analysis offers a nice view on divided differences: if $f$ is analytic, we may interpret divided differences contour integrals.

\begin{lem}[Cauchy's integral formula for divided differences]\label{div_cauchy}
If $\gamma$ is a closed counter-clockwise curve enclosing the numbers $x_{0}, x_{1}, \ldots, x_{n}$, we have
\[
	[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \frac{1}{2 \pi i} \int_{\gamma} \frac{f(z)}{(z - x_{0})(z - x_{1}) \cdots (z - x_{n})} dz.
\]
\end{lem}
\begin{proof}
Easy induction, by taking Cauchy's integral formula as a base case. Alternatively, the claim is a direct consequence of the Residue theorem.

There's another rather instructive proof for the statement. Write Newton expansion for $f$ and integrate both sides along $\gamma$. We get
\begin{align*}
	\frac{1}{2 \pi i} \int_{\gamma} \frac{f(z)}{(z - x_{0})(z - x_{1}) \cdots (z - x_{n})} dz &= \frac{1}{2 \pi i} \int_{\gamma} \frac{[x_{0}]_{f}}{(z - x_{0})(z - x_{1}) \cdots (z - x_{n})} dz \\
	&+ \frac{1}{2 \pi i} \int_{\gamma} \frac{[x_{0}, x_{1}]_{f}}{(z - x_{1}) \cdots (z - x_{n})} dz \\
	&+ \frac{1}{2 \pi i} \int_{\gamma} \frac{[x_{0}, x_{1}, x_{2}]_{f}}{(z - x_{2}) \cdots (z - x_{n})} dz \\
	&+ \ldots \\
	&+ \frac{1}{2 \pi i} \int_{\gamma} \frac{[x_{0}, x_{1}, x_{2}, \ldots, x_{n - 1}]_{f}}{(z - x_{n - 1})(z - x_{n})} dz \\
	&+ \frac{1}{2 \pi i} \int_{\gamma} \frac{[x_{0}, x_{1}, x_{2}, \ldots, x_{n}]_{f}}{(z - x_{n})} dz \\
	&+ \frac{1}{2 \pi i} \int_{\gamma} [z, x_{0}, x_{1}, x_{2}, \ldots, x_{n}]_{f} dz \\
\end{align*}
As $z \mapsto [z, x_{0}, x_{1}, x_{2}, \ldots, x_{n}]_{f}$ is analytic, the last integral vanishes. First $n$ integrals vanish also, since the integrands decay at least as $\frac{1}{z^2}$. Finally, the $(n + 1)$:the term given exactly what we wanted.
\end{proof}

If all the points coincide, we get the familiar formula for the $n$'th derivative. Also, if $f$ is polynomial of degree at most $n - 1$, the integrand decays as $|z|^{-2}$ and hence the divided differences vanish. Also, for $z \mapsto z^{n}$ one can use the formula to calculate the $n$'th divided difference with a residue at infinity. Formula is slightly more concisely expressed by writing for a sequence $X = (x_{i})_{i = 0}^{n}$ $p_{X}(x) = \prod_{i = 0}^{n} (x - x_{i})$. Also if one shortens $[X]_{f} = [x_{0}, x_{1}, \ldots, x_{n}]_{f}$, we have
\[
	[X]_{f} = \frac{1}{2 \pi i} \int_{\gamma} \frac{f(z)}{p_{X}(z)} dz.
\]

Cauchy's integral formula is a convenient way to think about severel identities.

\begin{esim}
For instance we may express the Lagrange interpolation polynomial of a analytic function $f$ and sequence $X = (x_{i})_{i = 0}^{n}$ by
\[
	P_{X}(x) = \frac{1}{2 \pi i} \int_{\gamma} \frac{p_{X}(x) - p_{X}(z)}{x - z}\frac{f(z)}{p_{X}(z)} dz = [X]_{f [x, \cdot]_{p_{X}}}.
\]
\end{esim}

\begin{prop}[Leibniz formula for divided differences]
	We have
	\begin{align*}
		[x_{0}, x_{1}, \ldots, x_{n}]_{f g} &= [x_{0}]_{f} [x_{0}, x_{1}, \ldots, x_{n}]_{g} \\
		&+ [x_{0}, x_{1}]_{f} [x_{1}, \ldots, x_{n}]_{g} \\
		&+ [x_{0}, x_{1}, x_{2}]_{f} [x_{2}, \ldots, x_{n}]_{g} \\
		&+ \ldots \\
		&+ [x_{0}, x_{1}, x_{2}, \ldots, x_{n}]_{f} [x_{n}]_{g}.
	\end{align*}
\end{prop}
\begin{proof}
	Write Newton expansion for $f$ and $g$ with points reversed for $g$. The rest follows as in the final proof of Theorem (\ref{div_cauchy}).
\end{proof}

TODO: use the previous for some tricks. Lagrange interpolation polynomial, Leibniz formula for divided differences.

\section{$k$-tone functions}

\begin{maar}
	$f : (a, b) \to \R$ is called $k$-tone if for any $X = (x_{i})_{i = 0}^{n}$ of distinct points we have
	\[
		[X]_{f} \geq 0,
	\]
	i.e. the $n$'th divided difference is non-negative.
\end{maar}

As we noticed, $1$-tone and $2$-tone functions are exactly the monotone increasing and convex functions. The terminology is not very established, and such functions are also occasionally called $k$-monotone or $k$-convex.

Mean value theorem for divided differences tells us that $C^{k}$ $k$-tone functions are exactly the functions with non-negative $k$'th derivative. In some sense the further smoothness assumption is not that much of a game changer. It turns out $k$-tone functions are always $k$ times differentiable in a weak sense (?), and the weak derivative is non-negative. One can also usually use regularization techniques discussed in ? to reduce a problem about general $k$-tone functions to smooth $k$-tone functions. In general:

\begin{phil}
	One should not worry about smoothness issues.
\end{phil}

We denote the space of $k$-tone functions by on interval $(a, b)$ by $P^{(k)}(a, b)$. $k$-tone functions the following enjoy the following useful properties.

\begin{prop}
	For any positive integer $k$ and open interval $(a, b)$ $P^{(k)}(a, b)$ is a closed (under pointwise convergence) convex cone.
\end{prop}
\begin{proof}
	Convex cone property is immediate form the linearity of divided differences. Also, if $f_{i} \to f$ pointwise, the respective divided differences converge, so also the closedness is clear.
\end{proof}

\begin{prop}
	$P^{(k)}$ is a local property i.e. $P^{(k)}(a, b) \cap P^{(k)}(c, d) \subset P^{(k)}(a, d)$ for any $-\infty \leq a \leq c < b \leq d \leq \infty$. To be more precise, if $f : (a, d) \to \R$ such that $\restr{f}{(a, b)} \in P^{(k)}(a, b)$ and $\restr{f}{(c, d)} \in P^{(k)}(c, d)$, then $f \in P^{(k)}(a, d)$.
\end{prop}
\begin{proof}
	For $f \in C^{k}$ the statement is immediate form the mean value theorem. For more general functions: regularizations TODO.
\end{proof}

\section{Basis $k$-tone functions}

We noticed that $k$-tone functions correspond more or less to functions with non-negative $k$'th derivative. In other words, $k$-tone functions should be $k$-fold integrals of positive functions, at least in sufficiently smooth setting. For instance $f : (a, b) \to \R$ is increasing and smooth if and only if it's of the form
\begin{align}\label{increasing_repr}
	f(x) = \int_{x_{0}}^{x} \rho(t) dt
\end{align}
for some positive $\rho \in C^{\infty}(a, b)$ and $x_{0} \in (a, b)$, up to a constant at least. For non-smooth case, we could require $\rho$ only to be a positive $L^{1}$-function: this gives us absolutely continuous increasing functions. If we further drop $\rho$ but replace the Lebesgue measure by an arbitrary Radon measure $\mu$, we get every right-continuous increasing function. Measuretheoretically these are already all the increasing functions, although we miss some functions like $\chi_{(0, \infty)}$.

If $\mu = \delta_{0}$, for instance, $f = \chi_{[0, \infty)}$. One could think that $\delta_{0}$ gives a jump for $f$ at $0$. More generally, if $\mu$ is positive linear combination if $m$ (distinct) Dirac deltas, $f$ is a function with $m$ jumps. Now every Radon measure is a weak limit of positive linear combination Dirac deltas, so every increasing function is limit of finite sums of jump functions, at least in some weak sense.

This is fact is actually contained in \ref{increasing_repr}: we may rewrite
\[
	f(x) = \int_{a}^{b} \chi_{[t, \infty)}(x)d\mu(t),
\]
$f$ is essentially sum of functions of the form $\chi_{[t, \infty)}$, again up to a constant. We will call those the basis functions for

The point is: whenever something holds for any step function, it should hold for any increasing function. In this context by ``something" I mean linear inequalities: if $\nu$ is a signed Radon measure such that for any step function $\chi_{[t, \infty)}$ we have
\[
	\int \chi_{[x, \infty)}(t) d \nu(t),
\]
then also
\[
	\int f(t) d \nu(t)
\]
for any increasing function. Actually we should also require that $\int d \nu(t) = 0$. I'm being deliberately vague about the domains, they don't really matter too much.

Things get much more interesting when we move to $k$-tone functions of higher order. For $k$-tone functions, i.e. convex functions we can make similar statements.

We can write any (smooth enough) convex function in the form
\[
	f(x) = \int_{x_{0}}^{x} \int_{x_{0}}^{x_{1}} \rho(t) dt dx_{1},
\]
at least up to a constant and linear term. By simple partial integration this can be rewritten as
\[
	f(x) = \int_{x_{0}}^{x} (x - t) \rho(t) d t,
\]
or even, better, as
\[
	f(x) = \int_{a}^{b} (x - t)_{+} \rho(t) dt,
\]
where $(x - t)_{+}$ denotes $\max(0, x - t)$. What this means is that the functions $(\cdot - t)_{+}$ work as a basis functions for convex functions, up to a affine term. By affine transformation we could equivalently take the functions of the form $|\cdot - t|$ as a basis functions.

Now if a linear equality holds for functions of the form $|x - t|$, it holds for any convex function. So since for any $x_{1}, x_{2}, \ldots, x_{m} \in \R$ we have
\[
	\sum_{1 \leq i \leq m} |x_{i} - t| \geq m |\frac{\sum_{1 \leq i \leq m} x_{i}}{m} - t|,
\]
also for any convex function
\[
	\sum_{1 \leq i \leq m} f(x_{i}) \geq m f\left(\frac{\sum_{1 \leq i \leq m} x_{i}}{m}\right),
\]
Jensen's inequaltity.

\section{Majorization}

Of course there should be a larger family of inequalities which hold for functions of the form $|x - t|$: it turns out that there is a rather simple characterization for such inequalities, by \textit{majorization}.

\begin{maar}
	Let $x = (x_{i})_{i = 1}^{n}$ and $y = (y_{i})_{i = 1}^{n}$ be two sequences of reals. We say that $y$ majorizes $x$, and write $x \prec y$, if TODO
\end{maar}

\begin{lause}[Polya-Hardy-Littlewood-Karamata-Inequality]
	Let $(a, b)$ be an open interval, $n$ positive integer, and $x = (x_{i})_{i = 1}^{n} \in (a, b)^{n}$ and $y = (y_{i})_{i = 1}^{n} \in (a, b)^{n}$. Then the following are equivalent.
	\begin{enumerate}
		\item $x \prec y$
		\item For any real number $t$ we have
		\[
			\sum_{1 \leq i \leq n} |x_{i} - t| \leq \sum_{1 \leq i \leq n} |y_{i} - t|.
		\]
		\item For any convex $f : (a, b) \to \R$ we have
		\[
			\sum_{1 \leq i \leq n} f(x_{i}) \leq \sum_{1 \leq i \leq n} f(y_{i}).
		\]
	\end{enumerate}
\end{lause}
\begin{proof}
	TODO
\end{proof}

\section{Spectral majorization}

In addition to being convenient notion to discuss $k$-tone functions, majorization explains many phenomena related to spectra of real maps. Basic fact is the following.

\begin{prop}
	If $A \leq B$, the $\spec(A) \prec_{1} \spec(B)$.
\end{prop}
\begin{proof}
	By Theorem (?) it suffices to check that for any $t \in \R$ we have $\#([t, \infty) \cap \spec(A)) \leq \#([t, \infty) \cap \spec(B))$. Translating by $t I$ this amounts to proving that if $A \leq B$, $B$ has at least as many non-negative eigenvalues as $A$. But this follows from Lemma \ref{subspace_lemma}.
\end{proof}

TODO: Higher orders, majorization


\section{TODO}
\begin{itemize}
	\item Mean value theorem, coefficient of the interpolating polynomial
	\item Basic properties, product rule.
	\item $k$-tone functions, smoothness, and representation
	\item Majorization, Jensen and Karamata inequalities, generalizations, and corollaries concerning spectrum and trace functions. Schur-Horn conjectures and Honey-Combs
	\item Tohoku contains nice proof of Lidskii inequality
	\item How to understand the inequalities arising from $k$-tone functions: is there nice way to parametrize the tuples coming from the $k$-majorization.
	\item For $k = 3$ and $3$ numbers, it's all about the biggers number: one with the largest largest number dominates.
	\item The previous probably generalizes: for $k$-tone functions and $k$ numbers on both sides, with all polynomials of degree less than $k$ vanishing on both tuples, one with largest largest value dominates, or equivalently, it's all about the constant term. This is clearly necessary, by is it also sufficient? Should be: express the whole thing as an integral, differentiate with respect to the constant term, and finally interpret as a divided difference.
	\item What if we add more terms: is there simple characterization? Why have similar integral representation, and can probably differentiate: Maybe not, or one has to be really careful. Is there characterization with linear inequalities (in addition to the equalities)?
	\item Peano Kernels: Smoothness properties, Bernstein (?) polynomials as examples.
	\item Opitz formula
	\item Regularization techniques
	\item Notion of midpoint-convexity should generalize by regularization techniques.
	\item Should Legendre transform generalize to higher orders?  For smooth enough functions probably with derivatives being inverses of each other, but what is the correct definition? And is it of any use? Maybe differentiating $k - 2$ times and then having similar characterization. Is there higher order duality?
	\item Is there elementary transformations for $k$-tone Karamata?
	\item Divided-difference series for entire functions (Newton expansion)? For analytic function? When does it converge? When does it converge to the right function?
	\item Given domain $U \subset \C$ and analytic function $f : U \to \C$, determine all subsets $V \subset U$ such that there exists Newton series with some sequence $x_{1}, x_{2}, \ldots$ converging in $V$. This is very much related to logarithmic potentials and subharmonic functions: sequence, if say bounded for starters, corresponds to a radon measure. Indeed, take weak limit of radon measures averaged exprimental measures of first elements in the sequence, if the limit exists (if not...). Now if $f = \frac{1}{z}$ for starters, we have the logarithmic potential $U(z)$ and the Newton series converges whenever $U(z) < U(0)$.
	\item Harnack-type inequalities for derivatives of Pick functions?
	\item Smooth function is in $P(0, \infty)$ if it's negative of Laplace transform of Laplace transform of a measure on $[0, \infty)$?
\end{itemize}
