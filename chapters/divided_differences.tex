\chapter{Divided differences}

Divided differences are derivatives without limits.

Consider a function $f : \R \to \R$. Its (first) divided difference is defined as
\begin{align*}
	[\cdot , \cdot]_{f} : \R^{2} \setminus \{x \neq y\} &\to \R \\
	[x, y]_{f} &= \frac{f(x) - f(y)}{x - y}.
\end{align*}

If $f$ is sufficiently smooth, we should also define $[x, x]_{f} = f'(x)$: if $f \in C^{1}(\R)$, this gives continuous extension to $[\cdot, \cdot]_{f}$. Much of the power of divided differences comes however from the fact that they conveniently carry same information even if we do not do such extension.

Consider the case of increasing $f$. This information is exactly carried by the inequality $[x, y]_{f} \geq 0$. Again, if $f$ is differentiable, this is equivalent to $f'(x) = [x, x]_{f} \geq 0$. There are many ways to see this fact, one of the more standard being the mean value theorem: If $x \neq y$, for some $\xi$ between $x$ and $y$ we have
\[
	\frac{f(x) - f(y)}{x - y} = f'(\xi).
\]
Now if the derivative is everywhere non-negative, so are divided differences. Also divided differences are sort of approximations for derivative.

We can push these ideas to higher orders. Second order divided differences should be something that captures second order behaviour of a function. In particular, if $f \in C^{2}(\R)$ has non-negative second derivative everywhere, i.e. it is convex, its second divided difference should be non-negative, and vice versa. Standard definition of convexity is almost what we are looking for: $f$ is convex if for any $x, y \in \R$ and $0 \leq t \leq 1$ we have $t f(x) + (1 - t) f(y) \geq f(t x + (1 - t)y)$. So if we define the mapping $[\cdot, \cdot, \cdot]_{f} : \R^{2} \times [0, 1]$ by $t f(x) + (1 - t) f(y) - f(t x + (1 - t)y)$, we have $[x, y, t]_{f} \geq 0$ for any $(x, y, t) \in \R^{2} \times [0, 1]$ if and only if $f^{2}(x) \geq 0$ for any $x \in \R$.

There is however much better version for the function. If we write $z = t x + (1 - t) y$, we can solve that $t = \frac{z - y}{x - y}$ and express
\begin{eqnarray*}
	[x, y, t]_{f} &=& t f(x) + (1 - t) f(y) - f(t x + (1 - t)y) \\
	&=& \frac{z - y}{x - y} f(x) + \frac{x - z}{x - y} f(y) - f(z) \\
	&=& -(z - y)(z - x) \left(\frac{f(x)}{(x - y)(x - z)} + \frac{f(y)}{(y - z)(y - x)} + \frac{f(z)}{(z - x)(z - y)} \right)
\end{eqnarray*}

If $t \notin \{0, 1\}$, $-(z - y)(z - x)$ is positive, so if $f$ is convex,
\[
	\frac{f(x)}{(x - y)(x - z)} + \frac{f(y)}{(y - z)(y - x)} + \frac{f(z)}{(z - x)(z - y)} \geq 0
\]
for any $x, y$ and $z$ such that $z$ is between $x$ and $y$. This is new expression is symmetric in its variables, so actually there's no need to assume anything on the fo $x, y$ and $z$, just that they're distinct. We can also easily carry this argument to the other direction. If this expression is non-negative any distinct $x, y$ and $z$, $f$ is convex. This motivates us to crap the previous definition and set instead
\[
	[x, y, z]_{f} := \frac{f(x)}{(x - y)(x - z)} + \frac{f(y)}{(y - z)(y - x)} + \frac{f(z)}{(z - x)(z - y)}.
\]

One would naturally except that by setting
\[ 
	[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \sum_{i = 0}^{n} \frac{f(x_{i})}{\prod_{j \neq i} (x_{i} - x_{j})},
\]
one obtains something that naturally generalizes divided differences for higher orders. This is indeed the case.

TODO:
\begin{itemize}
	\item Mean value theorem, coefficient of the interpolating polynomial
	\item Basic properties, product rule.
	\item $k$-tone functions, smoothness, and representation
	\item Majorization, Jensen and Karamata inequalities, generalizations, and corollaries conserning spectrum and trace functions. Schur-Horn conjectures and Honey-Combs
	\item Cauchy's integral formula
	\item Polynomial interpolation
	\item Peano Kernels: Smoothness properties, Bernstein (?) polynomials as examples.
	\item Opitz formula
	\item Regularization techniques
\end{itemize}
