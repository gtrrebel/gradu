\chapter{Divided differences}

\section{Motivation}
Divided differences are derivatives without limits.

Consider a function $f : \R \to \R$. Its (first) divided difference is defined as
\begin{align*}
	[\cdot , \cdot]_{f} : \R^{2} \setminus \{x \neq y\} &\to \R \\
	[x, y]_{f} &= \frac{f(x) - f(y)}{x - y}.
\end{align*}

If $f$ is sufficiently smooth, we should also define $[x, x]_{f} = f'(x)$: if $f \in C^{1}(\R)$, this gives continuous extension to $[\cdot, \cdot]_{f}$. Much of the power of divided differences comes however from the fact that they conveniently carry same information even if we do not do such extension.

Consider the case of increasing $f$. This information is exactly carried by the inequality $[x, y]_{f} \geq 0$. Again, if $f$ is differentiable, this is equivalent to $f'(x) = [x, x]_{f} \geq 0$. There are many ways to see this fact, one of the more standard being the mean value theorem: If $x \neq y$, for some $\xi$ between $x$ and $y$ we have
\[
	\frac{f(x) - f(y)}{x - y} = f'(\xi).
\]
Now if the derivative is everywhere non-negative, so are divided differences. Also divided differences are sort of approximations for derivative.

We can push these ideas to higher orders. Second order divided differences should be something that captures second order behaviour of a function. In particular, if $f \in C^{2}(\R)$ has non-negative second derivative everywhere, i.e. it is convex, its second divided difference should be non-negative, and vice versa. Standard definition of convexity is almost what we are looking for: $f$ is convex if for any $x, y \in \R$ and $0 \leq t \leq 1$ we have $t f(x) + (1 - t) f(y) \geq f(t x + (1 - t)y)$. So if we define the mapping $[\cdot, \cdot, \cdot]_{f} : \R^{2} \times [0, 1]$ by $t f(x) + (1 - t) f(y) - f(t x + (1 - t)y)$, we have $[x, y, t]_{f} \geq 0$ for any $(x, y, t) \in \R^{2} \times [0, 1]$ if and only if $f^{2}(x) \geq 0$ for any $x \in \R$.

There is however much better version for the function. If we write $z = t x + (1 - t) y$, we can solve that $t = \frac{z - y}{x - y}$ and express
\begin{eqnarray*}
	[x, y, t]_{f} &=& t f(x) + (1 - t) f(y) - f(t x + (1 - t)y) \\
	&=& \frac{z - y}{x - y} f(x) + \frac{x - z}{x - y} f(y) - f(z) \\
	&=& -(z - y)(z - x) \left(\frac{f(x)}{(x - y)(x - z)} + \frac{f(y)}{(y - z)(y - x)} + \frac{f(z)}{(z - x)(z - y)} \right)
\end{eqnarray*}

If $t \notin \{0, 1\}$, $-(z - y)(z - x)$ is positive, so if $f$ is convex,
\[
	\frac{f(x)}{(x - y)(x - z)} + \frac{f(y)}{(y - z)(y - x)} + \frac{f(z)}{(z - x)(z - y)} \geq 0
\]
for any $x, y$ and $z$ such that $z$ is between $x$ and $y$. This is new expression is symmetric in its variables, so actually there's no need to assume anything on the fo $x, y$ and $z$, just that they're distinct. We can also easily carry this argument to the other direction. If this expression is non-negative any distinct $x, y$ and $z$, $f$ is convex. This motivates us to crap the previous definition and set instead
\[
	[x, y, z]_{f} := \frac{f(x)}{(x - y)(x - z)} + \frac{f(y)}{(y - z)(y - x)} + \frac{f(z)}{(z - x)(z - y)}.
\]

One would naturally except that by setting
\[ 
	[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \sum_{i = 0}^{n} \frac{f(x_{i})}{\prod_{j \neq i} (x_{i} - x_{j})},
\]
one obtains something that naturally generalizes divided differences for higher orders. This is indeed the case.

\section{Basic properties}

For $n \geq 1$ define $D_{n} = \{x\in \R^{n} | x_{i} = x_{j} \text{ for some $1 \leq i < j \leq n$}\}$.
\begin{maar}
Let $n \geq 0$. For any real function $f : (a, b) \to \R$ we define the corresponding $n$'th divided difference $[\cdots]_{f} : (a, b)^{n + 1} \setminus D_{n + 1}$ by setting
\[
	[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \sum_{i = 0}^{n} \frac{f(x_{i})}{\prod_{j \neq i} (x_{i} - x_{j})}.
\]
\end{maar}

As with the first order divided differences, if we can continuously extend divided differences to the set $D_{n + 1}$, we should do that, and we identify the resulting function with the original one.

Albeit a rather direct generalization for the cases $n = 1$ and $n = 2$, We defined divided differences only for real valued functions, but codomain could just as well any real or complex vector space. it's not very clear why such definition should correspond to anything useful. We have however the following important properties.

\begin{prop}
	Divided differences are symmetric in the variable, i.e. for any $f : (a, b) \to \R$, $a < x_{0}, x_{1}, \ldots, x_{n} < b$ permutation $\sigma : \{1, 2, \ldots, n\} \to \{1, 2, \ldots, n\}$ we have
	\[
		[x_{1}, x_{2}, \ldots, x_{n}]_{f} = [x_{\sigma(1)}, x_{\sigma(2)}, \ldots, x_{\sigma(n)}]_{f}
	\]
	and linear in the function, i.e. for any $\alpha, \beta \in \R$, $f, g : (a, b) \to \R$ and $a < x_{0}, x_{1}, \ldots, x_{n} < b$ we have
	\[
		[x_{0}, x_{1}, \ldots, x_{n}]_{\alpha f + \beta g} = \alpha [x_{0}, x_{1}, \ldots, x_{n}]_{f} + \beta [x_{0}, x_{1}, \ldots, x_{n}]_{g}.
	\]
	In addition, divided differences can be calculated recursively as
	\begin{align}\label{divdif_rec}
		[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \frac{[x_{0}, x_{1}, \ldots, x_{n - 1}]_{f} - [x_{1}, \ldots, x_{n}]_{f}}{x_{0} - x_{n}}.
	\end{align}
\end{prop}
\begin{proof}
	Easy to check.
\end{proof}

Also, we have following classic characterization.

\begin{prop}
We have $[x_{0}, x_{1}, \ldots, x_{n}]_{(x \mapsto x^{n})} = 1$ and $[x_{0}, x_{1}, \ldots, x_{n}]_{p} = 0$ for any polynomial of degree at most $n - 1$. In other words, $[x_{0}, x_{1}, \ldots, x_{n}]_{f}$ is the leading coefficient of the Lagrange interpolation polynomial on pairs $(x_{0}, f(x_{0})), (x_{1}, f(x_{1}), \ldots, (x_{n}, f(x_{n}))$.
\end{prop}
\begin{proof}
	Claims are easily derived from each other since if $f$ is itself a polynomial of degree at most $n$, its lagrange interpolation polynomial is $f$ itself. Recall that the Lagrange interpolation polynomial of a dataset $(x_{0}, y_{0}), (x_{1}, y_{1}), \ldots, (x_{n}, y_{n})$ is given by
	\[
		\sum_{i = 0}^{n} y_{i} \frac{\prod_{j \neq i}(x - x_{j})}{\prod_{j \neq i}(x_{i} - x_{j})},
	\]
	and in our case the leading coefficient of this polynomial leads exactly to our definition for the divided differences.
\end{proof}

These observation already partially justify the terminology: as higher order derivatives are defined recursively using (first order) derivatives, higher order divided differences can be calculated recursively using (the usual) divided differences.

The most important property of the divided differences is the following.

\begin{lause}[Mean value theorem for divided differences]
	Let $n \geq 1$ and $f \in C^{n}(a, b)$. Then for any $x_{0}, x_{1}, \ldots, x_{n}$ we have $\min_{0 \leq i \leq n}(x_{i}) \leq \xi \leq \max_{0 \leq i \leq n}(x_{i})$ such that
	\begin{align}\label{mean_value}
		[x_{0}, x_{1}, \ldots, x_{n}] = \frac{f^{(n)}(\xi)}{n!}.
	\end{align}
\end{lause}
\begin{proof}
	We prove the statement assuming additionally that the divided differences define a continuous function on the whole set $(a, b)^{n + 1}$: this will proven later. Note that if one manages to prove the statement for distinct points, one may take sequence of tuples of distinct points, $((x_{i}^{(j)})_{i = 0}^{n})_{j = 1}^{\infty}$ converging to $(x_{i})_{i = 0}^{\infty}$. Now the left-hand side will converge to the respective divided difference (assuming the continuity), and by moving to a convergent subsequence, so will the $\xi_{n}$'s on the right-hand side. By the continuity of $f^{(n)}$ we are done.

	For the case of distinct $x_{i}$'s note that we have already proven the statement for polynomials of order at most $n$. By linearity it hence suffices to prove the statement for $C^{n}(a, b)$ functions vanishing on the set $\{x_{i} | 0 \leq i \leq n\}$. This we know already for $n = 1$ ; this is the mean value theorem. Let us prove the statement by induction on $n$. To simplify notation we may assume that $x_{0} < x_{1} < \ldots < x_{n}$. Note that by the mean value theorem, given that $f(x_{i}) = 0$ for any $0 \leq i \leq n$, we also have $f'(y_{i}) = 0$ for some $x_{i} < y_{i} < y_{i + 1}$, for $0 \leq i \leq n - 1$. By the induction hypothesis the $(n - 1)$:th derivative of $f'$, $f^{(n)}$ has a zero $\xi$ with $x_{0} \leq \xi \leq x_{n}$. But this is exactly what we wanted.

	TODO: figure of recursive procedure.
\end{proof}

We'll get back to smoothness in a minute. This is already a very precise sense in which divided differences work like derivatives, up to a constant. In some sense though $\frac{f^{(n)}}{n!}$, the Taylor coefficients are even more natural objects than the pure derivatives. They are the coefficients in the Taylor expansion, and they satisfy very natural Leibniz rule
\[
	\frac{(f g)^{(n)}(x)}{n!} = \sum_{k = 0}^{n} \left(\frac{f^{(k)}(x)}{k!}\right)\left(\frac{g^{(n - k)}(x)}{(n - k)!}\right),
\]
which is of course just a formula for polynomial convolution.

Divided differences enjoy similar Leibniz formula, which is related to a generalization of Taylor expansion, called Newton expansion. In Newton expansion we first fix a sequence of points $x_{0}, x_{1}, \ldots x_{n} \in (a, b)$, say pairwise distinct for starters. For $f : (a, b) \to \R$ and $x \in (a, b)$ we may start a process of rewriting
\begin{align*}
	f(x) &= f(x_0) + f(x) - f(x_{0}) \\
	&= [x_{0}]_{f} + [x, x_{0}]_{f} (x - x_{0}) \\
	&= [x_{0}]_{f} + ([x_{0}, x_{1}]_{f} + ([x, x_{0}]_{f}- [x_{0}, x_{1}]_{f}))(x - x_{0}) \\
	&= [x_{0}]_{f} + [x_{0}, x_{1}]_{f} (x - x_{0})  + [x, x_{0}, x_{1}]_{f} (x - x_{0}) (x - x_{1}) \\
	&= \ldots \\
	&= [x_{0}]_{f} + [x_{0}, x_{1}]_{f} (x - x_{0})  + [x_{0}, x_{1}, x_{2}]_{f} (x - x_{0}) (x - x_{1}) + \ldots \\
	& + [x_{0}, x_{1}, \ldots, x_{n}]_{f} (x - x_{0}) (x - x_{1}) \cdots (x - x_{n - 1}) \\
	& + [x, x_{0}, x_{1}, \ldots, x_{n}]_{f} (x - x_{0}) (x - x_{1}) \cdots (x - x_{n}).
\end{align*}

By taking first $1, 2, \ldots$ terms of the sum, one obtains Newton form of interpolating polynomial for the first $1, 2, \ldots, $ points of the sequence $x_{0}, x_{1}, \ldots$. If the points $x_{i}$ coincide, we get
\begin{align}\label{taylor_expansion}
	f(x) = \sum_{k = 0}^{n - 1} \frac{f^{(k)}(x_{0})}{k!} + [x, x_{0}, x_{0}, \ldots, x_{0}]_{f} (x - x_{0})^{n},
\end{align}
the usual Taylor expansion. Consequently
\[
	[x, x_{0}, x_{0}, \ldots, x_{0}]_{f} = \frac{f(x) - \sum_{k = 0}^{n - 1} \frac{f^{(k)}(x_{0})}{k!}}{(x - x_{0})^{n}},
\]
and as is well known, this error term can be also written in the form
\[
	[x, x_{0}, x_{0}, \ldots, x_{0}]_{f} = \frac{1}{(x - x_{0})^{n}}\int_{x_{0}}^{x} \frac{f^{(n)}(t) (x - t)^{n - 1}}{(n - 1)!} dt.
\]

\begin{huom}
	As Taylor expansion lead to Taylor series, one might wonder under which conditions do Newton expansions lead to Newton series. That is, under which conditions for analytic $f : U \to \C$ and a sequence $z_{0}, z_{1}, \ldots, z_{n}, \ldots \in \C$ and $z \in \C$ the following converges
	\begin{align*}
		f(z) &= [z_{0}]_{f} \\
		&+ [z_{0}, z_{1}]_{f} (z - z_{0}) \\
		&+ [z_{0}, z_{1}, z_{2}]_{f} (z - z_{0}) (z - z_{1}) \\
		&+ \ldots \\
		&+ [z_{0}, z_{1}, z_{2}, \ldots, z_{n}]_{f} (z - z_{0}) (z - z_{1}) \cdots (z - z_{n - 1}) \\
		&+ \ldots
	\end{align*}
	If the points coincide we recover the usual Taylor expansion and from the \ref{taylor_expansion} we see that the Taylor series converges on disc $\D(z_{0}, r)$ if $\left|\frac{f^{(n)}}{n!}\right| \leq C/r^{n}$ for some $C > 0$. If $f$ is entire, we have such bound for every $r$ and the series converges everywhere. In a similar vein,
	if $f$ is entire and $Z = (z_{i})_{i \geq 0}$ is bounded, also Newton series converges for every $z \in \C$, but if $Z$ is not bounded, series need not converge for any $z$ outside $Z$.

	For other domains the question is more subtle, and it's closely related to logarithmic potentials and subharmonic functions.
\end{huom}

Divided differences enjoy also the following nesting property.

\begin{prop}
	For any $f : (a, b) \to \R$ and pairwise distinct $x_{1}, x_{2}, \ldots, x_{n}, y_{1}, y_{2}, \ldots, y_{m}$ we have
	\[
		[y_{1}, y_{2}, \ldots, y_{m}]_{[\cdot, x_{1}, x_{2}, \ldots, x_{n}]_{f}} = [y_{1}, y_{2}, \ldots, y_{m}, x_{1}, x_{2}, \ldots, x_{n}]_{f}.
	\]
	In particular
	\[
		\frac{d^{m}}{d x^{m}} \left([x, x_{1}, x_{2}, \ldots, x_{n}]_{f}\right) = m! [x, x, \ldots, x, x_{1}, x_{2}, \ldots, x_{m}]
	\]
\end{prop}
\begin{proof}
	Note that both $[y_{1}, y_{2}, \ldots, y_{m}]_{[\cdot, x_{1}, x_{2}, \ldots, x_{n}]_{f}}$ and $[y_{1}, y_{2}, \ldots, y_{m}, x_{1}, x_{2}, \ldots, x_{n}]_{f}$ satisfy \ref{divdif_rec} and they agree when $m = 1$.
\end{proof}

\section{Cauchy's integral formula}

Complex analysis offers a nice view on divided differences: if $f$ is analytic, we may interpret divided differences contour integrals.

\begin{lem}[Cauchy's integral formula for divided differences]\label{div_cauchy}
If $\gamma$ is a closed counter-clockwise curve enclosing the numbers $x_{0}, x_{1}, \ldots, x_{n}$, we have
\[
	[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \frac{1}{2 \pi i} \int_{\gamma} \frac{f(z)}{(z - x_{0})(z - x_{1}) \cdots (z - x_{n})} dz.
\]
\end{lem}
\begin{proof}
Easy induction, by taking Cauchy's integral formula as a base case. Alternatively, the claim is a direct consequence of the Residue theorem.

There's another rather instructive proof for the statement. Write Newton expansion for $f$ and integrate both sides along $\gamma$. We get
\begin{align*}
	\frac{1}{2 \pi i} \int_{\gamma} \frac{f(z)}{(z - x_{0})(z - x_{1}) \cdots (z - x_{n})} dz &= \frac{1}{2 \pi i} \int_{\gamma} \frac{[x_{0}]_{f}}{(z - x_{0})(z - x_{1}) \cdots (z - x_{n})} dz \\
	&+ \frac{1}{2 \pi i} \int_{\gamma} \frac{[x_{0}, x_{1}]_{f}}{(z - x_{1}) \cdots (z - x_{n})} dz \\
	&+ \frac{1}{2 \pi i} \int_{\gamma} \frac{[x_{0}, x_{1}, x_{2}]_{f}}{(z - x_{2}) \cdots (z - x_{n})} dz \\
	&+ \ldots \\
	&+ \frac{1}{2 \pi i} \int_{\gamma} \frac{[x_{0}, x_{1}, x_{2}, \ldots, x_{n - 1}]_{f}}{(z - x_{n - 1})(z - x_{n})} dz \\
	&+ \frac{1}{2 \pi i} \int_{\gamma} \frac{[x_{0}, x_{1}, x_{2}, \ldots, x_{n}]_{f}}{(z - x_{n})} dz \\
	&+ \frac{1}{2 \pi i} \int_{\gamma} [z, x_{0}, x_{1}, x_{2}, \ldots, x_{n}]_{f} dz \\
\end{align*}
As $z \mapsto [z, x_{0}, x_{1}, x_{2}, \ldots, x_{n}]_{f}$ is analytic, the last integral vanishes. First $n$ integrals vanish also, since the integrands decay at least as $\frac{1}{z^2}$. Finally, the $(n + 1)$:th term gives exactly what we wanted.
\end{proof}

If all the points coincide, we get the familiar formula for the $n$'th derivative. Also, if $f$ is polynomial of degree at most $n - 1$, the integrand decays as $|z|^{-2}$ and hence the divided differences vanish. Also, for $z \mapsto z^{n}$ one can use the formula to calculate the $n$'th divided difference with a residue at infinity. Formula is slightly more concisely expressed by writing for a sequence $X = (x_{i})_{i = 0}^{n}$ $p_{X}(x) = \prod_{i = 0}^{n} (x - x_{i})$. Also if one shortens $[X]_{f} = [x_{0}, x_{1}, \ldots, x_{n}]_{f}$, we have
\[
	[X]_{f} = \frac{1}{2 \pi i} \int_{\gamma} \frac{f(z)}{p_{X}(z)} dz.
\]

Cauchy's integral formula is a convenient way to think about severel identities.

\begin{esim}
For instance we may express the Lagrange interpolation polynomial of a analytic function $f$ and sequence $X = (x_{i})_{i = 0}^{n}$ by
\[
	P_{X}(x) = \frac{1}{2 \pi i} \int_{\gamma} \frac{p_{X}(x) - p_{X}(z)}{x - z}\frac{f(z)}{p_{X}(z)} dz = [X]_{f [x, \cdot]_{p_{X}}}.
\]
\end{esim}

\begin{prop}[Leibniz formula for divided differences]
	We have
	\begin{align*}
		[x_{0}, x_{1}, \ldots, x_{n}]_{f g} &= [x_{0}]_{f} [x_{0}, x_{1}, \ldots, x_{n}]_{g} \\
		&+ [x_{0}, x_{1}]_{f} [x_{1}, \ldots, x_{n}]_{g} \\
		&+ [x_{0}, x_{1}, x_{2}]_{f} [x_{2}, \ldots, x_{n}]_{g} \\
		&+ \ldots \\
		&+ [x_{0}, x_{1}, x_{2}, \ldots, x_{n}]_{f} [x_{n}]_{g}.
	\end{align*}
\end{prop}
\begin{proof}
	Write Newton expansion for $f$ and $g$ with points reversed for $g$. The rest follows as in the final proof of Theorem (\ref{div_cauchy}).
\end{proof}

\section{Peano kernels}

We already noticed that the $n$:th divided differences of the form $[x, x, \ldots, x, y]_{f}$ can be written as an integral of $n$:th derivative of $f$ with some positive polynomial weight. This is no anomaly: if $f \in C^{n}(a, b)$ and $a < x_{0}, x_{1}, \ldots, x_{n} < b$, there exits a function $w = w_{x_{0}, x_{1}, \ldots, x_{n}}$ such that
\[
	[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \int_{\R} f^{(n)}(t) w(t) dt.
\]
It turns out that $w$, \textit{Peano kernel} with nodes $x_{0}, x_{1}, \ldots, x_{n}$ is piecewise polynomial compactly supported $C^{n - 2}(\R)$-function. TODO

\section{$k$-tone functions}

\begin{maar}
	$f : (a, b) \to \R$ is called $k$-tone if for any $X = (x_{i})_{i = 0}^{n}$ of distinct points we have
	\[
		[X]_{f} \geq 0,
	\]
	i.e. the $n$'th divided difference is non-negative.
\end{maar}

As we noticed, $1$-tone and $2$-tone functions are exactly the monotone increasing and convex functions. The terminology is not very established, and such functions are also occasionally called $k$-monotone or $k$-convex.

Mean value theorem for divided differences tells us that $C^{k}$ $k$-tone functions are exactly the functions with non-negative $k$'th derivative. It turns out that this almost true in general case, namely we have the following result.

\begin{lause}\label{k-tone_smooth}
	Let $f : (a, b) \to \R$ and $k \geq 2$. Then $f$ is $k$-tone, if and only if $f \in C^{k - 2}(a, b)$, $f^{(k - 2)}(x)$ is convex.
\end{lause}

We will postpone the proof.

In some sense the further smoothness assumption is not that much of a game changer. It turns out $k$-tone functions are always $k$ times differentiable in a weak sense (?), and the weak derivative is non-negative.

One can also usually use regularization techniques discussed in ? to reduce a problem about general $k$-tone functions to smooth $k$-tone functions. In general:

\begin{phil}
	One should not worry about smoothness issues.
\end{phil}

We will not resort to such sorcery, however, but try to understand the true reasons behind the smoothness.

We denote the space of $k$-tone functions by on interval $(a, b)$ by $P^{(k)}(a, b)$. $k$-tone functions the following enjoy the following useful properties.

\begin{prop}
	For any positive integer $k$ and open interval $(a, b)$ $P^{(k)}(a, b)$ is a closed (under pointwise convergence) convex cone.
\end{prop}
\begin{proof}
	Convex cone property is immediate form the linearity of divided differences. Also, if $f_{i} \to f$ pointwise, the respective divided differences converge, so also the closedness is clear.
\end{proof}

\begin{prop}
	$P^{(k)}$ is a local property i.e. $P^{(k)}(a, b) \cap P^{(k)}(c, d) \subset P^{(k)}(a, d)$ for any $-\infty \leq a \leq c < b \leq d \leq \infty$. To be more precise, if $f : (a, d) \to \R$ such that $\restr{f}{(a, b)} \in P^{(k)}(a, b)$ and $\restr{f}{(c, d)} \in P^{(k)}(c, d)$, then $f \in P^{(k)}(a, d)$.
\end{prop}
\begin{proof}
	For $f \in C^{k}$ the statement is immediate form the mean value theorem. In general we can argue bit similarly as in the case $k = 1$. For $k = 1$ note that if $a < x < c < b < z < d$ we may take $c < y < b$. Now
	\[
		f(z) - f(x) = (f(z) - f(y)) + (f(y) - f(x)) \geq 0.
	\]
	In terms of divided differences
	\[
		[x, z]_{f} = [z, y]_{f} \frac{z - y}{z - x} + [x, y]_{f} \frac{y - x}{z - x}.
	\]
	The point is that we can express divided differences as weighted sums of divided differences of tuples with smaller supports. More generally, if $a < x_{0} < \ldots < x_{k} < d$ with $x_{0} < c$ and $d < x_{k}$, take any $y \in (c, b)$ distinct from $x_{i}$'s and we have
	\[
		[x_{0}, \ldots, x_{k}]_{f} = [x_{1}, \ldots, x_{k}, y]_{f} \frac{x_{k} - y}{x_{k} - x_{0}} + [x_{0}, \ldots, x_{k - 1}, y]_{f} \frac{y - x_{0}}{x_{k} - x_{0}}.
	\]
	This identity is easily verified by applying the previous version for the function $x \mapsto [\cdot, x_{1}, x_{2}, \ldots, x_{n - 1}]_{f}$. By repeating this process, we will end up with divided differences of tuples completely lying on $(a, b)$ or $(c, d)$. Formally, we could induct on $l$ number of $x_{i}$'s outside $(c, b)$ and note that if tuple isn't good already, the two new tuples have lower number $l$.
\end{proof}

\section{Basis $k$-tone functions}

We noticed that $k$-tone functions correspond more or less to functions with non-negative $k$'th derivative. In other words, $k$-tone functions should be $k$-fold integrals of positive functions, at least in sufficiently smooth setting. For instance $f : (a, b) \to \R$ is increasing and smooth if and only if it's of the form
\begin{align}\label{increasing_repr}
	f(x) = \int_{x_{0}}^{x} \rho(t) dt
\end{align}
for some positive $\rho \in C^{\infty}(a, b)$ and $x_{0} \in (a, b)$, up to a constant at least. For non-smooth case, we could require $\rho$ only to be a positive $L^{1}$-function: this gives us absolutely continuous increasing functions. If we further drop $\rho$ but replace the Lebesgue measure by an arbitrary Radon measure $\mu$, we get every right-continuous increasing function. Measuretheoretically these are already all the increasing functions, although we miss some functions like $\chi_{(0, \infty)}$.

If $\mu = \delta_{0}$, for instance, $f = \chi_{[0, \infty)}$. One could think that $\delta_{0}$ gives a jump for $f$ at $0$. More generally, if $\mu$ is positive linear combination if $m$ (distinct) Dirac deltas, $f$ is a function with $m$ jumps. Now every Radon measure is a weak limit of positive linear combination Dirac deltas, so every increasing function is limit of finite sums of jump functions, at least in some weak sense.

This is fact is actually contained in \ref{increasing_repr}: we may rewrite
\[
	f(x) = \int_{a}^{b} \chi_{[t, \infty)}(x)d\mu(t),
\]
$f$ is essentially sum of functions of the form $\chi_{[t, \infty)}$, again up to a constant. We will call those the basis functions for

The point is: whenever something holds for any step function, it should hold for any increasing function. In this context by ``something" I mean linear inequalities: if $\nu$ is a signed Radon measure such that for any step function $\chi_{[t, \infty)}$ we have
\[
	\int \chi_{[x, \infty)}(t) d \nu(t),
\]
then also
\[
	\int f(t) d \nu(t)
\]
for any increasing function. Actually we should also require that $\int d \nu(t) = 0$. I'm being deliberately vague about the domains, they don't really matter too much.

Things get much more interesting when we move to $k$-tone functions of higher order. For $k$-tone functions, i.e. convex functions we can make similar statements.

We can write any (smooth enough) convex function in the form
\[
	f(x) = \int_{x_{0}}^{x} \int_{x_{0}}^{x_{1}} \rho(t) dt dx_{1},
\]
at least up to a constant and linear term. By simple partial integration this can be rewritten as
\[
	f(x) = \int_{x_{0}}^{x} (x - t) \rho(t) d t,
\]
or even, better, as
\[
	f(x) = \int_{a}^{b} (x - t)_{+} \rho(t) dt,
\]
where $(x - t)_{+}$ denotes $\max(0, x - t)$. What this means is that the functions $(\cdot - t)_{+}$ work as a basis functions for convex functions, up to a affine term. By affine transformation we could equivalently take the functions of the form $|\cdot - t|$ as a basis functions.

Now if a linear equality holds for functions of the form $|x - t|$, it holds for any convex function. So since for any $x_{1}, x_{2}, \ldots, x_{m} \in \R$ we have
\[
	\sum_{1 \leq i \leq m} |x_{i} - t| \geq m |\frac{\sum_{1 \leq i \leq m} x_{i}}{m} - t|,
\]
also for any convex function
\[
	\sum_{1 \leq i \leq m} f(x_{i}) \geq m f\left(\frac{\sum_{1 \leq i \leq m} x_{i}}{m}\right),
\]
Jensen's inequaltity.

\section{Majorization}

Of course there should be a larger family of inequalities which hold for functions of the form $|x - t|$: it turns out that there is a rather simple characterization for such inequalities, by \textit{majorization}.

\begin{maar}
	Let $x = (x_{i})_{i = 1}^{n}$ and $y = (y_{i})_{i = 1}^{n}$ be two sequences of reals. We say that $y$ majorizes $x$, and write $x \prec y$, if TODO
\end{maar}

\begin{lause}[Polya-Hardy-Littlewood-Karamata-Inequality]
	Let $(a, b)$ be an open interval, $n$ positive integer, and $x = (x_{i})_{i = 1}^{n} \in (a, b)^{n}$ and $y = (y_{i})_{i = 1}^{n} \in (a, b)^{n}$. Then the following are equivalent.
	\begin{enumerate}
		\item $x \prec y$
		\item For any real number $t$ we have
		\[
			\sum_{1 \leq i \leq n} |x_{i} - t| \leq \sum_{1 \leq i \leq n} |y_{i} - t|.
		\]
		\item For any convex $f : (a, b) \to \R$ we have
		\[
			\sum_{1 \leq i \leq n} f(x_{i}) \leq \sum_{1 \leq i \leq n} f(y_{i}).
		\]
	\end{enumerate}
\end{lause}
\begin{proof}
	TODO
\end{proof}

\section{Spectral majorization}

In addition to being convenient notion to discuss $k$-tone functions, majorization explains many phenomena related to spectra of real maps. Basic fact is the following.

\begin{prop}
	If $A \leq B$, the $\spec(A) \prec_{1} \spec(B)$.
\end{prop}
\begin{proof}
	By Theorem (?) it suffices to check that for any $t \in \R$ we have $\#([t, \infty) \cap \spec(A)) \leq \#([t, \infty) \cap \spec(B))$. Translating by $t I$ this amounts to proving that if $A \leq B$, $B$ has at least as many non-negative eigenvalues as $A$. But this follows from Lemma \ref{subspace_lemma}.
\end{proof}

TODO: Higher orders, majorization

\section{Smoothness}

\begin{lause}
	Let $f : (a, b) \to \R$ and $n \geq 1$. Then $f \in C^{n}(a, b)$, if and only if $n$:th divided difference of $f$ extends to continuous function on $(a, b)^{n + 1}$.
\end{lause}

Actually we can prove a slightly better statement. Let $n, m \geq 1$ and and denote
\[
	D_{n, m} = \{x \in \R^{n} | x_{i_{1}} = x_{i_{2}} = \ldots = x_{i_{k}} \text{ for some $1 \leq i_{1} < i_{2} < \ldots < i_{m} \leq n$} \}.
\]
\begin{lause}
	Let $f : (a, b) \to \R$, $0 \leq m \leq n$. Then $f \in C^{m}(a, b)$, if and only if $n$:th divided difference of $f$ extends to continuous function to $(a, b)^{n + 1} \setminus D_{n + 1, m + 2}$. Moreover, this extension is unique and satisfies \ref{divdif_rec} and \ref{mean_value}.
\end{lause}

\begin{proof}
	We first prove the ``only if"-direction by induction on $m$.

	The case $m = 0$ is clear. Now fix $m > 0$. Let us prove the statement for this $m$ by induction on $n$.

	Consider the base case case $n = m$. Take any sequence $a < x_{0} \leq x_{1} \leq x_{2} \ldots \leq x_{n} < b$. By induction hypothesis for the pair $(n, m - 1)$ we can extend the divided difference to this point if $x_{0} < x_{n}$. If $x_{0} = x_{1} = \ldots = x_{n}$, we will extend the divided difference as
	\[
		[x_{0}, x_{1}, \ldots, x_{n}] = \frac{f^{(n)}(x_{0})}{n!}.
	\]
	But the mean value theorem for the divided difference immediately implies this extension is continuous at $(x_{0}, x_{1}, \ldots, x_{n})$.

	If $n > m$, and $x \in \R^{n + 1} \setminus D_{n + 1, m + 2}$, we necessarily have $x_{0} < x_{n}$ and hence we can extend the divided differences using \ref{divdif_rec}. By construction, our extension satisfies \ref{divdif_rec} and \ref{mean_value} and since $\R^{n + 1} \setminus D_{n + 1}$ is dense in $\R^{n + 1} \setminus D_{n + 1, m + 2}$, the extension is necessarily unique.

	Let us then prove the ``if"-direction. We start with a lemma.

	\begin{lem}\label{smooth_lemma}
		Let $n \geq 1$ and $m \geq 0$. If the $n$:th divided difference of $f$ has continuous extension to the set $\R^{n + 1} \setminus D_{n + 1, m + 2}$, then there also is a continuous extension for the $(n - 1)$:th divided difference of $f$ to the set $\R^{n} \setminus D_{n, m + 2}$. 
	\end{lem}
	\begin{proof}
		Take any $x \in \R^{n} \setminus D_{n, m + 2}$. We induct downward on $l$, the number of distinct components of $x$. If $l = n$, the statement is clear. Assume then that $l < n$. Now there exist $x' \in \R^{n} \setminus D_{n, m + 2}$ with $l + 1$ distinct components, which differs from $x$ by exactly one component, say i'th one. We then extend
		\[
			[x_{1}, x_{2}, \ldots, x_{n}]_{f} := [x_{1}, x_{2}, \ldots, x'_{i}, \ldots, x_{n}]_{f} + (x'_i - x_{i}) [x_{1}, x_{2}, \ldots, x_{i}, x'_{i}, \ldots, x_{n}]_{f}.
		\]
		Now if $x^{(n)} \to x$, then by comparing both sides also $[x_{1}, x_{2}, \ldots, x_{n}]_{f}$'s converge, and we have the continuous extension.
	\end{proof}

	Now let us continue with the proof. We induct on $m$. The case $m = 0$ is clear. The case $n = m = 1$ is also rather clear, the diagonal $[x, x]_{f}$ given the derivative of $f$ and continuity of the derivative is implied by the continuity of $[\cdot, \cdot]_{f}$ along the diagonal.

	\begin{lem}\label{divdif_der}
		If $f \in C^{1}(a, b)$ and $a < x_{1}, x_{2}, \ldots, x_{n} < b$ are distinct, then 
		\[
			[x_{1}, x_{2}, \ldots, x_{n}]_{f'} = \sum_{1 \leq i \leq n} [x_{1}, x_{2}, \ldots, x_{i - 1}, x_{i}, x_{i}, x_{i + 1}, \ldots, x_{n}]_{f}
		\]
	\end{lem}
	\begin{proof}
		We induct on $n$: the case $n = 1$ is clear. When $n > 1$, by \ref{divdif_rec} we have
		\begin{align*}
			[x_{1}, x_{2}, \ldots, x_{n}]_{f'} &= \frac{[x_{1}, \ldots, x_{n - 1}]_{f'} - [x_{2}, \ldots, x_{n}]_{f'}}{x_{1} - x_{n}} \\
			&= \frac{\left(\sum_{1 \leq i \leq n - 1} [x_{1}, \ldots, x_{i}, x_{i}, \ldots, x_{n - 1}]_{f}\right) - \left(\sum_{2 \leq i \leq n} [x_{2}, \ldots, x_{i}, x_{i}, \ldots, x_{n}]_{f}\right)}{x_{1} - x_{n}} \\
			&= \sum_{2 \leq i \leq n - 1} \frac{[x_{1}, \ldots, x_{i}, x_{i}, \ldots, x_{n - 1}]_{f} - [x_{2}, \ldots, x_{i}, x_{i}, \ldots, x_{n}]_{f}}{x_{1} - x_{n}} \\
			&+ \frac{[x_{1}, x_{1}, x_{2}, \ldots, x_{n - 1}]_{f} - [x_{2}, \ldots, x_{n}, x_{n}]_{f}}{x_{1} - x_{n}} \\
			&= \sum_{2 \leq i \leq n - 1}  [x_{1}, \ldots, x_{i}, x_{i}, \ldots, x_{n}]_{f} \\
			&+ \frac{[x_{1}, x_{1}, \ldots, x_{n - 1}]_{f} - [x_{1}, \ldots, x_{n} ]_{f}}{x_{1} - x_{n}} + \frac{[x_{1}, \ldots, x_{n}]_{f} - [x_{2}, \ldots, x_{n}, x_{n} ]_{f}}{x_{1} - x_{n}} \\
			&= \sum_{1 \leq i \leq n}  [x_{1}, \ldots, x_{i}, x_{i}, \ldots, x_{n}]_{f} \\
		\end{align*}
	\end{proof}

	Let us then take $2 \leq m = n$. By the lemma \ref{smooth_lemma} and the case $m = 1$ we see that $f \in C^{1}(a, b)$. But by the lemma \ref{divdif_der} $(n - 1)$:th divided differences of $f'$ extend to continuous function to $\R^{n}$. Hence by the induction hypothesis $f' \in C^{(n - 1)}(a, b)$ and hence $f \in C^{n}(a, b)$.

	Finally the lemma \ref{smooth_lemma} immediately implies the remaining cases $\leq m < n$, by induction on $n$.
\end{proof}

There is however more interesting equivalence to be made.

\begin{lause}\label{bounded_div}
	Let $f : (a, b) \to \R$ and $n \geq 1$. Then $f \in C^{n - 1}(a, b)$ and $f^{(n - 1)}$ is Lipschitz, if and only if $n$:th divided difference of $f$ is bounded. Moreover,
	\[
		\sup_{a < x_{0} < x_{1} < \ldots < x_{n}< b} |[x_{0}, x_{1}, \ldots, x_{n}]_{f}| = \frac{\Lip(f^{(n - 1)})}{n!}
	\]
\end{lause}

\begin{proof}
	We first prove the ``if"-direction.

	The case $n = 1$ is clear. In the case $n = 2$ note that
	\[
		[x, y, z] = \frac{[y, x]_{f} - [z, x]_{f}}{y - z}
	\]
	Since this quantity is bounded, it follows that $[\cdot, x]_{f}$ has a limit at $x$, which means exactly that $f$ is differentiable at $x$. Note that in addition for any $x, x', y, y'$ we have
	\[
		[x', x, y]_{f} + [y', y, x] = \frac{([x', x]_{f} - [x, y]_{f}) - ([y', y]_{f} - [x, y]_{f})}{x - y} = \frac{[x', x] - [y, y']}{x - y}.
	\]
	Letting $x' \to x$ and $y' \to y$ we see that $f'$ is Lipschitz and hence also $f \in C^{1}(a, b)$ .

	Now for general $n$ we argue by induction on $n$. Let $n > 2$. Note that since
	\[
		[x_{0}, x_{1}, x_{2}, \ldots, x_{n}]_{f} = \frac{[x_{0}, x_{1}, \ldots, x_{n - 1}]_{f} - [x_{1}, \ldots, x_{n}]_{f}}{x_{0} - x_{n}}.
	\]
	The map $x \mapsto [x, x_{1}, x_{2}, \ldots, x_{n - 1}]$ is $1$-Lipschitz for any $x_{1}, x_{2}, \ldots, x_{n - 1}$ and hence $(n - 1)$:th divided difference is Lipschitz and consequently bounded. By induction hypothesis $f$ is $C^{n - 2}(a, b)$ and hence at least $C^{1}(a, b)$. But now since
	\[
		[x_{1}, x_{2}, \ldots, x_{n}]_{f'} = \sum_{1 \leq i \leq n} [x_{1}, x_{2}, \ldots, x_{i - 1}, x_{i}, x_{i}, x_{i + 1}, \ldots, x_{n}]_{f}
	\]
	$f'$ has bounded $(n - 1)$:th divided differences, and by the induction hypothesis, $f \in C^{(n - 1)}(a, b)$ and $f^{(n - 1)}$ is Lipschitz. Induction also immediately gives
	\[
		\frac{\Lip(f^{(n - 1)})}{n!} \leq \sup_{a < x_{0} < x_{1} < \ldots < x_{n}< b} |[x_{0}, x_{1}, \ldots, x_{n}]_{f}|.
	\]

	Let us then prove the ``only if"-direction. Take any $a < x_{0} < x_{1} < \ldots < x_{n} < b$. By the mean-value theorem for divided differences we have
	\begin{align*}
		\left|[x_{0}, x_{1}, \ldots, x_{n}]_{f} \right| &= \left|\frac{[x_{0}, x_{1}, \ldots, x_{n - 1}]_{f} - [x_{1}, x_{2}, \ldots, x_{n}]_{f}}{x_{0} - x_{n}} \right| \\
		&= \frac{1}{(n - 1)!}\left|\frac{f^{(n - 1)}(\xi_{1}) - f^{(n - 1)}(\xi_{2})}{x_{0} - x_{n}} \right| \\
		&\leq \frac{\Lip(f^{(n - 1)})}{(n - 1)!} \left|\frac{\xi_{1} - \xi_{2}}{x_{0} - x_{n}}\right|
	\end{align*}
	for some $x_{0} \leq \xi_{1} \leq \xi_{2} \leq x_{n}$. Hence $n$:th divided difference is bounded, but the inequality is not quite sharp enough.

	We can make it sharp with some tricks. Firstly, when considering the supremum, we only need to consider tuples where all but one of entries are equal. Indeed pick any $a < x_{0} < x_{1} < x_{2} < \ldots < x_{n}$. Now consider the map $g(x) = [x, x_{0}]_{f}$. This is $C^{n - 1}(x_{0}, b)$ so by the mean-value theorem we have
	\[
		[x_{0}, x_{1}, \ldots, x_{n}]_{f} = [x_{1}, x_{2}, \ldots, x_{n}]_{g} = \frac{g^{(n - 1)}(\xi)}{(n - 1)!} = [\xi, \xi, \ldots, \xi]_{g} = [x_{0}, \xi, \xi, \ldots]_{f}.
	\]
	Hence it suffices to consider only tuples $(x, x, x, \ldots, y)$ where $x$ appears $n$ times. Now by the Taylor's theorem
	\begin{align*}
		\left|[x, x, \ldots, y]_{f}\right| &= \left|\frac{[x, x, \ldots, x]_{f} - [x, x, \ldots, y]}{x - y} \right| \\
		&= \left|\frac{\frac{f^{(n - 1)}}{(n - 1)!} - \int_{x}^{y} \frac{f^{(n - 1)}(t) (y - t)^{n - 2}}{(n - 2)!} dt}{x - y} \right| = \\
		&= \frac{1}{(n - 2)!|y - x|^{n - 1}} \left|\int_{x}^{y} (f^{(n - 1)}(x) - f^{(n - 1)}(t)) (y - t)^{n - 2} dt \right| \\
		&\leq \frac{\Lip(f^{(n - 1)})}{(n - 2)!} \int_{x}^{y} |x - t| |y - t|^{n - 2} dt \\
		&= \frac{\Lip(f^{(n - 1)})}{n!},
	\end{align*}
	and we are done.
\end{proof}

With such tools we are ready to tackle the regularity of $k$-tone functions

\begin{proof}[Proof of the theorem \ref{k-tone_smooth}]

	We start with a lemma.
	\begin{lem}
		If $k \geq 1$ and $f : (a, b) \to \R$ is $k$-tone, then the $(k - 1)$:th divided differences of $f$ are locally bounded, i.e. bounded on every closed subinterval of $(a, b)$.
	\end{lem}
	\begin{proof}
		We induct on $k$. The case $k = 1$ is rather clear: for any $a < c < x < d < b$ we have $f(x) \in [f(c), f(d)]$. Take then $k > 1$ and any closed interval $[c, d] \subset (a, b)$. Take $a < x_{0} < c$. The map $g = [\cdot, x_{0}]_{f}$ is $(k - 1)$-tone, so by induction hypothesis the $(k - 2)$:th divided differences of $g$ are bounded on $[c, d]$. Now for any $c \leq x_{1} < x_{2} < \ldots < x_{k} \leq d$ we have
		\[
			[x_{1}, x_{2}, \ldots, x_{k}]_{f} = (x_{k} - x_{0}) [x_{0}, x_{1}, \ldots, x_{k}]_{f} + [x_{0}, x_{1}, \ldots, x_{k - 1}]_{f} \geq [x_{0}, x_{1}, \ldots, x_{k - 1}]_{f}
		\]
		But $[x_{0}, x_{1}, \ldots, x_{k - 1}]_{f} = [x_{1}, \ldots, x_{k - 1}]_{g}$ is bounded, so we have lower bound for $(k - 1)$:th divided differences of $f$. Similarly, by taking $d < x_{0} < b$ we get upper bound, and we are done.
	\end{proof}
	Combining the lemma with theorem \ref{bounded_div} gives the right smoothness. Convexity condition is implied by the lemma \ref{divdif_der} combined with induction on $k$.
\end{proof}

\section{Analyticity and Bernstein's theorems}

By requiring (some kind of) regularity for the divided differences of all orders, occasionally we get more than smoothness, namely analyticity. Most basic result of this kind is the following.

\begin{lause}\label{div_anal}
	Let $f : (a, b) \to \R$. Then $f$ is real analytic, if and only if for every closed subinteval $[c, d]$ of $(a, b)$ there exists constant $C$ such that for any $n \geq 1$
	\[
		\sup_{a < x_{0} < x_{1} < \ldots < x_{n}< b} |[x_{0}, x_{1}, \ldots, x_{n}]_{f}| \leq C^{n + 1}.
	\]
\end{lause}
\begin{proof}
	Let's first prove that ``if"-direction. We need to prove that the for any $x_{0} \in (a, b)$ Taylor series at $x_{0}$ converges at some neighbourhood of $x_{0}$. As observed before, the $n$:th error term in Taylor series is given by
	\[
		[x, x_{0}, x_{0}, \ldots, x_{0}]_{f} (x - x_{0})^{n}
	\]
	with $n$ $x_{0}$'s. Now choose $a < c < x_{0} < d < b$ and take any $x$ with $x \in [c, d]$ and $|x - x_{0}| C < 1$, where $C$ is given by the assumption for interval $c, d$. But then the error term tends to zero and we are done.

	For the other direction note that if $x_{0} \in (a, b)$ and $f$ extends to analytic function on $\D(x_{0}, r)$, we definititely have $\left|\frac{f^{(n)}(x_{0})}{n!}\right| \leq C^{n + 1}$ for some $C$. If $|x - x_{0}| < r$ we have
	\[
		\frac{f^{(k)}(x)}{k!} = \sum_{n = k}^{\infty} \binom{n}{k} \frac{f^{(n)}(x_{0})}{n!} (x - x_{0})^{n - k},
	\]
	which may be estimated by
	\[
	 \left|\frac{f^{(k)}(x)}{k!}\right| \leq C^{k + 1}\sum_{n = k}^{\infty} \binom{n}{k} C^{n - k} (x - x_{0})^{n - k} = \frac{C^{k + 1} }{(1 - |x - x_{0}| C)^{k}},
	\]
	whenever $|x - x_{0}| C < 1$. By the mean value theorem for divided differences it follows that we get required bound for some neighbourhood of $x_{0}$ and consequently, by compactness for any closed subinteval of $(a, b)$.
\end{proof}

Of course, we could just as well replace the closed inteval by any compact compact subset of $(a, b)$. The previous result is some kind of relative of \ref{bounded_div}. Also theorem \ref{k-tone_smooth} has rather interesting relative.

\begin{lause}[Bernstein's little theorem]
	If $f : (a, b) \to \R$ is $k$-tone for every $k \geq 0$, then $f$ is real-analytic on $(a, b)$.
\end{lause}
\begin{proof}
	We prove that the conditions of the theorem \ref{div_anal} are satisfied. Pick any $a < x_{0} < x < b$. Now for any $n \geq 0$ we have
	\[
		f(x) = \sum_{k = 0}^{n - 1} \frac{f^{(k)}(x_{0})}{k!}(x - x_{0})^{k} + \frac{f^{(n)}(x_{0})}{n!} (x - x_{0})^{n} + [x, x_{0}, x_{0}, \ldots, x_{0}]_{f} (x - x_{0})^{n + 1}.
	\]
	Note that all the terms on the right-hand side are non-negative, and hence
	\[
		0 \leq \frac{f^{(n)}(x_{0})}{n!} \leq f(x) (x - x_{0})^{-n}.
	\]
	Now given any interval $[c, d] \subset (a, b)$ we can make such estimate uniform over $x_{0} \in [c, d]$ simply by picking $x \in (d, b)$, and we are done.
\end{proof}


\section{TODO}
\begin{itemize}
	\item Mean value theorem, coefficient of the interpolating polynomial
	\item Basic properties, product rule.
	\item $k$-tone functions, smoothness, and representation
	\item Majorization, Jensen and Karamata inequalities, generalizations, and corollaries concerning spectrum and trace functions. Schur-Horn conjectures and Honey-Combs
	\item Tohoku contains nice proof of Lidskii inequality
	\item How to understand the inequalities arising from $k$-tone functions: is there nice way to parametrize the tuples coming from the $k$-majorization.
	\item For $k = 3$ and $3$ numbers, it's all about the biggers number: one with the largest largest number dominates.
	\item The previous probably generalizes: for $k$-tone functions and $k$ numbers on both sides, with all polynomials of degree less than $k$ vanishing on both tuples, one with largest largest value dominates, or equivalently, it's all about the constant term. This is clearly necessary, by is it also sufficient? Should be: express the whole thing as an integral, differentiate with respect to the constant term, and finally interpret as a divided difference.
	\item What if we add more terms: is there simple characterization? Why have similar integral representation, and can probably differentiate: Maybe not, or one has to be really careful. Is there characterization with linear inequalities (in addition to the equalities)?
	\item Peano Kernels: Smoothness properties, Bernstein (?) polynomials as examples.
	\item Opitz formula
	\item Regularization techniques
	\item Notion of midpoint-convexity should generalize by regularization techniques.
	\item Should Legendre transform generalize to higher orders?  For smooth enough functions probably with derivatives being inverses of each other, but what is the correct definition? And is it of any use? Maybe differentiating $k - 2$ times and then having similar characterization. Is there higher order duality?
	\item Is there elementary transformations for $k$-tone Karamata?
	\item Divided-difference series for entire functions (Newton expansion)? For analytic function? When does it converge? When does it converge to the right function?
	\item Given domain $U \subset \C$ and analytic function $f : U \to \C$, determine all subsets $V \subset U$ such that there exists Newton series with some sequence $x_{1}, x_{2}, \ldots$ converging in $V$. This is very much related to logarithmic potentials and subharmonic functions: sequence, if say bounded for starters, corresponds to a radon measure. Indeed, take weak limit of radon measures averaged exprimental measures of first elements in the sequence, if the limit exists (if not...). Now if $f = \frac{1}{z}$ for starters, we have the logarithmic potential $U(z)$ and the Newton series converges whenever $U(z) < U(0)$.
	\item Harnack-type inequalities for derivatives of Pick functions?
	\item Smooth function is in $P(0, \infty)$ if it's negative of Laplace transform of Laplace transform of a measure on $[0, \infty)$?
	\item Are there better bounds for theorem \ref{bounded_div}?
\end{itemize}
