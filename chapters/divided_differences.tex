\chapter{$k$-tone functions}

\section{Motivation}

To understand the regularity properties of the matrix monotone functions we look at a closely related class of $k$-tone functions. $k$-tone functions are more or less functions with non-negative $k$:th derivative\footnote{The terminology is not very established, and such functions are also occasionally called $k$-convex.}. What should that mean?

We already know the perfect answer for the case $k = 1$: $1$-tone functions should be the increasing functions.

\begin{lause}\label{increasing_1tone}
	Let $f : (a, b) \to \R$ be differentiable. Then $f$ is increasing, if and only if $f'(x) \geq 0$ for every $x \in (a, b)$.
\end{lause}

\begin{proof}
	If $f$ is increasing, then all its divided differences, i.e. the quotients of the form
	\begin{align*}
		\frac{f(x) - f(y)}{x - y}
	\end{align*}
	for $x \neq y$ are non-negative. As derivatives are limits of such quotients, also they are non-negative at any point. Conversely, by the mean value theorem for every $x \neq y$ we may find $\xi$ such that
	\begin{align*}
		\frac{f(x) - f(y)}{x - y} = f'(\xi).
	\end{align*}
	Now if the derivatives are non-negative, so are the divided differences, so the function is increasing.
\end{proof}

While this proof by the mean value theorem works in more general setting, if $f \in C^{1}$, one has more instructive proof.\footnote{The following argument would also work with slightly weaker assumptions, but that's not important to us.}

\begin{proof}[Alternate proof for the theorem \ref{increasing_1tone} (in the case $f \in C^{1}(a, b)$)]
	If $f \in C^{1}(a, b)$, we may write
	\begin{align*}
		\frac{f(y) - f(x)}{y - x} = \frac{1}{y - x}\int_{x}^{y} f'(t) dt = \int_{0}^{1} f'(t x + (1 - t) y) dt.
	\end{align*}
	Note that on the right-hand side we have average of the derivative over the interval. This means that the claim can be translated to: continuous function is non-negative, if and only if its averages over all intervals are non-negative. But this is clear.
\end{proof}

This is a really powerful point of view. While one would like to say the increasing functions are the functions with non-negative derivative, that's a bit of a lie. Instead, one can say that they are the functions whose derivative is non-negative on average, and all the problems are gone. This should roughly mean that the derivative defines a positive distribution and it is hence a measure. Thus all increasing functions should be integrals of a positive measure (at least almost everywhere). Although this kind of thinking could be carried out, the details aren't important for us. The main point is that one should think that increasing functions, i.e. the $1$-tone functions are functions whose first derivative is a (positive) measure. The divided differences are an averaged (i.e. weak) way of talking about the positivity of the derivative (measure).

This is essentially the distributional way of thinking, and we could keep going and end up with the whole business of weak derivatives and stuff. But we don't have to: the plain averages suffice. We write
\begin{align*}
	[x, y]_{f} := \frac{f(x) - f(y)}{x - y},
\end{align*}
and say that $[\cdot, \cdot]_{f}$ is the (first) divided difference of $f$. The domain of $[\cdot, \cdot]_{f}$ should naturally be $(a, b)^{2}$ minus the diagonal. And of course, if $f \in C^{1}$, we should extend $[\cdot, \cdot]_{f}$ to the diagonal, as the derivative. Divided differences then becomes a continuous function on the whole set $(a, b)^2$.

Aside from capturing the first derivative, divided difference have two rather convenient properties.

\begin{itemize}
	\item For given $x$ and $y$, $f \mapsto [x, y]_{f}$ defines a linear map, which is continuous with the topology of pointwise convergence (i.e. the product topology).
	\item Divided differences are local in the sense that if $f$ and $g$ agree on $\{x, y\}$, divided differences agree.
\end{itemize}

These are the ways that divided difference is a compromise between the real derivative and the weak derivative. The first point says that one doesn't have worry too much, only about pointwise convergence, while the second says that things are still rather concrete.

What about the case $k = 2$? Again, we already know the perfect answer: $2$-tone functions should be the convex functions.

\begin{lause}\label{2-tone}
	Let $f : (a, b) \to \R$ be twice differentiable. Then $f$ is convex, if and only if $f^{(2)}(x) \geq 0$ for every $x \in (a, b)$.
\end{lause}

\begin{proof}
	While the result is true as stated, let us only prove the case $f \in C^{2}(a, b)$ (we'll come back to the more general case). Recall that $f$ is convex, if and only if for any $x, y \in (a, b)$ and $t \in [0, 1]$ we have
	\begin{align*}
		t f(x) + (1 - t) f(y) \geq f(t x + (1 - t)y).
	\end{align*}
	The alternate proof of theorem \ref{increasing_1tone} suggests that we may write
	\begin{align*}
		t f(x) + (1 - t) f(y) - f(t x + (1 - t)y) = \int_{x}^{y} w(t) f^{(2)}(t) dt
	\end{align*}
	for some weight $w$. Note that if we manage to find such weight, which is non-negative (and positive enough), we would be done.

	How to find the weight $w$? The idea is rather simple: we want to ``sieve out" the values of $w$ by choosing $f$ such that $f^{(2)} = \delta_{a}$ for $a \in \R$ (in some sense). Now, this should mean that $f(t) = (t - a)_{+} + c t + d$ for some $c, d \in \R$, where we write $t_{+} = \max(t, 0)$. Plugging this is on the left hand side we get
	\begin{align*}
		t (x - a)_{+} + (1 - t) (y - a)_{+} - (t x + (1 - t) y - a)_{+} = w(a).
	\end{align*}
	Now, while the steps taken might have contained some leaps of faith, it can be easily verified with partial integration that the given $w$ really works.
\end{proof}

The giveaway is that while the divided differences are a convenient averaged way to talk about first derivative, the quantity $tf(x) + (1 - t) f(y) - f(t x + (1 - t)y)$ is a convenient averaged way to talk about the second derivative. It captures the fact that the second derivative should be a positive measure -- without talking about derivatives. We won't call the quantity the second divided difference, however, as, as it turns out, we can rewrite it in much more convenient form.

If we denote $z = t x + (1 - t) y$, we can solve that $t = \frac{z - y}{x - y}$ and express
\begin{eqnarray*}
	&& t f(x) + (1 - t) f(y) - f(t x + (1 - t)y) \\
	&=& \frac{z - y}{x - y} f(x) + \frac{x - z}{x - y} f(y) - f(z) \\
	&=& -(z - y)(z - x) \left(\frac{f(x)}{(x - y)(x - z)} + \frac{f(y)}{(y - z)(y - x)} + \frac{f(z)}{(z - x)(z - y)} \right)
\end{eqnarray*}

If $t \notin \{0, 1\}$, $-(z - y)(z - x)$ is positive, so if $f$ is convex,
\begin{align*}
	\frac{f(x)}{(x - y)(x - z)} + \frac{f(y)}{(y - z)(y - x)} + \frac{f(z)}{(z - x)(z - y)} \geq 0
\end{align*}
for any $x, y$ and $z$ such that $z$ is between $x$ and $y$. This new expression is symmetric in its variables, so actually there's no need to assume anything on the order of $x, y$ and $z$, just that they're distinct. We can also easily carry this argument to the other direction: if the expression is non-negative for any distinct $x, y$ and $z$, then $f$ is convex. This motivates us to define
\begin{align*}
	[x, y, z]_{f} := \frac{f(x)}{(x - y)(x - z)} + \frac{f(y)}{(y - z)(y - x)} + \frac{f(z)}{(z - x)(z - y)}
\end{align*}
as the second divided difference of $f$.

One would hope that by setting
\begin{align*}
	[x_{0}, x_{1}, \ldots, x_{n}]_{f} := \sum_{i = 0}^{n} \frac{f(x_{i})}{\prod_{j \neq i} (x_{i} - x_{j})},
\end{align*}
one obtains something that naturally generalizes divided differences for higher orders. This is indeed the case.

\section{Divided differences}

Define $D_{n} = \{x\in \R^{n} | x_{i} = x_{j} \text{ for some $1 \leq i < j \leq n$}\}$.
\begin{maar}
Let $n \geq 0$. For any $f : (a, b) \to \R$ we define the corresponding $n$:th divided difference $[\cdots]_{f} : (a, b)^{n + 1} \setminus D_{n + 1}$ by setting
\begin{align*}
	[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \sum_{i = 0}^{n} \frac{f(x_{i})}{\prod_{j \neq i} (x_{i} - x_{j})}.
\end{align*}
\end{maar}

We will soon prove that divided differences (of order $n$) are simply weighted averages of the $n$:th derivative.

\subsection{Basic properties}

Divided differences have the following important properties.

\begin{prop}
	Divided differences are symmetric in the variable, i.e. for any $f : (a, b) \to \R$ and pairwise distinct $a < x_{0}, x_{1}, \ldots, x_{n} < b$ permutation $\sigma : \{0, 1, 2, \ldots, n\} \to \{0, 1, 2, \ldots, n\}$ we have
	\begin{align*}
		[x_{0}, x_{1}, x_{2}, \ldots, x_{n}]_{f} = [x_{\sigma(0)}, x_{\sigma(1)}, x_{\sigma(2)}, \ldots, x_{\sigma(n)}]_{f}.
	\end{align*}
	If $f$ is continuous, so are the divided differences.
	Finally, for fixed (pairwise distinct) $a < x_{0}, x_{1}, \ldots, x_{n} < b$ the map $[x_{0}, x_{1}, \ldots, x_{n}]_{\cdot} : \R^{(a, b)} \to \R$ is linear and continuous (with respect to the topology of pointwise convergence).
\end{prop}
\begin{proof}
	Easy to check.
\end{proof}

The name ``divided differences" stems from the fact that the higher order divided differences are itself (usual) divided differences of lower order ones.

\begin{prop}\label{nesting_property}
	For any $f : (a, b) \to \R$ and pairwise distinct $x_{0}, x_{1}, \ldots, x_{n} \in (a, b)$ we have
	\begin{align}\label{divdif_rec}
		[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \frac{[x_{0}, x_{1}, \ldots, x_{n - 1}]_{f} - [x_{1}, x_{2}, \ldots, x_{n}]_{f}}{x_{0} - x_{n}} = [x_{0}, x_{n}]_{[\cdot, x_{1}, \ldots, x_{n - 1}]_f}
	\end{align}
	More generally, for any pairwise distinct $x_{1}, x_{2}, \ldots, x_{n}, y_{0}, y_{1}, y_{2}, \ldots, y_{m} \in (a, b)$ we have
	\begin{align}\label{nesting_rule}
		[y_{0}, y_{1}, y_{2}, \ldots, y_{m}]_{[\cdot, x_{1}, x_{2}, \ldots, x_{n}]_{f}} = [y_{0}, y_{1}, y_{2}, \ldots, y_{m}, x_{1}, x_{2}, \ldots, x_{n}]_{f}.
	\end{align}
\end{prop}
\begin{proof}
	\ref{divdif_rec} is easy to check directly. For \ref{nesting_rule} note that both
	\begin{align*}
		[y_{1}, y_{2}, \ldots, y_{m}]_{[\cdot, x_{1}, x_{2}, \ldots, x_{n}]_{f}} \text{ and } [y_{1}, y_{2}, \ldots, y_{m}, x_{1}, x_{2}, \ldots, x_{n}]_{f}
	\end{align*}
	satisfy \ref{divdif_rec} (as a function of the $y$'s) and they agree when $m = 1$.
\end{proof}

We call \ref{nesting_rule} the \textit{nesting property} of divided differences. Although the analogy isn't perfect, one could think that this identity says that $m$:th derivative of the $n$:th derivative is the $(m + n)$:th derivative. 

The following observation tells us that the divided differences work as $n$:th derivative insomuch that it kills polynomials of degree less than $n$, and works with degree $n$ as expected, up to a constant at least.

\begin{prop}\label{lagrange_divided}
We have $[x_{0}, x_{1}, \ldots, x_{n}]_{(x \mapsto x^{n})} = 1$ and $[x_{0}, x_{1}, \ldots, x_{n}]_{p} = 0$ for any polynomial p of degree at most $n - 1$. In other words, $[x_{0}, x_{1}, \ldots, x_{n}]_{f}$ is the leading coefficient of the interpolation polynomial on pairs $(x_{0}, f(x_{0})), (x_{1}, f(x_{1})), \ldots, (x_{n}, f(x_{n}))$.
\end{prop}
\begin{proof}
	As the interpolation polynomial of a polynomial of degree at most $n$ on a dataset of $(n + 1)$ pairs is the polynomial itself, the second claim readily implies the first. Recall that the Lagrange form of the interpolation polynomial of a dataset $(x_{0}, y_{0}), (x_{1}, y_{1}), \ldots, (x_{n}, y_{n})$ is given by
	\begin{align*}
		\sum_{i = 0}^{n} y_{i} \frac{\prod_{j \neq i}(x - x_{j})}{\prod_{j \neq i}(x_{i} - x_{j})},
	\end{align*}
	and the leading coefficient of this polynomial is exactly the divided difference.
\end{proof}

\subsection{Peano representation}

Coming back to the original motivation, divided differences enjoy an integral representation also for larger $n$.

\begin{lause}\label{peano_theorem}
	If $f \in C^{n}(a, b)$, then for any pairwise distinct $a < x_{0}, x_{1}, x_{2}, \ldots, x_{n} < b$ we have
	\begin{align}\label{peano_identity}
		[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \int_{\R} f^{(n)}(t) w(t) dt,
	\end{align}
	where
	\begin{align}\label{peano_weight}
		w(t) := w_{x_{0}, x_{1}, \ldots, x_{n}}(t) = \frac{1}{(n - 1)!}\sum_{i = 0}^{n} \frac{((x_{i} - t)_{+})^{n - 1}}{\prod_{j \neq i} (x_{i} - x_{j})}.
	\end{align}
	In addition, $w$ is non-negative, supported on $[\min(x_{i}), \max(x_{i})]$ and integrates to $(n!)^{-1}$.
\end{lause}
The case $n = 1$ of definition \ref{peano_weight} should be understood with convention $0^{0} = 0$.
\begin{proof}
	Note that the weight is simply the $n$:th divided difference of the map $g_{t, n} : x \mapsto ((x - t)_{+})^{n - 1}/(n - 1)!$. This is not very surprising: one should think that $g_{t, n}$ is the function whose $n$:th derivative is $\delta_{t}$. If we plug in $f = g_{t, n}$, (as in the proof of \ref{2-tone}), we, at least morally, get the claim. While the previous argument could be pushed through, we take safer route. To prove that the formula even makes sense, we should prove the claim on the support. It is clear that $w$ is zero whenever $t \geq \max(x_{i})$. If on the other hand $t \leq \min(x_{i})$, $w(t)$ agrees with the $n$:th divided difference of the map $x \mapsto (x - t)^{n - 1}/(n - 1)!$, which is zero by the proposition \ref{lagrange_divided}.

	We may hence repeatedly partially integrate the right-hand side:
	\begin{align*}
		 \int_{\R} f^{(n)}(t) w(t) dt &= \int_{\R} f^{(n - 1)}(t) (-1) w'(t) dt \\
		 &= \int_{\R} f^{(n - 2)}(t) w^{(2)}(t) dt \\
		 &= \ldots \\
		 &= \int_{\R} f^{(1)}(t) (-1)^{n} w^{(n - 1)}(t) dt,
	\end{align*}
	where
	\begin{align*}
		(-1)^{n} w^{(n - 1)}(t) = \sum_{i = 0}^{n} \frac{\chi_{(t, \infty)}(x_{i})}{\prod_{j \neq i} (x_{i} - x_{j})}.
	\end{align*}
	Note that $w^{(j)}$ is continuous, piecewise $C^{1}$, and compactly supported for every $0 \leq j < n - 1$, so the partial integration is legitimate. The final step is an easy calculation.

	Applying the identity to $x \mapsto x^{n}$ shows the claim on the integral of $w$.

	Only non-negativity remains: we prove it by induction on $n$. The case $n = 1$ is clear. The idea is rather simple: we should prove that the functions $g_{t, n}$ have non-negative divided differences, which should roughly mean it has non-negative $n$:th derivative (being $\delta_{t}$). By the nesting property we have
	\begin{align*}
		[x_{0}, x_{1}, \ldots, x_{n}]_{g_{t, n}} = [x_{0}, x_{1}, \ldots, x_{n - 1}]_{[\cdot, x_{n}]_{g_{t, n}}}.
	\end{align*}
	Now if we could replace $[\cdot, x_{n}]_{g_{t, n}}$ with the derivative of $g_{t, n}$, which is conveniently $g_{t, n - 1}$, we would be done by the induction hypothesis. Note that while these functions aren't the same in general, they agree (up to constant) if $x_{n} = t$. But if $x_{n} \neq t$, we can play the same game as before: $[\cdot, x_{n}]_{g_{t, n}}$ is weighted average of the derivative $g_{t, n}' = g_{t, n - 1}$. Indeed, as
	\begin{align*}
		[\cdot, x_{n}]_{g_{t, n}} = \int_{0}^{1} g_{t, n - 1}(s \cdot + (1 - s) x_{n}) ds,
	\end{align*}
	we have
	\begin{align*}
		[x_{0}, x_{1}, \ldots, x_{n - 1}]_{[\cdot, x_{n}]_{g_{t, n}}} = \int_{0}^{1} [x_{0}, x_{1}, \ldots, x_{n - 1}]_{g_{t, n - 1}(s \cdot + (1 - s) x_{n})} ds,
	\end{align*}
	Now since all the divided differences of $g_{t, n - 1}$ are non-negative, the same is clearly true for $g_{t, n - 1}(s \cdot + (1 - s) x_{n})$, so we are done.
\end{proof}

The weight \ref{peano_weight} is called \textbf{Peano kernel} (of order $n$). The points $x_{0}, x_{1}, \ldots, x_{n}$ are called the \textbf{nodes} of $w$.

As a very important corollary we get the following.

\begin{lause}[Mean value theorem for divided differences]
	Let $f \in C^{n}(a, b)$. Then for any pairwise distinct $x_{0}, x_{1}, \ldots, x_{n} \in (a, b)$ we have
	\begin{align*}
		\min_{0 \leq i \leq n}(x_{i}) < \xi < \max_{0 \leq i \leq n}(x_{i})
	\end{align*}
	such that
	\begin{align}\label{mean_value}
		[x_{0}, x_{1}, \ldots, x_{n}] = \frac{f^{(n)}(\xi)}{n!}.
	\end{align}
\end{lause}
\begin{proof}
	t.f.i.f. Theorem \ref{peano_theorem} and the mean value theorem for integrals.
\end{proof}
\begin{proof}[Alternate proof]
	By linearity and proposition \ref{lagrange_divided} it suffices to verify the claim in the case where $f(x_{i}) = 0$ for $0 \leq i \leq n$.
	\begin{lem}
		If $f$ is $n$ times differentiable, and has $n + 1$ roots, then $f^{(n)}$ has a root (in the interior of the convex hull of the roots).
	\end{lem}
	\begin{proof}
		If $f$ has $n + 1$ roots, by the mean value theorem its derivative has $n$ roots (in the interior of the convex hull of the roots of $f$) and is $(n - 1)$ times differentiable. Since the derivative satisfies the same assumptions for $n - 1$, the claim follows by induction.
	\end{proof}
\end{proof}

Note that the alternate proofs works even if $f$ is merely $n$ times differentiable.

The mean value theorem could be also used to prove the non-negativity of the weight $w$: if $w$ were somewhere negative, one could construct a function with non-negative derivative and negative divided difference, which would contradict \ref{mean_value}.

As in the case $n = 1$, if for $n > 1$ we can continuously extend divided differences to the set $D_{n + 1}$, we should do that; we identify the resulting function with the original one. We will later prove that, as expected, this can be done, if and only $f \in C^{n}(a, b)$. In this case by \ref{mean_value} the extesion satisfies
\begin{align*}
	[x_{0}, x_{0}, \ldots, x_{0}]_{f} = \frac{f^{(n)}(x_{0})}{n!},
\end{align*}
which together with \ref{divdif_rec} is enough to expand the divided differences with values of the function and its derivatives.

\subsection{Cauchy's integral formula}

Complex analysis offers a nice view on divided differences: if $f$ is analytic, we may interpret divided differences as contour integrals.

\begin{lem}[Cauchy's integral formula for divided differences]\label{div_cauchy}
If $\gamma$ is a simple closed counter-clockwise curve enclosing the numbers $x_{0}, x_{1}, \ldots, x_{n}$, we have
\begin{align*}
	[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \frac{1}{2 \pi i} \int_{\gamma} \frac{f(z)}{(z - x_{0})(z - x_{1}) \cdots (z - x_{n})} dz.
\end{align*}
\end{lem}
\begin{proof}
	This is a direct consequence of the residue theorem.
\end{proof}

If all the points coincide, we get the familiar formula for the $n$:th derivative. If $f$ is a polynomial of degree at most $n - 1$, the integrand decays as $|z|^{-2}$ (at infinity) and the divided differences vanish, as expected. Also, for $z \mapsto z^{n}$ one can use the formula to calculate the $n$:th divided difference with a residue at infinity. This formula is slightly more concisely expressed by writing $p_{X}(x) = \prod_{i = 0}^{n} (x - x_{i})$ for a sequence $X = (x_{i})_{i = 0}^{n}$. Now we have
\begin{align*}
	[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \frac{1}{2 \pi i} \int_{\gamma} \frac{f(z)}{p_{X}(z)} dz.
\end{align*}

Cauchy's integral formula is a convenient way to think about severel identities.

\begin{esim}
We may express an interpolation polynomial of an analytic function $f$ and sequence $X = (x_{i})_{i = 0}^{n}$ by
\begin{align*}
	P_{X, f}(x) := \frac{1}{2 \pi i} \int_{\gamma} \frac{p_{X}(x) - p_{X}(z)}{x - z}\frac{f(z)}{p_{X}(z)} dz = [x_{0}, x_{1}, \ldots, x_{n}]_{f [x, \cdot]_{p_{X}}}.
\end{align*}
Indeed: $P_{X, f}(x_{i})$ evaluates to $f(x_{i})$ by Cauchy's integral formula. More generally, if some of the points coincide, we get the Hermite interpolation polynomial, as can be shown with slightly more careful considerations.
\end{esim}

While the previous argument works stricly speaking only for analytic functions (and even then one would have to be careful with domains and $\gamma$), the identity holds more generally. The expression for $P_{X, f}$ can be expanded as some kind polynomial, coefficients of which are linear combinations of evaluations $f(x_{i})$: such expressions make perfect sense irrespective of the regularity of $f$. Same is true for the evaluations of this polynomial at points $x_{i}$, numbers $P_{X, f}(x_{i})$. We know that $P_{X, f}(x_{i}) = f(x_{i})$ holds for all analytic (or at least entire) functions. On the other hand the map $f \mapsto P_{X, f}(x_{i}) - f(x_{i})$ is simply a finite linear combination of point evaluations, so it vanishes for any function if we manage to prove that

\begin{lem}\label{entire_interpolation}
	For any pairwise distinct $x_{0}, x_{1}, \ldots, x_{n} \in \C$ and $y_{0}, y_{1}, \ldots, y_{n} \in \C$ there exists an entire function $f$ with $f(x_{i}) = y_{i}$.
\end{lem}
\begin{proof}
	Simply take $f$ to be the interpolating polynomial.
\end{proof}

One can also interpret such identities to be strictly formal: Cauchy's integral formula can be thought as a bijection between rational functions with simple poles and the span of $\delta$-measures. Such signed measures look often simpler as rational functions.

\subsection{Identities}

Many of the familiar identities for the derivatives have analogs with divided differences. We won't need these formulas, but it's nevertheless nice to know that there are such. Also, they are not really more complicated than the derivative counterparts. On the contrary; the author honestly thinks that they are in fact easier to remember. One of the downsides of the divided difference identities is however that they are usually not symmetric with respect to the sequence $x_{0}, x_{1}, \ldots, x_{n}$ anymore. That's life.

\begin{prop}\label{div_identities}
	Let $n$, $k$, $f, g$, $f_{1}, f_{2}, \ldots, f_{k}$ and $x_{0}, x_{1}, \ldots, x_{n}$ be such that the folliowing identities make sense.
	\begin{enumerate}[(i)]
		\item (Newton expansion)
		\begin{align}\label{newton_expansion}
			f(x) &= [x_{0}]_{f} + [x_{0}, x_{1}]_{f} (x - x_{0})  + [x_{0}, x_{1}, x_{2}]_{f} (x - x_{0}) (x - x_{1}) + \ldots \\
			& + [x_{0}, x_{1}, \ldots, x_{n}]_{f} (x - x_{0}) (x - x_{1}) \cdots (x - x_{n - 1}) \nonumber\\
			& + [x, x_{0}, x_{1}, \ldots, x_{n}]_{f} (x - x_{0}) (x - x_{1}) \cdots (x - x_{n}) \nonumber,
		\end{align}
		in particular, if the points coincide we get the familiar Taylor expansion
		\begin{align}\label{taylor_expansion}
			f(x) = \sum_{k = 0}^{n - 1} \frac{f^{(k)}(x_{0})}{k!} + [x, x_{0}, x_{0}, \ldots, x_{0}]_{f} (x - x_{0})^{n},
		\end{align}
		\item (Product rule)
		\begin{align*}
			[x_{0}, x_{1}]_{f g} = [x_{0}]_{f} [x_{0}, x_{1}]_{g} + [x_{0}, x_{1}]_{f} [x_{1}]_{g}.
		\end{align*}
		\item (Leibniz rule)
		\begin{align}\label{leibniz_rule}
			[x_{0}, x_{1}, \ldots, x_{n}]_{f g} &= [x_{0}]_{f} [x_{0}, \ldots, x_{n}]_{g} + [x_{0}, x_{1}]_{f} [x_{1}, \ldots, x_{n}]_{g} + \ldots\\
			&+ [x_{0}, x_{1}, \ldots, x_{n - 1}]_{f} [x_{n - 1}, x_{n}]_{g} + [x_{0}, x_{1}, \ldots, x_{n}]_{f} [x_{n}]_{g} \nonumber.
		\end{align}
		More generally
		\begin{align*}
			[x_{0}, x_{1}, \ldots, x_{n}]_{f_{1} f_{2} \cdots f_{k}} &= \sum_{0 = i_{0} < i_{1} < i_{2} < \ldots < i_{k - 1} < i_{k} = n} \prod_{j = 1}^{k} [x_{i_{j - 1}}, \ldots, x_{i_{j}}]_{f_{j}}
		\end{align*}
		\item (Chain rule)
		\begin{align*}
			[x_{0}, x_{1}]_{f \circ g} = [g(x_{0}), g(x_{1})]_{f} [x_{0}, x_{1}]_{g}
		\end{align*}
		\item (Fa\`{a} di Bruno formula)
		\begin{align*}
			& [x_{0}, x_{1}, \ldots, x_{n}]_{f \circ g} \\
			=& \sum_{k = 1}^{n} \sum_{0 = i_{0}< i_{1} < i_{2} < \ldots < i_{k - 1} < i_{k} =  n} [g(x_{i_{0}}), g(x_{i_{1}})\ldots , g(x_{i_{k}})]_{f} \prod_{j = 1}^{k} [x_{i_{j - 1}}, \ldots, x_{i_{j}}]_{g}
		\end{align*}
	\end{enumerate}
\end{prop}
\begin{proof}[Proof sketches]
	\begin{enumerate}[(i)]
		\item Easy induction using \ref{divdif_rec}. Notice that also this formula makes it clear that the divided difference agrees with the degree $n$ coefficient of the interpolating polynomial.
		\item Easy to check.
		\item Induction using the product rule (i.e. the case $n = 1$) and the nesting rule \ref{nesting_rule}. Alternatively one could write Newton expansions of both $f$ and $g$ with sequences $(x_{0}, x_{1}, \ldots, x_{n})$ and $(x_{n}, x_{n - 1}, \ldots, x_{0})$ and notice that the given sum gives exactly the leading term of the interpolating polynomial of $f g$. The more general case follows from the case of two functions by induction.
		\item Easy to check.
		\item A bit tedious induction using the Leibniz rule and \ref{divdif_rec}.
	\end{enumerate}
\end{proof}

One might ask under what kind of conditions \ref{newton_expansion} leads to Newton series, i.e. we have
\begin{align}\label{newton_series}
	f(z) = \sum_{i = 0}^{\infty} [z_{0}, z_{1}, \ldots, z_{i}]_{f} (z - z_{0}) (z - z_{1}) \cdots (z - z_{i - 1}),
\end{align}
for some sequence $z_{0}, z_{1}, \ldots$ and analytic $f$. While Newton series can be globally rather subtle, locally they work almost like Taylor series.

\begin{prop}
	Let $z_{\infty} \in \C$, $\rho > 0$, $f : \D(z_{\infty}, \rho) \to \C$ analytic and $z_{0}, z_{1}, \ldots \in \D(z_{\infty}, \rho)$ sequence converging to $z_{\infty}$. Then \ref{newton_series} holds for any $z \in \D(z_{\infty}, \rho)$.
\end{prop}
\begin{proof}
	We need to verify that the error term $[z, z_{0}, z_{1}, \ldots, z_{n}]_{f} (z - z_{0}) (z - z_{1}) \cdots (z - z_{n})$ in Newton expansion tends to zero as $n \to \infty$. But for any $\rho' < \rho$ such that $z, z_{0}, z_{1}, \ldots \in \D(z_{\infty}, \rho')$ we have
	\begin{align*}
		&[z, z_{0}, z_{1}, \ldots, z_{n}]_{f} (z - z_{0}) (z - z_{1}) \cdots (z - z_{n}) \\
		= & \frac{1}{2 \pi i} \int_{\partial \D(z_{\infty}, \rho')} \frac{(z - z_{0}) (z - z_{1}) \cdots (z - z_{n})}{(w - z)(w - z_{0}) (w - z_{1}) \cdots (w - z_{n})} f(w) dw.
	\end{align*}
	As $z_{n} \to z_{\infty}$, the absolute values of the quotients $(z - z_{n})/(w - z_{n})$ tend to $|z - z_{\infty}|/|\rho'| < 1$ uniformly on $\partial \D(z_{\infty}, \rho')$ and hence the integrand tends uniformly to $0$.
\end{proof}

In similar vein one could prove that if $f$ is entire, its Newton series converge whenever $(z_{i})_{i \geq 0}$ is bounded. Note that \ref{newton_series} easily implies the identity theorem for analytic functions.


\subsection{$k$-tone functions}

\begin{maar}
	Function $f : (a, b) \to \R$ is called $k$-tone if for any $x_{0}, x_{1}, \ldots, x_{k} \in (a, b)$ of distinct points we have
	\begin{align*}
		[x_{0}, x_{1}, \ldots, x_{k}]_{f} \geq 0,
	\end{align*}
	i.e. the $k$:th divided difference is non-negative.
\end{maar}

We denote the space of $k$-tone functions by on interval $(a, b)$ by $P^{(k)}(a, b)$.

\begin{lause}
	Let $k$ be an non-negative integer. Then $P^{(k)}(a, b) \subset \R^{(a, b)}$ is a closed convex cone.
\end{lause}
\begin{proof}
	t.f.i.f Theorem \ref{positive_machine}.
\end{proof}

Mean value theorem tells us that $C^{k}$ $k$-tone functions are exactly the functions with non-negative $k$:th derivative.

The cones $P^{(k)}(a, b)$ aren't quite salient. Instead we have
\begin{align*}
	[\cdot, \cdot, \ldots, \cdot]_{f} = 0 \Leftrightarrow \text{ $f$ is a polynomial of degree less than $k$}.
\end{align*}

This suggests that a better object of study should be $\R^{(a, b)}$ quotiented by polynomials of degree less than $k$. We won't follow that trail, however.

\section{Locality}

One of the properties of the divided differences, which might not be clear from the definition, is that they can also be used to model local phenomena. One of the important properties of the $k$-tone functions is that if a function is $k$-tone on two overlapping intervals, then the function is $k$-tone on their union. While this definitely holds for $C^{k}$ functions, it's not really clear how to change the argument for the general case.

If one thinks that $k$-tone functions have $k$:th derivative as a positive measure, the locality property should be thought of a special case of a general property of distributions.
\begin{prop}
	Let $a < c < b < d$ and $\mu$ distribution on $(a, d)$, restriction of which to $(a, b)$ and $(c, d)$ is a positive measure. Then $\mu$ is a positive measure.
\end{prop}
\begin{proof}
	We should prove that $\mu(f)$ is non-negative for every non-negative test function $f$ on $(a, d)$. But every such function can be written as sum of two non-negative test functions, $f_{1}$ and $f_{2}$, $f_{1}$ supported on $(a, b)$ and $f_{2}$ on $(c, d)$, so $\mu(f) = \mu(f_{1}) + \mu(f_{2}) \geq 0$ by the assumption.
\end{proof}

They key idea in the proof was to split the test functions to two parts, one supported on $(a, b)$ and one on $(c, d)$. One can do the same with Peano kernels, except larger the order $k$, the more parts we need. 

\begin{lem}\label{peano_splitting_theorem}
	Let $a < c < b < d$ be reals and $w$ a Peano kernel supported on $(a, d)$. Then $w$ can written as a (finite) weighted average of Peano kernels, all of which are supported either on $(a, b)$ or on $(c, d)$.
\end{lem}
\begin{proof}
	Let $n$ be the order of the Peano kernel and let $a < x_{0} < x_{1} < \ldots < x_{n} < d$ be the nodes of $w$.

	The case $n = 1$ is rather clear: we simply split characteristic function of an interval to characteristic function of two intervals. In terms of the kernels, if $a < x_{0} < c < b < x_{1} < d$, we can pick $c < y_{0} < b$ and write
	\begin{align*}
		w_{x_{0}, x_{1}} = \frac{y_{0} - x_{0}}{x_{1} - x_{0}} w_{x_{0}, y_{0}} + \frac{x_{1} - y_{0}}{x_{1} - x_{0}} w_{y_{0}, x_{1}}:
	\end{align*}
	this is a sought decomposition.

	The case $n = 2$ is not much harder. Peano kernels of order $2$ are essentially triangles sitting on $x$-axis, corners of which have distinct $x$-coordinates. So we have one such triangle and we should split it to smaller triangles in such a way that

	\begin{itemize}
		\item No triangle has two equal $x$-coordinates.
		\item All triangles have all their corners' $x$-coordinates either on $(a, b)$ and $(c, d)$.
	\end{itemize}
	We call such triangles good. While the above picture should be rather convincing already, one can write an general algorithm generating such decomposition.

	\noindent \textbf{Input:} A triangle (Peano kernel of order $2$) supported on $(a, d)$. \\
	\noindent \textbf{Output:} A decomposition of the input as a positive linear combination of triangles all supported completely either on $(a, b)$ or $(c, d)$.
	\begin{enumerate}[\textbf{Step} 1.]
		\item If the triangle is good already, we are done.
		\item Pick $y_{0} \in (c, b)$, which does not coincide with any of the $x_{0}, x_{1}, x_{2}$.
		\item Divide the triangle into two triagles with $x$-coordinates $(x_{0}, x_{1}, y_{0})$ and $(x_{1}, x_{2},  y_{0})$.
		\item Run this algorithm recursively for these two triangles.
	\end{enumerate}
	Why does this algorithm terminate? Note that if any of the $x_{i}$'s are in $(c, b)$, the triangle is either good or $x_{1} \in (c, b)$ (or maybe both). In the former case we are done, and in the latter the two parts of the split are both good. If none of $x_{i}$'s are in $(c, b)$, both of the parts of the split has a coordinate in $(c, b)$, so also this case leads to a good split. In other words we can keep splitting triangles in such a way that they either become good or they have more nodes on $(c, b)$.

	It's easy to verify that splitting the triangle corresponds to the identity
	\begin{align*}
		w_{x_{0}, x_{1}, x_{2}} = \frac{y_{0} - x_{0}}{x_{2} - x_{0}} w_{x_{0}, x_{1}, y_{0}} + \frac{x_{2} - y_{0}}{x_{2} - x_{0}} w_{x_{1}, x_{2}, y_{0}}.
	\end{align*}

	When $n > 2$, the geometric picture is largely lost (at least by the author), but the algebra generalizes perfectly: we can still split Peano kernels using the following identity:
	\begin{align}\label{peano_splitting}
		w_{x_{0}, x_{1}, \ldots, x_{n}} = \frac{y_{0} - x_{0}}{x_{n} - x_{0}} w_{x_{0}, x_{1}, \ldots, x_{n - 1}, y_{0}} + \frac{x_{n} - y_{0}}{x_{n} - x_{0}} w_{x_{1}, \ldots, x_{n}, y_{0}}.
	\end{align}
	Where does this come from? Recall that the Peano kernels are just the divided differences of the functions $g_{t, n} = ((\cdot - t)_{+})^{n - 1}/(n - 1)!$. The first identity immediately generalizes to
	\begin{align*}
		[x_{0}, x_{1}]_{f} = \frac{y_{0} - x_{0}}{x_{1} - x_{0}} [x_{0}, y_{0}]_{f} + \frac{x_{1} - y_{0}}{x_{1} - x_{0}} [y_{0}, x_{1}]_{f},
	\end{align*}
	where $f$ is now any function. By the nesting property the identity \ref{peano_splitting} is nothing more than the previous identity applied to $f = [\cdot, x_{1}, \ldots, x_{n - 1}]_{g_{t, n}}$. Note that we need $x_{0} < y_{0} < x_{n}$, so that the weighted average is really a convex combination.

	Now we are ready to generalize the algorithm to bigger $n$:

	\noindent \textbf{Input:} A Peano kernel of order $n$ supported on $(a, d)$. \\
	\noindent \textbf{Output:} A Decomposition of the input as a positive linear combination of Peano kernels of order $n$, all supported completely either on $(a, b)$ or $(c, d)$.
	\begin{enumerate}[\textbf{Step} 1.]
		\item If the kernel is good already, we are done.
		\item Pick $y_{0} \in (c, b)$, which does not coincide with any of the $x_{0}, x_{1}, \ldots, x_{n}$.
		\item Divide the kernel to two kernels with nodes $(x_{0}, x_{1}, \ldots, y_{0})$ and $(x_{1}, \ldots, x_{n}, y_{0})$ as in the \ref{peano_splitting}.
		\item Run this algorithm recursively for these two kernels.
	\end{enumerate}
	This algorithm terminates basically because of the same reason: if the kernel isn't good already, the two splits have more nodes on $(c, b)$, and this quantity cannot increase forever.
\end{proof}

While this property is of independent interest, the real use of it is its generalization to divided differences.

\begin{lem}\label{divdif_splitting_theorem}
	Let $a < c < b < d$ be reals and $x_{0}, x_{1}, \ldots, x_{n} \in (a, d)$. Then we may find $N, M \in \N$, sequences $(y_{0, i}, \ldots, y_{n, i})$ and $(z_{0, j}, \ldots, z_{n, j})$ and numbers $p_{i}$ and $q_{j}$ for $1 \leq i \leq N$ and $1 \leq j \leq M$, such that
	\begin{itemize}
		\item $\sum_{i = 1}^{N} p_{i} + \sum_{j = 1}^{M} q_{j} = 1$ and $p_{i}, q_{j} \geq 0$ for every $1 \leq i \leq N$ and $1 \leq j \leq M$.
		\item $y_{0, i}, y_{1, i}, \ldots, y_{n, i}$ are pairwise distinct elements of $(a, b)$ for every $1 \leq i \leq N$.
		\item $z_{0, j}, z_{1, j}, \ldots, z_{n, j}$ are pairwise distinct elements of $(c, d)$ for every $1 \leq j \leq M$.
		\item For every $f : (a, d) \to \R$ we have
		\begin{align*}
			[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \sum_{i = 1}^{N} p_{i} [y_{0, i}, y_{1, i}, \ldots, y_{n, i}]_{f} +  \sum_{j = 1}^{M} q_{j} [z_{0, j}, z_{1, j}, \ldots, z_{n, j}]_{f}.
		\end{align*}
	\end{itemize}
\end{lem}
\begin{proof}
	Proof is almost identical to that of the Lemma \ref{peano_splitting_theorem}: we more or less just replace the identity \ref{peano_splitting} by
	\begin{align}\label{divdif_splitting_property}
		[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \frac{y_{0} - x_{0}}{x_{n} - x_{0}} [x_{0}, x_{1}, \ldots, x_{n - 1}, y_{0}]_{f} + \frac{x_{n} - y_{0}}{x_{n} - x_{0}} [x_{1}, \ldots, x_{n}, y_{0}]_{f},
	\end{align}
	which is valid because of essentially the same reasoning, and replace the word ``kernel" with word ``tuple".
\end{proof}

The proof of \ref{peano_splitting_theorem} gives that we may take $N$ and $M$ (in \ref{divdif_splitting_theorem}) with $N + M \leq 2^{n}$. With slightly more careful argument one can achieve $M + N \leq n + 1$.

We are now ready to prove the locality property of the $k$-tone functions.

\begin{prop}\label{k_tone_local}
	$P^{(k)}$ is a local property i.e. $P^{(k)}(a, b) \cap P^{(k)}(c, d) \subset P^{(k)}(a, d)$ for any $-\infty \leq a \leq c < b \leq d \leq \infty$. To be more precise, if $f : (a, d) \to \R$ such that $\restr{f}{(a, b)} \in P^{(k)}(a, b)$ and $\restr{f}{(c, d)} \in P^{(k)}(c, d)$, then $f \in P^{(k)}(a, d)$.
\end{prop}
\begin{proof}
	t.f.i.f. Lemma \ref{divdif_splitting_theorem}.
\end{proof}

Note that we could have also used the splitting property \ref{divdif_splitting_property} to slightly simplify the proof of Theorem \ref{peano_theorem}. Recall that in the induction step we were to prove that the function $g_{t, n}$ is $n$-tone for any $n \geq 1$. We also observed that by the induction hypothesis
\begin{align*}
	[x_{0}, x_{1}, \ldots, x_{n - 1}, t]_{g_{t, n}} = \frac{1}{n - 1}[x_{0}, x_{1}, \ldots, x_{n - 1}]_{g_{t, n - 1}} =  \frac{1}{n - 1} w_{x_{0}, \ldots, x_{n - 1}}(t)\geq 0
\end{align*}
for any $x_{0}, x_{1}, \ldots, x_{n - 1}$. But this readily implies that the divided differences are non-negative on all tuples as we have
\begin{align*}
	& [x_{0}, x_{1}, \ldots, x_{n - 1}, x_{n}]_{g_{t, n}} \\
	=& \frac{t - x_{0}}{x_{n} - x_{0}} [x_{0}, x_{1}, \ldots, x_{n - 1}, t]_{g_{t, n}} + \frac{x_{n} - t}{x_{n} - x_{0}} [x_{1}, \ldots, x_{n}, t]_{g_{t, n}} \\
	=& \frac{1}{n - 1} \frac{t - x_{0}}{x_{n} - x_{0}} [x_{0}, x_{1}, \ldots, x_{n - 1}]_{g_{t, n - 1}} + \frac{1}{n - 1}\frac{x_{n} - t}{x_{n} - x_{0}} [x_{1}, \ldots, x_{n}]_{g_{t, n - 1}} \\
	\geq & 0.
\end{align*}
Of course, this approach only works if $\min(x_{i}) \leq t \leq \max(x_{i})$, but if this is not the case, the divided differences are zero anyway. The previous identity can be also used to recursively compute Peano kernels.

\begin{huom}
	The above arguments are a bit awkward and one might be tempted to think that one should instead consider positive linear combinations of Peano kernels, called (positive) splines, to get better analogue for the ``locality of distributions" -proof. For $k \leq 3$ positive splines can be also characterized as suitable non-negative piecewise polynomial functions, but for $k > 3$ there's a problem: positive splines are merely subclass thereof. See \cite{deBoor} for details.
\end{huom}

\section{Regularity}

The real power of the divided differences comes in when they are used to carry regularity information.

\begin{lause}\label{k-tone_smooth}
	Let $k \geq 2$. Then $f \in P^{(k)}(a, b)$, if and only if $f \in C^{k - 2}(a, b)$ and $f^{(k - 2)}$ is convex, i.e. $2$-tone.
\end{lause}
\begin{proof}[``Proof"]
	Let $f \in P^{(k)}(a, b)$. Since $f^{(k)}$ is a positive measure, $f^{(k - 1)}$ is increasing and $f^{(k - 2)}$ is convex. As convex functions are continuous, we are done with $\Rightarrow$. Conversely, if $f \in C^{k - 2}(a, b)$ and $f^{(k - 2)}$ is convex, then $f^{(k - 2)}$ has second derivative as a positive measure. But this measure is also the $k$:th derivative of $f$, so $f \in P^{(k)}(a, b)$.
\end{proof}

In this section we will translate the previous argument to the language of the divided differences.

The first step is to connect the divided differences of a function to the divided differences (of one lower order) of the derivative.

\begin{lem}
	Let $f \in C^{1}(a, b)$. Then for any (pairwise distinct) $x_{0}, x_{1}, \ldots, x_{n} \in (a, b)$ we have
	\begin{align}\label{derivative_divdif}
		[x_{0}, x_{1}, \ldots, x_{n - 1}]_{f'} = \sum_{i = 0}^{n - 1} [x_{0}, x_{1}, \ldots, x_{i - 1}, x_{i}, x_{i}, x_{i + 1}, \ldots, x_{n - 1}]_{f}
	\end{align}
	and
	\begin{align}\label{integral_divdif}
		[x_{0}, x_{1}, \ldots, x_{n}]_{f} &= \int_{0}^{1} [x_{0}, x_{1}, \ldots, x_{n - 1}]_{f'(s \cdot + (1 - s) x_{n})} ds \\
		&= \int_{0}^{1} [s x_{0} + (1 - s) x_{n}, \ldots, s x_{n - 1} + (1 - s) x_{n}]_{f'} s^{n - 1} d s \nonumber.
	\end{align}
\end{lem}
\begin{proof}
	Note that divided differences of $f$ have repeated entries in the first identity. As mentioned, these values of the divided difference are defined as a continuous extension. We will take the existence of this extension given for now.

	We have
	\begin{align*}
		[x_{0}, x_{1}, \ldots, x_{n - 1}]_{f'} &= \lim_{h \to 0} [x_{0}, x_{1}, \ldots, x_{n - 1}]_{\frac{f(\cdot + h) - f(\cdot)}{h}} \\
		&= \lim_{h \to 0} \frac{[x_{0}, x_{1}, \ldots, x_{n - 1}]_{f(\cdot + h)} -[x_{0}, x_{1}, \ldots, x_{n - 1}]_{f}}{h} \\
		&= \lim_{h \to 0} \frac{[x_{0} + h, x_{1} + h, \ldots, x_{n - 1} + h]_{f} -[x_{0}, x_{1}, \ldots, x_{n - 1}]_{f}}{h}.
	\end{align*}
	Now the approach is basically the same as with differentiation of multivariate functions: we write the difference as sum of $n$ telescoping differences, where only one of the entries is changed at a time.
	\begin{align*}
		& \lim_{h \to 0} \frac{[x_{0} + h, x_{1} + h, \ldots, x_{n - 1} + h]_{f} -[x_{0}, x_{1}, \ldots, x_{n - 1}]_{f}}{h} \\
		=& \lim_{h \to 0} \left(\frac{[x_{0} + h, x_{1} + h, \ldots, x_{n - 1} + h]_{f} - [x_{0} + h, x_{1} + h, \ldots, x_{n - 2} + h,  x_{n - 1}]_{f}}{h} \right. \\
		+& \frac{[x_{0} + h, x_{1} + h, \ldots, x_{n - 2} + h, x_{n - 1}]_{f} - [x_{0} + h, x_{1} + h, \ldots, x_{n - 2}, x_{n - 1}]_{f}}{h} \\
		+& \ldots \\
		+& \left. \frac{[x_{0} + h, x_{1}, \ldots, x_{n - 1}]_{f} - [x_{0}, x_{1}, \ldots, x_{n - 1}]_{f}}{h} \right)\\
		&= \lim_{h \to 0} \left(\sum_{i = 0}^{n} [x_{0} + h, \ldots, x_{i - 1} + h, x_{i} + h, x_{i}, x_{i + 1}, \ldots, x_{n - 1}]_{f} \right).
	\end{align*}
	Now assuming the claim on the continuity, the limit is exactly what we wanted.

	First equality of second claim was already essentially proved in the proof of Theorem \ref{peano_theorem}; the second is a simple computation.
\end{proof}

Analogous telescoping trick gives also the following identity.
\begin{prop}\label{total_derivative}
	Let $x_{0}, x_{1}, \ldots, x_{n}$ and $y_{0}, y_{1}, \ldots, y_{n}$ be pairwise distinct points on $(a, b)$. Then for any $f : (a, b) \to \R$ we have
	\begin{align}
		[y_{0}, y_{1}, \ldots, y_{n - 1}]_{f} - [x_{0}, x_{1}, \ldots, x_{n - 1}]_{f} = \sum_{i = 0}^{n - 1} [x_{0}, \ldots, x_{i - 1}, x_{i}, y_{i}, y_{i + 1}, \ldots, y_{n - 1}]_{f} (y_{i} - x_{i}).
	\end{align}
\end{prop}

Next step is to connect the regularity of divided differences to regularity of divided differences of the derivative. Denote
\begin{align*}
	D_{n, m} = \{x \in \R^{n} | x_{i_{1}} = x_{i_{2}} = \ldots = x_{i_{m}} \text{ for some $1 \leq i_{1} < i_{2} < \ldots < i_{m} \leq n$} \}.
\end{align*}

Note that $D_{n + 1, 2}$ is exactly the set where the divided differences aren't defined. Still, if $f$ is smooth enough, we should be able to continuously extend the divided differences to this set, or at least to some subset set of it. This thinking leads to the following notion of the regularity of a function.

\begin{maar}
	Let $f : (a, b) \to \R$ and $k \geq 0$. We call $f$ \textit{weakly $C^{k}$} (on $(a, b)$), or write $f \in C_{w}^{k}(a, b)$, if its order $k$ divided differences can be continuously extended to $(a, b)^{k + 1}$.
\end{maar}

Since $(a, b)^{k + 1} \setminus D_{k + 1, 2}$ in dense in $(a, b)^{k + 1}$, such extension is necessarily unique.

Our aim is to prove that a function is weakly $C^{k}$, if and only if it's $C^{k}$. This trivially holds for $k = 0$.

\begin{lem}\label{weaklyCk_basic}
	Let $n \geq k$. Then $f \in C_{w}^{k}(a, b)$, if and only if the order $n$ divided differences of $f$ extend continuously to $(a, b)^{n + 1} \setminus D_{n + 1, k + 2}$.
\end{lem}
\begin{proof}
	Let us denote
	\begin{align*}
		S(n, k, f) = \text{``order $n$ divided differences of $f$ extend continuously to $(a, b)^{n + 1} \setminus D_{n + 1, k + 2}$"}.
	\end{align*}
	As $S(k, k, f)$ is just saying that $f \in C_{w}^{k}(a, b)$, it is enough to prove that for any $n > k$ we have $S(n - 1, k, f) \Leftrightarrow S(n, k, f)$.

	$\Rightarrow$: Assume $S(n - 1, k, f)$ and take any $x = (x_{0}, x_{1}, \ldots, x_{n}) \in (a, b)^{n + 1} \setminus D_{n + 1, k + 2}$. Since $n + 1 \geq k + 2$, we may assume that $x_{0} \neq x_{n}$. But now by the assumption the map
	\begin{align*}
		(y_{0}, y_{1}, \ldots, y_{n}) \mapsto \frac{[y_{0}, y_{1}, \ldots, y_{n - 1}]_{f} - [y_{1}, y_{2}, \ldots, y_{n}]_{f}}{y_{0} - y_{n}}
	\end{align*}
	extends continuously to some neighbourhood of $x$ (in $(a, b)^{n + 1} \setminus D_{n + 1, k + 2}$), and since this map agrees with the $n$:th order divided differences on $(a, b)^{n + 1} \setminus D_{n + 1, 2}$ by \ref{divdif_rec}, it gives the continuous extension to $x$.

	$\Leftarrow$: Assume then $S(n, k, f)$. Take any $x = (x_{0}, x_{1}, \ldots, x_{n - 1}) \in (a, b)^{n} \setminus D_{n, k + 2}$ and additional point $(z_{0}, z_{1}, \ldots, z_{n - 1})$ with pairwise distinct components and $x_{i} \neq z_{j}$ for $0 \leq i, j \leq n - 1$. Now the map
	\begin{align*}
		(y_{0}, \ldots, y_{n - 1}) \to [z_{0}, z_{1}, \ldots, z_{n - 1}]_{f} + \sum_{i = 0}^{n - 1}[z_{0}, \ldots, z_{i - 1} z_{i}, y_{i}, y_{i + 1}, \ldots, y_{n - 1}]_{f} (y_{i} - z_{i})
	\end{align*}
	is continuous on some neighbourhood of $x$ in $(a, b)^{n} \setminus D_{n, k + 2}$, and since it agrees with the order $n$ divided differences on $(a, b)^{n} \setminus D_{n, 2}$ (by proposition \ref{total_derivative}), it gives the extension at $x$.
\end{proof}

\begin{lem}\label{weaktoreal_lemma}
	Let $f : (a, b) \to \R$, $k \geq 1$. Then $f \in C_{w}^{k}(a, b)$, if and only if $f \in C^{1}(a, b)$ and $f' \in C_{w}^{k - 1}(a, b)$.
\end{lem}

\begin{proof}
	``$\Rightarrow$": Let's start by proving that $f$ is continuously differentiable. Lemma \ref{weaklyCk_basic} easily implies that it is sufficient prove this for the case $k = 1$. But in this case we know that the limits $\lim_{x \to x_{0}} [x, x_{0}]_{f} = [x_{0}, x_{0}]_{f}$ exist and $f$ is hence differentiable with $f'(x) = [x, x]_{f}$. Also, $x \mapsto [x, x]_{f} = f'(x)$ is continuous.

	Now the identity \ref{derivative_divdif} easily implies the claim.

	``$\Leftarrow$": By \ref{integral_divdif} it suffices to prove that the map
	\begin{align*}
		(x_{0}, x_{1}, \ldots, x_{k}) \to \int_{0}^{1}[s x_{0} + (1 - s) x_{k}, \ldots, s x_{k - 1} + (1 - s) x_{k}]_{f'} s^{k - 1} d s.
	\end{align*}
	is continuous. Since $f' \in C_{w}^{k - 1}(a, b)$, the order $(k - 1)$ divided differences are uniformly continuous on any compact subset of $(a, b)^{k}$. The integrand is consequently continuous with $\sup$-norm and hence the whole map continuous.
\end{proof}

\begin{kor}\label{div_cont}
	$f \in C_{w}^{k}(a, b)$ if and only if $f \in C^{k}(a, b)$.
\end{kor}
\begin{proof}
	Simply apply Lemma \ref{weaktoreal_lemma} inductively.
\end{proof}

This continuity results implies (among many other things) that mean value theorem also holds for general tuples.
\begin{kor}[General mean value theorem for divided differences]
	Let $f \in C^{n}(a, b)$ and $a < x_{0}, x_{1}, \ldots, x_{n} < b$. Then there exists $\xi$ with
	\begin{align*}
		\min_{0 \leq i \leq n}(x_{i}) \leq \xi \leq \max_{0 \leq i \leq n}(x_{i})
	\end{align*}
	such that
	\begin{align}
		[x_{0}, x_{1}, \ldots, x_{n}] = \frac{f^{(n)}(\xi)}{n!}.
	\end{align}
\end{kor}
\begin{proof}
	t.f.i.f \ref{mean_value} and \ref{div_cont}.
\end{proof}

Just like one can carry regularity information, one can carry boundedness information.

\begin{lem}\label{boundtoreal_lemma}
	Let $f : (a, b) \to \R$ and $k \geq 2$. Then the $k$:th order divided differences of $f$ are bounded, if and only if $f \in C^{1}$ and the order $(k - 1)$ divided differences of $f'$ are bounded. Moreover, the bounds satisfy
	\begin{align*}
		\sup_{a < x_{0} < x_{1} < \ldots < x_{k - 1} < b} |[x_{0}, x_{1}, \ldots, x_{k - 1}]_{f'}| = k \sup_{a < x_{0} < x_{1} < \ldots < x_{k}< b} |[x_{0}, x_{1}, \ldots, x_{k}]_{f}|
	\end{align*}
\end{lem}
\begin{proof}
	The bounds follow rather immediately from the identities \ref{derivative_divdif} and \ref{integral_divdif}, so it only remains to verify that $f \in C^{1}$ given the conditions. Since the $k$:th divided difference corresponds to $k$:th derivative, if it is bounded, $(k - 1)$:th derivative should be continuous. Since $k - 1 \geq 1$, in particular $f$ should be $C^{1}$. How to translate this argument to divided differences? Lemma \ref{weaktoreal_lemma} allows us translate the claim ``$f \in C^{k - 1}$" to ``order $k - 1$ divided difference of $f$ can be continuously extended to $(a, b)^{k}$". It is hence sufficient to prove that such extension is possible.

	Lemma \ref{total_derivative} immediately implies that $(k - 1)$:th divided difference of $f$ is Lipschitz. But Lipschitz functions on a dense set can be always extended to the whole space (as Lipschitz functions), so indeed, such extension can be done.
\end{proof}

\begin{lause}\label{bounded_div}
	Let $f : (a, b) \to \R$. Then $f \in C^{k - 1}(a, b)$ and $f^{(k - 1)}$ is Lipschitz, if and only if $k$:th divided difference of $f$ is bounded. Moreover,
	\begin{align*}
		\sup_{a < x_{0} < x_{1} < \ldots < x_{k}< b} |[x_{0}, x_{1}, \ldots, x_{k}]_{f}| = \frac{\Lip(f^{(k - 1)})}{k!}
	\end{align*}
\end{lause}
\begin{proof}
	Again, simply apply Lemma \ref{boundtoreal_lemma} inductively.
\end{proof}

Finally, one can carry positivity.

\begin{lem}\label{postoreal_lemma}
	Let $f : (a, b) \to \R$ and $k \geq 3$. Then $f$ is $k$-tone, if and only if $f' \in C^{1}(a, b)$ and $f'$ is $(k - 1)$-tone.
\end{lem}
\begin{proof}
	Again, only the claim on the regularity is non-trivial as the $k$-tone claim follows straightforwardly from  \ref{derivative_divdif} and \ref{integral_divdif}. As with the bounded case the idea is that if $f$ is $k$-tone $f^{(k)}$ is positive and hence $f^{(k - 1)}$ is increasing, and consequently locally bounded. We should hence prove that the $(n - 1)$:th divided differences are bounded, as then \ref{boundtoreal_lemma} would imply the claim. But this follow easily from \ref{total_derivative}.
\end{proof}

\begin{proof}[Proof of the theorem \ref{k-tone_smooth}]
	Yet again, simply apply Lemma \ref{postoreal_lemma} inductively.
\end{proof}

\section{Analyticity}

\begin{lause}\label{div_anal}
	Let $f : (a, b) \to \R$. Then $f$ is real analytic, if and only if for every closed subinteval $[c, d]$ of $(a, b)$ there exists constant $C_{c, d}$ such that for any $n \geq 1$
	\begin{align*}
		\sup_{a < x_{0} < x_{1} < \ldots < x_{n}< b} |[x_{0}, x_{1}, \ldots, x_{n}]_{f}| \leq C_{c, d}^{n + 1}.
	\end{align*}
\end{lause}
\begin{proof}
	``$\Leftarrow$": We need to prove that for any $x_{0} \in (a, b)$ the Taylor series at $x_{0}$ converges in some neighbourhood of $x_{0}$. As observed before, the $n$:th error term in Taylor series is given by
	\begin{align*}
		[x, x_{0}, x_{0}, \ldots, x_{0}]_{f} (x - x_{0})^{n}
	\end{align*}
	with $n$ $x_{0}$'s. Now choose $a < c < x_{0} < d < b$ and take any $x$ with $x \in [c, d]$ and $|x - x_{0}| C_{c, d} < 1$. But then the error term tends to zero and we are done.

	``$\Rightarrow$": Note that if $x_{0} \in (a, b)$ and $f$ extends to analytic function on $\D(x_{0}, r)$, we definititely have $\left|\frac{f^{(n)}(x_{0})}{n!}\right| \leq C_{x_{0}}^{n + 1}$ for some $C_{x_{0}}$. For $|x - x_{0}| < r$ we have
	\begin{align*}
		\frac{f^{(k)}(x)}{k!} = \sum_{n = k}^{\infty} \binom{n}{k} \frac{f^{(n)}(x_{0})}{n!} (x - x_{0})^{n - k},
	\end{align*}
	which may be estimated by
	\begin{align*}
	 \left|\frac{f^{(k)}(x)}{k!}\right| \leq C_{x_{0}}^{k + 1}\sum_{n = k}^{\infty} \binom{n}{k} C_{x_{0}}^{n - k} |x - x_{0}|^{n - k} = \frac{C_{x_{0}}^{k + 1} }{(1 - |x - x_{0}| C_{x_{0}})^{k + 1}},
	\end{align*}
	whenever $|x - x_{0}| C_{x_{0}} < 1$. By the mean value theorem for divided differences it follows that we get required bound for some neighbourhood of $x_{0}$ and consequently, by compactness for any closed subinteval of $(a, b)$.
\end{proof}

The previous result is some kind of relative of \ref{bounded_div}. Also Theorem \ref{k-tone_smooth} has rather interesting relative.

\begin{lause}[Bernstein's theorem]\label{bernstein_theorem}
	If $f : (a, b) \to \R$ is $k$-tone for every $k \geq 0$, then $f$ is real-analytic on $(a, b)$.
\end{lause}
\begin{proof}
	We prove that the conditions of the Theorem \ref{div_anal} are satisfied. Pick any $a < x_{0} < x < b$. Now for any $n \geq 0$ we have
	\begin{align*}
		f(x) = \sum_{k = 0}^{n - 1} \frac{f^{(k)}(x_{0})}{k!}(x - x_{0})^{k} + \frac{f^{(n)}(x_{0})}{n!} (x - x_{0})^{n} + [x, x_{0}, x_{0}, \ldots, x_{0}]_{f} (x - x_{0})^{n + 1}.
	\end{align*}
	Note that all the terms on the right-hand side are non-negative, and hence
	\begin{align*}
		0 \leq \frac{f^{(n)}(x_{0})}{n!} \leq f(x) (x - x_{0})^{-n}.
	\end{align*}
	Now given any interval $[c, d] \subset (a, b)$ we can make such estimate uniform over $x_{0} \in [c, d]$ simply by picking $x \in (d, b)$.
\end{proof}

\section{Notes and references}
Most of the results of this chapter (with appropriate references), can be found in \cite{Boo}. Fa\`{a} di Bruno formula for divided differences (\ref{div_identities} (v)) was first observed in \cite{Float}.

$k$-tone functions are examined extensively by Popoviciu in \cite{Popov}. The general philosophy ``positivity of sufficiently many linear functionals leads to regularity" is well studied; see for example \cite{Kemperm}. Theorem \ref{bernstein_theorem} is due to Bernstein in \cite{Bernst}.

See \cite{Boor2} for plentiful of illustrations of Peano Kernels.

\begin{comment}

TODO:

\begin{itemize}
	\item Mean value theorem, coefficient of the interpolating polynomial
	\item Basic properties, product rule.
	\item $k$-tone functions, smoothness, and representation
	\item Majorization, Jensen and Karamata inequalities, generalizations, and corollaries concerning spectrum and trace functions. Schur-Horn conjectures and Honey-Combs
	\item Tohoku contains nice proof of Lidskii inequality
	\item How to understand the inequalities arising from $k$-tone functions: is there nice way to parametrize the tuples coming from the $k$-majorization.
	\item For $k = 3$ and $3$ numbers, it's all about the biggers number: one with the largest largest number dominates.
	\item The previous probably generalizes: for $k$-tone functions and $k$ numbers on both sides, with all polynomials of degree less than $k$ vanishing on both tuples, one with largest largest value dominates, or equivalently, it's all about the constant term. This is clearly necessary, by is it also sufficient? Should be: express the whole thing as an integral, differentiate with respect to the constant term, and finally interpret as a divided difference.
	\item What if we add more terms: is there simple characterization? Why have similar integral representation, and can probably differentiate: Maybe not, or one has to be really careful. Is there characterization with linear inequalities (in addition to the equalities)?
	\item Peano Kernels: Smoothness properties, Bernstein (?) polynomials as examples.
	\item Opitz formula
	\item Regularization techniques
	\item Notion of midpoint-convexity should generalize by regularization techniques.
	\item Should Legendre transform generalize to higher orders?  For smooth enough functions probably with derivatives being inverses of each other, but what is the correct definition? And is it of any use? Maybe differentiating $k - 2$ times and then having similar characterization. Is there higher order duality?
	\item Is there elementary transformations for $k$-tone Karamata?
	\item Divided-difference series for entire functions (Newton expansion)? For analytic function? When does it converge? When does it converge to the right function?
	\item Given domain $U \subset \C$ and analytic function $f : U \to \C$, determine all subsets $V \subset U$ such that there exists Newton series with some sequence $x_{1}, x_{2}, \ldots$ converging in $V$. This is very much related to logarithmic potentials and subharmonic functions: sequence, if say bounded for starters, corresponds to a radon measure. Indeed, take weak limit of radon measures averaged exprimental measures of first elements in the sequence, if the limit exists (if not...). Now if $f = \frac{1}{z}$ for starters, we have the logarithmic potential $U(z)$ and the Newton series converges whenever $U(z) < U(0)$.
	\item Harnack-type inequalities for derivatives of Pick functions?
	\item Smooth function is in $P(0, \infty)$ if it's negative of Laplace transform of Laplace transform of a measure on $[0, \infty)$?
	\item Are there better bounds for theorem \ref{bounded_div}?
	\item The dual of completely monotone functions should be polynomials non-negative on $(0, 1)$.
\end{itemize}

\end{comment}
