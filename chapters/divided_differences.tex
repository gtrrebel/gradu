\chapter{$k$-tone functions}

\section{Motivation}

As mentioned in the introduction, $k$-tone functions should correspond to the functions with non-negative $k$'th derivative. What should this mean?

We already know the perfect answer for the case $k = 1$: $1$-tone functions should be the increasing functions.

\begin{lause}\label{increasing_1tone}
	Let $f : (a, b) \to \R$ be differentiable. Then $f$ is increasing, if and only if $f'(x) \geq 0$ for every $x \in (a, b)$.
\end{lause}

\begin{proof}
	If $f$ is increasing, then all its divided differences, i.e. the quotients of the form
	\begin{align*}
		\frac{f(x) - f(y)}{x - y}
	\end{align*}
	for $x \neq y$ are non-negative. As derivatives are limits of such quotients, also they are non-negative at any point. Conversely, by the mean value theorem for every $x \neq y$ we may find $\xi$ such that
	\begin{align*}
		\frac{f(x) - f(y)}{x - y} = f'(\xi).
	\end{align*}
	Now if the derivatives are non-negative, so are the divided differences, so the function is increasing.
\end{proof}

While this proof by the mean value theorem works in more general setting, if $f \in C^{1}$, one has more instructive proof.\footnote{Of course, the following argument would also work with slightly weaker assumptions, but that's not important to us.}

\begin{proof}[Alternate proof for the theorem \ref{increasing_1tone} (in the case $f \in C^{1}(a, b)$)]
	Note that if $f \in C^{1}(a, b)$, we may write
	\begin{align*}
		\frac{f(y) - f(x)}{y - x} = \frac{1}{y - x}\int_{x}^{y} f'(t) dt.
	\end{align*}
	Note that on the right-hand side one we have average of the derivative over the interval. This means that the claim can be translated to: continuous function is non-negative, if and only if its averages over all intervals are non-negative. But this is clear.
\end{proof}

This is really powerful point of view. While one would like to say the increasing functions are the functions with non-negative derivative, that's a bit of a lie. Instead, one can say that they are the functions whose derivative is non-negative average, and all the problems are gone. This should rougly mean that the derivative defines a positive distribution and it is hence a measure. Thus all increasing functions should be integrals of a positive measure (at least almost everywhere). Although this kind of thinking could be carried out, the details aren't important for us. The main point is that one should that think increasing functions, i.e. the $1$-tone functions are functions whose first derivative is a (positive) measure. The divided differences are an averaged (i.e. weak) way of talking about the positivity of the derivative (measure).

This is essentially distributional way of thinking, and we could keep going and end up with the whole business of weak derivatives and stuff. But we don't have to: the plain averages suffice. We write
\begin{align*}
	[x, y]_{f} := \frac{f(x) - f(y)}{x - y},
\end{align*}
and say that $[\cdot, \cdot]_{f}$ is the (first) divided difference of $f$. The domain of $[\cdot, \cdot]_{f}$ should naturally be $(a, b)^{2}$ minus the diagonal. And of course, if $f \in C^{1}$, we should extend $[\cdot, \cdot]_{f}$ to the diagonal, as the derivative. Divided differences then becomes a continuous function on the whole set $(a, b)^2$. Aside from capturing the first derivative, divided difference has two rather convenient properties.

\begin{itemize}
	\item For given $x$ and $y$, $f \mapsto [x, y]_{f}$ defines a linear map, which is continuous if the domain ($\R^{(a, b)}$) has any reasonable topology (any topology finer than the topology of pointwise convergence, i.e. the product topology will do).
	\item Divided differences are local in the sense that if $f$ and $g$ agree on $\{x, y\}$, divided differences agree; this observation readily implies the previous continuity claim.
\end{itemize}

These are the ways the divided differences are compromise between real derivative and the full distributional view. The first point says that one doesn't have worry too much, only about pointwise convergence, while the second says that things are still rather concrete (and it makes the life whole lotta easier).

Now, the real power of this approach comes with larger $k$. What about the case $k = 2$? Again, we already know the perfect answer: $2$-tone functions should be the convex functions.

\begin{lause}
	Let $f : (a, b) \to \R$ be twice differentiable. Then $f$ is convex, if and only if $f^{(2)}(x) \geq 0$ for every $x \in (a, b)$.
\end{lause}

\begin{proof}
	While the result is true as stated, let us only proof the case $f \in C^{2}(a, b)$ (we'll come back to the more general case). Recall that $f$ is convex, if and only if for any $x, y \in (a, b)$ and $t \in [0, 1]$ we have
	\begin{align*}
		t f(x) + (1 - t) f(y) \geq f(t x + (1 - t)y).
	\end{align*}
	This suggest that we may write
	\begin{align*}
		t f(x) + (1 - t) f(y) - f(t x + (1 - t)y) = \int_{x}^{y} w(t) f^{(2)}(t) dt
	\end{align*}
	for some weight $w$. Note that if we manage to find such weight, which is non-negative (and positive enough), we would be done.

	How to find the weight $w$? The idea is rather simple: we want to choose $f$ such that $f^{(2)} = \delta_{a}$ for $a \in \R$ (in some sense). Now, this should mean that $f(t) = (t - a)_{+} + c t + d$ for some $c, d \in \R$, where we write $t_{+} = \max(t, 0)$. Plugging this is on the left hand side we get
	\begin{align*}
		t (x - a)_{+} + (1 - t) (y - a)_{+} - (t x + (1 - t) y - a)_{+} = w(a).
	\end{align*}
	TODO: picture
	Now while the steps taken might have contained some leaps of faith, it can be easily verified with partial integration that the given $w$ really works.
\end{proof}

The giveaway is that while the divided differences are a convenient averaged way to talk about first derivative, the quantity $tf(x) + (1 - t) f(y) - f(t x + (1 - t)y)$ is a convenient averaged way to talk about the second derivative: and it captures the second derivative being a positive measure -- without talking about derivatives. We won't call the quantity the second divided difference, as, as it turns out, we can rewrite it in much more convenient form.

If we write $z = t x + (1 - t) y$, we can solve that $t = \frac{z - y}{x - y}$ and express
\begin{eqnarray*}
	&& t f(x) + (1 - t) f(y) - f(t x + (1 - t)y) \\
	&=& \frac{z - y}{x - y} f(x) + \frac{x - z}{x - y} f(y) - f(z) \\
	&=& -(z - y)(z - x) \left(\frac{f(x)}{(x - y)(x - z)} + \frac{f(y)}{(y - z)(y - x)} + \frac{f(z)}{(z - x)(z - y)} \right)
\end{eqnarray*}

If $t \notin \{0, 1\}$, $-(z - y)(z - x)$ is positive, so if $f$ is convex,
\[
	\frac{f(x)}{(x - y)(x - z)} + \frac{f(y)}{(y - z)(y - x)} + \frac{f(z)}{(z - x)(z - y)} \geq 0
\]
for any $x, y$ and $z$ such that $z$ is between $x$ and $y$. This new expression is symmetric in its variables, so actually there's no need to assume anything on the fo $x, y$ and $z$, just that they're distinct. We can also easily carry this argument to the other direction. If this expression is non-negative any distinct $x, y$ and $z$, $f$ is convex. This motivates to define
\[
	[x, y, z]_{f} := \frac{f(x)}{(x - y)(x - z)} + \frac{f(y)}{(y - z)(y - x)} + \frac{f(z)}{(z - x)(z - y)},
\]
the second divided difference of $f$.

One would naturally expect that by setting
\[ 
	[x_{0}, x_{1}, \ldots, x_{n}]_{f} := \sum_{i = 0}^{n} \frac{f(x_{i})}{\prod_{j \neq i} (x_{i} - x_{j})},
\]
one obtains something that naturally generalizes divided differences for higher orders. This is indeed the case.

\section{Divided differences}

TODO: rewrite the following section

For $n \geq 1$ define $D_{n} = \{x\in \R^{n} | x_{i} = x_{j} \text{ for some $1 \leq i < j \leq n$}\}$.
\begin{maar}
Let $n \geq 0$. For any real function $f : (a, b) \to \R$ we define the corresponding $k$'th divided difference $[\cdots]_{f} : (a, b)^{n + 1} \setminus D_{n + 1}$ by setting
\[
	[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \sum_{i = 0}^{n} \frac{f(x_{i})}{\prod_{j \neq i} (x_{i} - x_{j})}.
\]
\end{maar}

As in the case $n = 1$, if for $n > 1$ we can continuously extend divided differences to the set $D_{n + 1}$, we should do that, and we identify the resulting function with the original one. We will later proof that, as expected, this can be done, if and only $f \in C^{n}(a, b)$.

We have the following important properties.

\begin{prop}
	Divided differences are symmetric in the variable, i.e. for any $f : (a, b) \to \R$, $a < x_{0}, x_{1}, \ldots, x_{n} < b$ permutation $\sigma : \{1, 2, \ldots, n\} \to \{1, 2, \ldots, n\}$ we have
	\[
		[x_{1}, x_{2}, \ldots, x_{n}]_{f} = [x_{\sigma(1)}, x_{\sigma(2)}, \ldots, x_{\sigma(n)}]_{f}
	\]
	and linear in the function, i.e. for any $\alpha, \beta \in \R$, $f, g : (a, b) \to \R$ and $a < x_{0}, x_{1}, \ldots, x_{n} < b$ we have
	\[
		[x_{0}, x_{1}, \ldots, x_{n}]_{\alpha f + \beta g} = \alpha [x_{0}, x_{1}, \ldots, x_{n}]_{f} + \beta [x_{0}, x_{1}, \ldots, x_{n}]_{g}.
	\]
	In addition, divided differences can be calculated recursively as
	\begin{align}\label{divdif_rec}
		[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \frac{[x_{0}, x_{1}, \ldots, x_{n - 1}]_{f} - [x_{1}, \ldots, x_{n}]_{f}}{x_{0} - x_{n}}.
	\end{align}
\end{prop}
\begin{proof}
	Easy to check.
\end{proof}

Also, we have following classic characterization.

\begin{prop}
We have $[x_{0}, x_{1}, \ldots, x_{n}]_{(x \mapsto x^{n})} = 1$ and $[x_{0}, x_{1}, \ldots, x_{n}]_{p} = 0$ for any polynomial of degree at most $n - 1$. In other words, $[x_{0}, x_{1}, \ldots, x_{n}]_{f}$ is the leading coefficient of the Lagrange interpolation polynomial on pairs $(x_{0}, f(x_{0})), (x_{1}, f(x_{1}), \ldots, (x_{n}, f(x_{n}))$.
\end{prop}
\begin{proof}
	Claims are easily derived from each other since if $f$ is itself a polynomial of degree at most $n$, its Lagrange interpolation polynomial is $f$ itself. Recall that the Lagrange interpolation polynomial of a dataset $(x_{0}, y_{0}), (x_{1}, y_{1}), \ldots, (x_{n}, y_{n})$ is given by
	\[
		\sum_{i = 0}^{n} y_{i} \frac{\prod_{j \neq i}(x - x_{j})}{\prod_{j \neq i}(x_{i} - x_{j})},
	\]
	and in our case the leading coefficient of this polynomial leads exactly to our definition for the divided differences.
\end{proof}

These observation already partially justify the terminology: as higher order derivatives are defined recursively using (first order) derivatives, higher order divided differences can be calculated recursively using (the usual) divided differences.

The most important property of the divided differences is the following.

\begin{lause}[Mean value theorem for divided differences]
	Let $n \geq 1$ and $f \in C^{n}(a, b)$. Then for any $x_{0}, x_{1}, \ldots, x_{n}$ we have $\min_{0 \leq i \leq n}(x_{i}) \leq \xi \leq \max_{0 \leq i \leq n}(x_{i})$ such that
	\begin{align}\label{mean_value}
		[x_{0}, x_{1}, \ldots, x_{n}] = \frac{f^{(n)}(\xi)}{n!}.
	\end{align}
\end{lause}
\begin{proof}
	We prove the statement assuming additionally that the divided differences define a continuous function on the whole set $(a, b)^{n + 1}$: this will proven later. Note that if one manages to prove the statement for distinct points, one may take sequence of tuples of distinct points, $((x_{i}^{(j)})_{i = 0}^{n})_{j = 1}^{\infty}$ converging to $(x_{i})_{i = 0}^{\infty}$. Now the left-hand side will converge to the respective divided difference (assuming the continuity), and by moving to a convergent subsequence, so will the $\xi_{n}$'s on the right-hand side. By the continuity of $f^{(n)}$ we are done.

	For the case of distinct $x_{i}$'s note that we have already proven the statement for polynomials of order at most $n$. By linearity it hence suffices to prove the statement for $C^{n}(a, b)$ functions vanishing on the set $\{x_{i} | 0 \leq i \leq n\}$. This we know already for $n = 1$ ; this is the mean value theorem. Let us prove the statement by induction on $n$. To simplify notation we may assume that $x_{0} < x_{1} < \ldots < x_{n}$. Note that by the mean value theorem, given that $f(x_{i}) = 0$ for any $0 \leq i \leq n$, we also have $f'(y_{i}) = 0$ for some $x_{i} < y_{i} < y_{i + 1}$, for $0 \leq i \leq n - 1$. By the induction hypothesis the $(n - 1)$:th derivative of $f'$, $f^{(n)}$ has a zero $\xi$ with $x_{0} \leq \xi \leq x_{n}$. But this is exactly what we wanted.

	TODO: figure of recursive procedure.
\end{proof}

We'll get back to smoothness in a minute. This is already a very precise sense in which divided differences work like derivatives, up to a constant. In some sense though $\frac{f^{(n)}}{n!}$, the Taylor coefficients are even more natural objects than the pure derivatives. They are the coefficients in the Taylor expansion, and they satisfy Leibniz rule much more natural than the one with pure derivatives:
\[
	\frac{(f g)^{(n)}(x)}{n!} = \sum_{k = 0}^{n} \left(\frac{f^{(k)}(x)}{k!}\right)\left(\frac{g^{(n - k)}(x)}{(n - k)!}\right).
\]

Divided differences enjoy similar Leibniz formula, which is related to a generalization of Taylor expansion, called Newton expansion. In Newton expansion we first fix a sequence of points $x_{0}, x_{1}, \ldots x_{n} \in (a, b)$, say pairwise distinct for starters. For $f : (a, b) \to \R$ and $x \in (a, b)$ we may start a process of rewriting
\begin{align*}
	f(x) &= f(x_0) + f(x) - f(x_{0}) \\
	&= [x_{0}]_{f} + [x, x_{0}]_{f} (x - x_{0}) \\
	&= [x_{0}]_{f} + ([x_{0}, x_{1}]_{f} + ([x, x_{0}]_{f}- [x_{0}, x_{1}]_{f}))(x - x_{0}) \\
	&= [x_{0}]_{f} + [x_{0}, x_{1}]_{f} (x - x_{0})  + [x, x_{0}, x_{1}]_{f} (x - x_{0}) (x - x_{1}) \\
	&= \ldots \\
	&= [x_{0}]_{f} + [x_{0}, x_{1}]_{f} (x - x_{0})  + [x_{0}, x_{1}, x_{2}]_{f} (x - x_{0}) (x - x_{1}) + \ldots \\
	& + [x_{0}, x_{1}, \ldots, x_{n}]_{f} (x - x_{0}) (x - x_{1}) \cdots (x - x_{n - 1}) \\
	& + [x, x_{0}, x_{1}, \ldots, x_{n}]_{f} (x - x_{0}) (x - x_{1}) \cdots (x - x_{n}).
\end{align*}

By taking first $1, 2, \ldots$ terms of the sum, one obtains Newton form of interpolating polynomial for the first $1, 2, \ldots, $ points of the sequence $x_{0}, x_{1}, \ldots$. If the points $x_{i}$ coincide, we get
\begin{align}\label{taylor_expansion}
	f(x) = \sum_{k = 0}^{n - 1} \frac{f^{(k)}(x_{0})}{k!} + [x, x_{0}, x_{0}, \ldots, x_{0}]_{f} (x - x_{0})^{n},
\end{align}
the usual Taylor expansion. Consequently
\[
	[x, x_{0}, x_{0}, \ldots, x_{0}]_{f} = \frac{f(x) - \sum_{k = 0}^{n - 1} \frac{f^{(k)}(x_{0})}{k!}}{(x - x_{0})^{n}},
\]
and as is well known, this error term can be also written in the form
\[
	[x, x_{0}, x_{0}, \ldots, x_{0}]_{f} = \frac{1}{(x - x_{0})^{n}}\int_{x_{0}}^{x} \frac{f^{(n)}(t) (x - t)^{n - 1}}{(n - 1)!} dt.
\]

\begin{huom}
	As Taylor expansion lead to Taylor series, one might wonder under which conditions do Newton expansions lead to Newton series. That is, under which conditions for analytic $f : U \to \C$ and a sequence $z_{0}, z_{1}, \ldots, z_{n}, \ldots \in \C$ and $z \in \C$ the following converges
	\begin{align*}
		f(z) &= [z_{0}]_{f} \\
		&+ [z_{0}, z_{1}]_{f} (z - z_{0}) \\
		&+ [z_{0}, z_{1}, z_{2}]_{f} (z - z_{0}) (z - z_{1}) \\
		&+ \ldots \\
		&+ [z_{0}, z_{1}, z_{2}, \ldots, z_{n}]_{f} (z - z_{0}) (z - z_{1}) \cdots (z - z_{n - 1}) \\
		&+ \ldots
	\end{align*}
	If the points coincide we recover the usual Taylor expansion and from the \ref{taylor_expansion} we see that the Taylor series converges on disc $\D(z_{0}, r)$ if $\left|\frac{f^{(n)}}{n!}\right| \leq C/r^{n}$ for some $C > 0$. If $f$ is entire, we have such bound for every $r$ and the series converges everywhere. In a similar vein,
	if $f$ is entire and $Z = (z_{i})_{i \geq 0}$ is bounded, also Newton series converges for every $z \in \C$, but if $Z$ is not bounded, series need not converge for any $z$ outside $Z$.

	For other domains the question is more subtle, and it's closely related to logarithmic potentials and subharmonic functions.
\end{huom}

Divided differences enjoy also the following nesting property.

\begin{prop}
	For any $f : (a, b) \to \R$ and pairwise distinct $x_{1}, x_{2}, \ldots, x_{n}, y_{1}, y_{2}, \ldots, y_{m}$ we have
	\[
		[y_{1}, y_{2}, \ldots, y_{m}]_{[\cdot, x_{1}, x_{2}, \ldots, x_{n}]_{f}} = [y_{1}, y_{2}, \ldots, y_{m}, x_{1}, x_{2}, \ldots, x_{n}]_{f}.
	\]
	In particular
	\[
		\frac{d^{m}}{d x^{m}} \left([x, x_{1}, x_{2}, \ldots, x_{n}]_{f}\right) = m! [x, x, \ldots, x, x_{1}, x_{2}, \ldots, x_{m}]
	\]
\end{prop}
\begin{proof}
	Note that both $[y_{1}, y_{2}, \ldots, y_{m}]_{[\cdot, x_{1}, x_{2}, \ldots, x_{n}]_{f}}$ and $[y_{1}, y_{2}, \ldots, y_{m}, x_{1}, x_{2}, \ldots, x_{n}]_{f}$ satisfy \ref{divdif_rec} and they agree when $m = 1$.
\end{proof}

\section{Cauchy's integral formula}

Complex analysis offers a nice view on divided differences: if $f$ is analytic, we may interpret divided differences contour integrals.

\begin{lem}[Cauchy's integral formula for divided differences]\label{div_cauchy}
If $\gamma$ is a closed counter-clockwise curve enclosing the numbers $x_{0}, x_{1}, \ldots, x_{n}$, we have
\[
	[x_{0}, x_{1}, \ldots, x_{n}]_{f} = \frac{1}{2 \pi i} \int_{\gamma} \frac{f(z)}{(z - x_{0})(z - x_{1}) \cdots (z - x_{n})} dz.
\]
\end{lem}
\begin{proof}
Easy induction, by taking Cauchy's integral formula as a base case. Alternatively, the claim is a direct consequence of the Residue theorem.

There's another rather instructive proof for the statement. Write Newton expansion for $f$ and integrate both sides along $\gamma$. We get
\begin{align*}
	\frac{1}{2 \pi i} \int_{\gamma} \frac{f(z)}{(z - x_{0})(z - x_{1}) \cdots (z - x_{n})} dz &= \frac{1}{2 \pi i} \int_{\gamma} \frac{[x_{0}]_{f}}{(z - x_{0})(z - x_{1}) \cdots (z - x_{n})} dz \\
	&+ \frac{1}{2 \pi i} \int_{\gamma} \frac{[x_{0}, x_{1}]_{f}}{(z - x_{1}) \cdots (z - x_{n})} dz \\
	&+ \frac{1}{2 \pi i} \int_{\gamma} \frac{[x_{0}, x_{1}, x_{2}]_{f}}{(z - x_{2}) \cdots (z - x_{n})} dz \\
	&+ \ldots \\
	&+ \frac{1}{2 \pi i} \int_{\gamma} \frac{[x_{0}, x_{1}, x_{2}, \ldots, x_{n - 1}]_{f}}{(z - x_{n - 1})(z - x_{n})} dz \\
	&+ \frac{1}{2 \pi i} \int_{\gamma} \frac{[x_{0}, x_{1}, x_{2}, \ldots, x_{n}]_{f}}{(z - x_{n})} dz \\
	&+ \frac{1}{2 \pi i} \int_{\gamma} [z, x_{0}, x_{1}, x_{2}, \ldots, x_{n}]_{f} dz \\
\end{align*}
As $z \mapsto [z, x_{0}, x_{1}, x_{2}, \ldots, x_{n}]_{f}$ is analytic, the last integral vanishes. First $n$ integrals vanish also, since the integrands decay at least as $|z|^{-2}$. Finally, the $(n + 1)$:th term gives exactly what we wanted.
\end{proof}

If all the points coincide, we get the familiar formula for the $n$'th derivative. Also, if $f$ is polynomial of degree at most $n - 1$, the integrand decays as $|z|^{-2}$ and hence the divided differences vanish. Also, for $z \mapsto z^{n}$ one can use the formula to calculate the $n$'th divided difference with a residue at infinity. Formula is slightly more concisely expressed by writing for a sequence $X = (x_{i})_{i = 0}^{n}$ $p_{X}(x) = \prod_{i = 0}^{n} (x - x_{i})$. Also if one shortens $[X]_{f} = [x_{0}, x_{1}, \ldots, x_{n}]_{f}$, we have
\[
	[X]_{f} = \frac{1}{2 \pi i} \int_{\gamma} \frac{f(z)}{p_{X}(z)} dz.
\]

Cauchy's integral formula is a convenient way to think about severel identities.

\begin{esim}
For instance we may express the Lagrange interpolation polynomial of a analytic function $f$ and sequence $X = (x_{i})_{i = 0}^{n}$ by
\[
	P_{X}(x) = \frac{1}{2 \pi i} \int_{\gamma} \frac{p_{X}(x) - p_{X}(z)}{x - z}\frac{f(z)}{p_{X}(z)} dz = [X]_{f [x, \cdot]_{p_{X}}}.
\]
More generally, if some of the points coincide, we get the Hermite interpolation polynomial. In general, if one wants to find a polynomial which vanishes at points TODO
\end{esim}


\begin{prop}[Leibniz formula for divided differences]\label{Leibniz_rule}
	We have
	\begin{align*}
		[x_{0}, x_{1}, \ldots, x_{n}]_{f g} &= [x_{0}]_{f} [x_{0}, x_{1}, \ldots, x_{n}]_{g} \\
		&+ [x_{0}, x_{1}]_{f} [x_{1}, \ldots, x_{n}]_{g} \\
		&+ [x_{0}, x_{1}, x_{2}]_{f} [x_{2}, \ldots, x_{n}]_{g} \\
		&+ \ldots \\
		&+ [x_{0}, x_{1}, x_{2}, \ldots, x_{n}]_{f} [x_{n}]_{g}.
	\end{align*}
\end{prop}
\begin{proof}
	Write Newton expansion for $f$ and $g$ with points reversed for $g$. The rest follows as in the final proof of Theorem (\ref{div_cauchy}).
\end{proof}

Actually, we are not quite done yet. Cauchy's integral formula only works for analytic functions. We can however extend the prove with the following useful observation.
\begin{lem}\label{bootstrap_lemma}
	Let $n, m \geq 0$. Assume that for some constants $c_{i, j}$ and $a_{i, j} \in (a, b)$ we have
	\begin{align*}
	\sum_{\atop{0 \leq i \leq n}{1 \leq j \leq m}} c_{i, j} f^{(i)}(a_{i, j}) = 0
	\end{align*}
	for every polynomial. Then the numbers $c_{i, j}$ are all zeroes.
\end{lem}
\begin{proof}
	By Hermite interpolation TODO we can find for any pair $(i, j)$ polynomial with $f^{(i)}(a_{i, j}) = 1$ and $f^{(j')}(a_{i', j'})$ for every other pair $(i', j')$. Consequently $c_{i, j} = 0$ and we have the claim.
\end{proof}

Of course there's nothing really special about the functional being linear, but the point is: if the $F : C^{n}(a, b) \to \R$ depends only $f$ and it's derivatives up to some fixed order at some finite set of fixed points, then we know $F$ just by knowing the values at polynomials.

\begin{proof}[Rest of the proof of \ref{Leibniz_rule}]
	Note that if we expand the divided differences, we are almost in the situation of the lemma \ref{bootstrap_lemma}; now we just have product of two functions instead. Story is the same.
\end{proof}

\section{Why does this all work?}

We have already seen how divided differences offer a nice tool for discussing derivatives, and complex analytic point of view has certainly reinforced this idea. One might (and should), however, wonder why does this all work?

The reason is rather simple: $n$'th order divided difference is a certain weighted average of the $n$'th derivative. If $n = 1$ for instance, and $x < y$, we have
\begin{align*}
	[x, y]_{f} = \frac{1}{y - x}\int_{x}^{y} f'(t) dt = \int_{-\infty}^{\infty} \frac{\chi_{[x, y]}(t)}{y - x} f'(t) dt:
\end{align*}
first divided difference is average of the derivative over the interval $[x, y]$.

When $n = 2$, plain average doesn't work anymore. How should one choose the weight $w_{x, y, z}$, such that
\begin{align*}
	[x, y, z]_{f} = \int_{-\infty}^{\infty} w_{x, y, z}(t) f^{(2)}(t) dt
\end{align*}
holds for any $f$?

TODO (Also, rewrite the following section)

We already noticed that the $n$:th divided differences of the form $[x, x, \ldots, x, y]_{f}$ can be written as an integral of $n$:th derivative of $f$ with some positive polynomial weight. This is no anomaly.

\begin{lause}
	Let $n > 1$, $a < x_{1} < \ldots < x_{n} < b$ and $1 \leq d_{1}, d_{2}, \ldots, d_{n}$, and denote $d = d_{1} + d_{2} + \ldots + d_{n} - 1$. Then there exits an unique function $w = w_{x_{1}, \ldots, x_{n}, d_{1}, d_{2}, \ldots, d_{n}} : \R \to \R$ with the following properties
	\begin{enumerate}[(i)]
		\item $w$ is supported on $[x_{1}, x_{n}]$
		\item $w$ is polynomial of degree at most $d - 1$ on each of the intervals $[x_{i}, x_{i + 1}]$ for $1 \leq i \leq n - 1$
		\item $w$ is $C^{d - 2 - d_{i}}$ at $x_{i}$ (if $d - 2 - d_{i} < 0$, (in which case necessarily $d - 2 - d_{i} = -1$), no continuity is required)
		\item $w$ is non-negative
		\item For every $f \in C^{d}(a, b)$ we have
		\begin{align*}
			[x_{1}, \ldots, x_{1}, x_{2}, \ldots, x_{2}, \ldots, x_{n}, \ldots, x_{n}]_{f} = \int_{\R} f^{(d)}(t) w(t) dt,
		\end{align*}
		where $x_{i}$ appears $d_{i}$ times.
	\end{enumerate}
	\begin{proof}
		Standard bump function argument shows that should such $w$ exist, it is necessarily unique.

		Take any $x_{0} \in (x_{1}, x_{n})$. For every $0 \leq j < d$ and $1 \leq i \leq n$ we can Taylor expand
		\begin{align*}
			f^{(j)}(x_{i}) &= f^{(j)}(x_{0}) + f^{(j + 1)}(x_{0})(x_{i} - x_{0}) + \ldots + \frac{f^{(d - 1)}(x_{0})}{(d - 1 - j)!} (x_{i} - x_{0})^{d - 1 - j} \\
			&+ \int_{x_{0}}^{x_{i}} \frac{f^{(d)}(t)}{(d - 1 - j)!} (x_{i} - x_{0})^{d - 1 - j} dt.
		\end{align*}
		Since the divided differences can be written as a sum of such terms, we find $w$, and constants $c_{0}, c_{1}, \ldots, c_{d - 1}$ supported on $[x_{1}, x_{n}]$ such that
		\begin{align*}
			& [x_{1}, \ldots, x_{1}, x_{2}, \ldots, x_{2}, \ldots, x_{n}, \ldots, x_{n}]_{f} \\
			&= \int_{\R} f^{(d)}(t) w(t) dt \\
			&+  c_{0} f(x_{0}) + c_{1} f'(x_{0}) + \ldots + c_{d - 1} f^{(d - 1)}(x_{0})
		\end{align*}
		for every $f \in C^{d}(a, b)$. We claim that $w$ is the sought function.

		Note that since the divided difference of order $d$ kills all polynomials of degree less than $d$, the constants $c_{i}$ are necessarily all zeros. Now clearly $w$ satisfies $(i), (iii)$ and $v$. Also $w$ at least almost satisfies $(ii)$: it might not be polynomial at $x_{0}$. This cannot happen, however, since by the uniqueness we could have picked different $x_{0}$ resulting to different point of non-polynomiality. To prove $(iv)$ note that should $w$ be negative, we can easily construct  function $f \in C^{d}(a, b)$ such that $f^{(d)} \geq 0$ and
		\begin{align*}
			0 > \int_{\R} f^{(d)}(t) w(t) dt = [x_{1}, \ldots, x_{1}, x_{2}, \ldots, x_{2}, \ldots, x_{n}, \ldots, x_{n}]_{f},
		\end{align*}
		which clearly violates the mean value theorem.
	\end{proof}
\end{lause}
If $n = 1$, $w$ fails to be a function, it is a delta measure at $x_{1}$.

The functions $w$ are called \textit{Peano kernels} or \textit{B-splines}.

TODO: pictures

Peano kernels describe the precise sense in which divided differences are weak versions of $n$'th derivative. Of course on could also use more traditional weak approach using distributions, but here the nice thing is that our functionals are local. TODO

There are various ways to express Peano kernels. TODO

\section{$k$-tone functions}

\begin{maar}
	$f : (a, b) \to \R$ is called $k$-tone if for any $X = (x_{i})_{i = 0}^{n}$ of distinct points we have
	\[
		[X]_{f} \geq 0,
	\]
	i.e. the $n$'th divided difference is non-negative.
\end{maar}

As we noticed, $1$-tone and $2$-tone functions are exactly the monotone increasing and convex functions. The terminology is not very established, and such functions are also occasionally called $k$-monotone or $k$-convex.

Mean value theorem for divided differences tells us that $C^{k}$ $k$-tone functions are exactly the functions with non-negative $k$'th derivative. It turns out that this almost true in general case, namely we have the following result.

\begin{lause}\label{k-tone_smooth}
	Let $f : (a, b) \to \R$ and $k \geq 2$. Then $f$ is $k$-tone, if and only if $f \in C^{k - 2}(a, b)$, $f^{(k - 2)}(x)$ is convex.
\end{lause}

We will postpone the proof.

In some sense the further smoothness assumption is not that much of a game changer. It turns out $k$-tone functions are always $k$ times differentiable in a weak sense (?), and the weak derivative is non-negative.

One can also usually use regularization techniques discussed in ? to reduce a problem about general $k$-tone functions to smooth $k$-tone functions. In general:

\begin{phil}
	One should not worry about smoothness issues.
\end{phil}

We will not resort to such sorcery, however, but try to understand the true reasons behind the smoothness.

We denote the space of $k$-tone functions by on interval $(a, b)$ by $P^{(k)}(a, b)$. $k$-tone functions the following enjoy the following useful properties.

\begin{prop}
	For any positive integer $k$ and open interval $(a, b)$ $P^{(k)}(a, b)$ is a closed (under pointwise convergence) convex cone.
\end{prop}
\begin{proof}
	Convex cone property is immediate form the linearity of divided differences. Also, if $f_{i} \to f$ pointwise, the respective divided differences converge, so also the closedness is clear.
\end{proof}

\begin{prop}
	$P^{(k)}$ is a local property i.e. $P^{(k)}(a, b) \cap P^{(k)}(c, d) \subset P^{(k)}(a, d)$ for any $-\infty \leq a \leq c < b \leq d \leq \infty$. To be more precise, if $f : (a, d) \to \R$ such that $\restr{f}{(a, b)} \in P^{(k)}(a, b)$ and $\restr{f}{(c, d)} \in P^{(k)}(c, d)$, then $f \in P^{(k)}(a, d)$.
\end{prop}
\begin{proof}
	For $f \in C^{k}$ the statement is immediate form the mean value theorem. In general we can argue bit similarly as in the case $k = 1$. For $k = 1$ note that if $a < x < c < b < z < d$ we may take $c < y < b$. Now
	\[
		f(z) - f(x) = (f(z) - f(y)) + (f(y) - f(x)) \geq 0.
	\]
	In terms of divided differences
	\[
		[x, z]_{f} = [z, y]_{f} \frac{z - y}{z - x} + [x, y]_{f} \frac{y - x}{z - x}.
	\]
	The point is that we can express divided differences as weighted sums of divided differences of tuples with smaller supports. More generally, if $a < x_{0} < \ldots < x_{k} < d$ with $x_{0} < c$ and $d < x_{k}$, take any $y \in (c, b)$ distinct from $x_{i}$'s and we have
	\[
		[x_{0}, \ldots, x_{k}]_{f} = [x_{1}, \ldots, x_{k}, y]_{f} \frac{x_{k} - y}{x_{k} - x_{0}} + [x_{0}, \ldots, x_{k - 1}, y]_{f} \frac{y - x_{0}}{x_{k} - x_{0}}.
	\]
	This identity is easily verified by applying the previous version for the function $x \mapsto [\cdot, x_{1}, x_{2}, \ldots, x_{n - 1}]_{f}$. By repeating this process, we will end up with divided differences of tuples completely lying on $(a, b)$ or $(c, d)$. Formally, we could induct on $l$ number of $x_{i}$'s outside $(c, b)$ and note that if tuple isn't good already, the two new tuples have lower number $l$.
\end{proof}

\section{Smoothness}

\begin{lause}
	Let $f : (a, b) \to \R$ and $n \geq 1$. Then $f \in C^{n}(a, b)$, if and only if $n$:th divided difference of $f$ extends to continuous function on $(a, b)^{n + 1}$.
\end{lause}

Actually we can prove a slightly better statement. Let $n, m \geq 1$ and and denote
\[
	D_{n, m} = \{x \in \R^{n} | x_{i_{1}} = x_{i_{2}} = \ldots = x_{i_{k}} \text{ for some $1 \leq i_{1} < i_{2} < \ldots < i_{m} \leq n$} \}.
\]
\begin{lause}
	Let $f : (a, b) \to \R$, $0 \leq m \leq n$. Then $f \in C^{m}(a, b)$, if and only if $n$:th divided difference of $f$ extends to continuous function to $(a, b)^{n + 1} \setminus D_{n + 1, m + 2}$. Moreover, this extension is unique and satisfies \ref{divdif_rec} and \ref{mean_value}.
\end{lause}

\begin{proof}
	We first prove the ``only if"-direction by induction on $m$.

	The case $m = 0$ is clear. Now fix $m > 0$. Let us prove the statement for this $m$ by induction on $n$.

	Consider the base case case $n = m$. Take any sequence $a < x_{0} \leq x_{1} \leq x_{2} \ldots \leq x_{n} < b$. By induction hypothesis for the pair $(n, m - 1)$ we can extend the divided difference to this point if $x_{0} < x_{n}$. If $x_{0} = x_{1} = \ldots = x_{n}$, we will extend the divided difference as
	\[
		[x_{0}, x_{1}, \ldots, x_{n}] = \frac{f^{(n)}(x_{0})}{n!}.
	\]
	But the mean value theorem for the divided difference immediately implies this extension is continuous at $(x_{0}, x_{1}, \ldots, x_{n})$.

	If $n > m$, and $x \in \R^{n + 1} \setminus D_{n + 1, m + 2}$, we necessarily have $x_{0} < x_{n}$ and hence we can extend the divided differences using \ref{divdif_rec}. By construction, our extension satisfies \ref{divdif_rec} and \ref{mean_value} and since $\R^{n + 1} \setminus D_{n + 1}$ is dense in $\R^{n + 1} \setminus D_{n + 1, m + 2}$, the extension is necessarily unique.

	Let us then prove the ``if"-direction. We start with a lemma.

	\begin{lem}\label{smooth_lemma}
		Let $n \geq 1$ and $m \geq 0$. If the $n$:th divided difference of $f$ has continuous extension to the set $\R^{n + 1} \setminus D_{n + 1, m + 2}$, then there also is a continuous extension for the $(n - 1)$:th divided difference of $f$ to the set $\R^{n} \setminus D_{n, m + 2}$. 
	\end{lem}
	\begin{proof}
		Take any $x \in \R^{n} \setminus D_{n, m + 2}$. We induct downward on $l$, the number of distinct components of $x$. If $l = n$, the statement is clear. Assume then that $l < n$. Now there exist $x' \in \R^{n} \setminus D_{n, m + 2}$ with $l + 1$ distinct components, which differs from $x$ by exactly one component, say i'th one. We then extend
		\[
			[x_{1}, x_{2}, \ldots, x_{n}]_{f} := [x_{1}, x_{2}, \ldots, x'_{i}, \ldots, x_{n}]_{f} + (x'_i - x_{i}) [x_{1}, x_{2}, \ldots, x_{i}, x'_{i}, \ldots, x_{n}]_{f}.
		\]
		Now if $x^{(n)} \to x$, then by comparing both sides also $[x_{1}, x_{2}, \ldots, x_{n}]_{f}$'s converge, and we have the continuous extension.
	\end{proof}

	Now let us continue with the proof. We induct on $m$. The case $m = 0$ is clear. The case $n = m = 1$ is also rather clear, the diagonal $[x, x]_{f}$ given the derivative of $f$ and continuity of the derivative is implied by the continuity of $[\cdot, \cdot]_{f}$ along the diagonal.

	\begin{lem}\label{divdif_der}
		If $f \in C^{1}(a, b)$ and $a < x_{1}, x_{2}, \ldots, x_{n} < b$ are distinct, then 
		\[
			[x_{1}, x_{2}, \ldots, x_{n}]_{f'} = \sum_{1 \leq i \leq n} [x_{1}, x_{2}, \ldots, x_{i - 1}, x_{i}, x_{i}, x_{i + 1}, \ldots, x_{n}]_{f}
		\]
	\end{lem}
	\begin{proof}
		We induct on $n$: the case $n = 1$ is clear. When $n > 1$, by \ref{divdif_rec} we have
		\begin{align*}
			[x_{1}, x_{2}, \ldots, x_{n}]_{f'} &= \frac{[x_{1}, \ldots, x_{n - 1}]_{f'} - [x_{2}, \ldots, x_{n}]_{f'}}{x_{1} - x_{n}} \\
			&= \frac{\left(\sum_{1 \leq i \leq n - 1} [x_{1}, \ldots, x_{i}, x_{i}, \ldots, x_{n - 1}]_{f}\right) - \left(\sum_{2 \leq i \leq n} [x_{2}, \ldots, x_{i}, x_{i}, \ldots, x_{n}]_{f}\right)}{x_{1} - x_{n}} \\
			&= \sum_{2 \leq i \leq n - 1} \frac{[x_{1}, \ldots, x_{i}, x_{i}, \ldots, x_{n - 1}]_{f} - [x_{2}, \ldots, x_{i}, x_{i}, \ldots, x_{n}]_{f}}{x_{1} - x_{n}} \\
			&+ \frac{[x_{1}, x_{1}, x_{2}, \ldots, x_{n - 1}]_{f} - [x_{2}, \ldots, x_{n}, x_{n}]_{f}}{x_{1} - x_{n}} \\
			&= \sum_{2 \leq i \leq n - 1}  [x_{1}, \ldots, x_{i}, x_{i}, \ldots, x_{n}]_{f} \\
			&+ \frac{[x_{1}, x_{1}, \ldots, x_{n - 1}]_{f} - [x_{1}, \ldots, x_{n} ]_{f}}{x_{1} - x_{n}} + \frac{[x_{1}, \ldots, x_{n}]_{f} - [x_{2}, \ldots, x_{n}, x_{n} ]_{f}}{x_{1} - x_{n}} \\
			&= \sum_{1 \leq i \leq n}  [x_{1}, \ldots, x_{i}, x_{i}, \ldots, x_{n}]_{f} \\
		\end{align*}
	\end{proof}

	Let us then take $2 \leq m = n$. By the lemma \ref{smooth_lemma} and the case $m = 1$ we see that $f \in C^{1}(a, b)$. But by the lemma \ref{divdif_der} $(n - 1)$:th divided differences of $f'$ extend to continuous function to $\R^{n}$. Hence by the induction hypothesis $f' \in C^{(n - 1)}(a, b)$ and hence $f \in C^{n}(a, b)$.

	Finally the lemma \ref{smooth_lemma} immediately implies the remaining cases $\leq m < n$, by induction on $n$.
\end{proof}

There is however more interesting equivalence to be made.

\begin{lause}\label{bounded_div}
	Let $f : (a, b) \to \R$ and $n \geq 1$. Then $f \in C^{n - 1}(a, b)$ and $f^{(n - 1)}$ is Lipschitz, if and only if $n$:th divided difference of $f$ is bounded. Moreover,
	\[
		\sup_{a < x_{0} < x_{1} < \ldots < x_{n}< b} |[x_{0}, x_{1}, \ldots, x_{n}]_{f}| = \frac{\Lip(f^{(n - 1)})}{n!}
	\]
\end{lause}

\begin{proof}
	We first prove the ``if"-direction.

	The case $n = 1$ is clear. In the case $n = 2$ note that
	\[
		[x, y, z] = \frac{[y, x]_{f} - [z, x]_{f}}{y - z}
	\]
	Since this quantity is bounded, it follows that $[\cdot, x]_{f}$ has a limit at $x$, which means exactly that $f$ is differentiable at $x$. Note that in addition for any $x, x', y, y'$ we have
	\[
		[x', x, y]_{f} + [y', y, x] = \frac{([x', x]_{f} - [x, y]_{f}) - ([y', y]_{f} - [x, y]_{f})}{x - y} = \frac{[x', x] - [y, y']}{x - y}.
	\]
	Letting $x' \to x$ and $y' \to y$ we see that $f'$ is Lipschitz and hence also $f \in C^{1}(a, b)$ .

	Now for general $n$ we argue by induction on $n$. Let $n > 2$. Note that since
	\[
		[x_{0}, x_{1}, x_{2}, \ldots, x_{n}]_{f} = \frac{[x_{0}, x_{1}, \ldots, x_{n - 1}]_{f} - [x_{1}, \ldots, x_{n}]_{f}}{x_{0} - x_{n}}.
	\]
	The map $x \mapsto [x, x_{1}, x_{2}, \ldots, x_{n - 1}]$ is $1$-Lipschitz for any $x_{1}, x_{2}, \ldots, x_{n - 1}$ and hence $(n - 1)$:th divided difference is Lipschitz and consequently bounded. By induction hypothesis $f$ is $C^{n - 2}(a, b)$ and hence at least $C^{1}(a, b)$. But now since
	\[
		[x_{1}, x_{2}, \ldots, x_{n}]_{f'} = \sum_{1 \leq i \leq n} [x_{1}, x_{2}, \ldots, x_{i - 1}, x_{i}, x_{i}, x_{i + 1}, \ldots, x_{n}]_{f}
	\]
	$f'$ has bounded $(n - 1)$:th divided differences, and by the induction hypothesis, $f \in C^{(n - 1)}(a, b)$ and $f^{(n - 1)}$ is Lipschitz. Induction also immediately gives
	\[
		\frac{\Lip(f^{(n - 1)})}{n!} \leq \sup_{a < x_{0} < x_{1} < \ldots < x_{n}< b} |[x_{0}, x_{1}, \ldots, x_{n}]_{f}|.
	\]

	Let us then prove the ``only if"-direction. Take any $a < x_{0} < x_{1} < \ldots < x_{n} < b$. By the mean-value theorem for divided differences we have
	\begin{align*}
		\left|[x_{0}, x_{1}, \ldots, x_{n}]_{f} \right| &= \left|\frac{[x_{0}, x_{1}, \ldots, x_{n - 1}]_{f} - [x_{1}, x_{2}, \ldots, x_{n}]_{f}}{x_{0} - x_{n}} \right| \\
		&= \frac{1}{(n - 1)!}\left|\frac{f^{(n - 1)}(\xi_{1}) - f^{(n - 1)}(\xi_{2})}{x_{0} - x_{n}} \right| \\
		&\leq \frac{\Lip(f^{(n - 1)})}{(n - 1)!} \left|\frac{\xi_{1} - \xi_{2}}{x_{0} - x_{n}}\right|
	\end{align*}
	for some $x_{0} \leq \xi_{1} \leq \xi_{2} \leq x_{n}$. Hence $n$:th divided difference is bounded, but the inequality is not quite sharp enough.

	We can make it sharp with some tricks. Firstly, when considering the supremum, we only need to consider tuples where all but one of entries are equal. Indeed pick any $a < x_{0} < x_{1} < x_{2} < \ldots < x_{n}$. Now consider the map $g(x) = [x, x_{0}]_{f}$. This is $C^{n - 1}(x_{0}, b)$ so by the mean-value theorem we have
	\[
		[x_{0}, x_{1}, \ldots, x_{n}]_{f} = [x_{1}, x_{2}, \ldots, x_{n}]_{g} = \frac{g^{(n - 1)}(\xi)}{(n - 1)!} = [\xi, \xi, \ldots, \xi]_{g} = [x_{0}, \xi, \xi, \ldots]_{f}.
	\]
	Hence it suffices to consider only tuples $(x, x, x, \ldots, y)$ where $x$ appears $n$ times. Now by the Taylor's theorem
	\begin{align*}
		\left|[x, x, \ldots, y]_{f}\right| &= \left|\frac{[x, x, \ldots, x]_{f} - [x, x, \ldots, y]}{x - y} \right| \\
		&= \left|\frac{\frac{f^{(n - 1)}(x)}{(n - 1)!} - \int_{x}^{y} \frac{f^{(n - 1)}(t) (y - t)^{n - 2}}{(n - 2)!} dt}{x - y} \right| = \\
		&= \frac{1}{(n - 2)!|y - x|^{n - 1}} \left|\int_{x}^{y} (f^{(n - 1)}(x) - f^{(n - 1)}(t)) (y - t)^{n - 2} dt \right| \\
		&\leq \frac{\Lip(f^{(n - 1)})}{(n - 2)!} \int_{x}^{y} |x - t| |y - t|^{n - 2} dt \\
		&= \frac{\Lip(f^{(n - 1)})}{n!},
	\end{align*}
	and we are done.
\end{proof}

With such tools we are ready to tackle the regularity of $k$-tone functions

\begin{proof}[Proof of the theorem \ref{k-tone_smooth}]

	We start with a lemma.
	\begin{lem}
		If $k \geq 1$ and $f : (a, b) \to \R$ is $k$-tone, then the $(k - 1)$:th divided differences of $f$ are locally bounded, i.e. bounded on every closed subinterval of $(a, b)$.
	\end{lem}
	\begin{proof}
		We induct on $k$. The case $k = 1$ is rather clear: for any $a < c < x < d < b$ we have $f(x) \in [f(c), f(d)]$. Take then $k > 1$ and any closed interval $[c, d] \subset (a, b)$. Take $a < x_{0} < c$. The map $g = [\cdot, x_{0}]_{f}$ is $(k - 1)$-tone, so by induction hypothesis the $(k - 2)$:th divided differences of $g$ are bounded on $[c, d]$. Now for any $c \leq x_{1} < x_{2} < \ldots < x_{k} \leq d$ we have
		\[
			[x_{1}, x_{2}, \ldots, x_{k}]_{f} = (x_{k} - x_{0}) [x_{0}, x_{1}, \ldots, x_{k}]_{f} + [x_{0}, x_{1}, \ldots, x_{k - 1}]_{f} \geq [x_{0}, x_{1}, \ldots, x_{k - 1}]_{f}
		\]
		But $[x_{0}, x_{1}, \ldots, x_{k - 1}]_{f} = [x_{1}, \ldots, x_{k - 1}]_{g}$ is bounded, so we have lower bound for $(k - 1)$:th divided differences of $f$. Similarly, by taking $d < x_{0} < b$ we get upper bound, and we are done.
	\end{proof}
	Combining the lemma with theorem \ref{bounded_div} gives the right smoothness. Convexity condition is implied by the lemma \ref{divdif_der} combined with induction on $k$.
\end{proof}

\section{Analyticity and Bernstein's theorems}

By requiring (some kind of) regularity for the divided differences of all orders, occasionally we get more than smoothness, namely analyticity. Most basic result of this kind is the following.

\begin{lause}\label{div_anal}
	Let $f : (a, b) \to \R$. Then $f$ is real analytic, if and only if for every closed subinteval $[c, d]$ of $(a, b)$ there exists constant $C$ such that for any $n \geq 1$
	\[
		\sup_{a < x_{0} < x_{1} < \ldots < x_{n}< b} |[x_{0}, x_{1}, \ldots, x_{n}]_{f}| \leq C^{n + 1}.
	\]
\end{lause}
\begin{proof}
	Let's first prove that ``if"-direction. We need to prove that the for any $x_{0} \in (a, b)$ Taylor series at $x_{0}$ converges in some neighbourhood of $x_{0}$. As observed before, the $n$:th error term in Taylor series is given by
	\[
		[x, x_{0}, x_{0}, \ldots, x_{0}]_{f} (x - x_{0})^{n}
	\]
	with $n$ $x_{0}$'s. Now choose $a < c < x_{0} < d < b$ and take any $x$ with $x \in [c, d]$ and $|x - x_{0}| C < 1$, where $C$ is given by the assumption for interval $c, d$. But then the error term tends to zero and we are done.

	For the other direction note that if $x_{0} \in (a, b)$ and $f$ extends to analytic function on $\D(x_{0}, r)$, we definititely have $\left|\frac{f^{(n)}(x_{0})}{n!}\right| \leq C^{n + 1}$ for some $C$. If $|x - x_{0}| < r$ we have
	\[
		\frac{f^{(k)}(x)}{k!} = \sum_{n = k}^{\infty} \binom{n}{k} \frac{f^{(n)}(x_{0})}{n!} (x - x_{0})^{n - k},
	\]
	which may be estimated by
	\[
	 \left|\frac{f^{(k)}(x)}{k!}\right| \leq C^{k + 1}\sum_{n = k}^{\infty} \binom{n}{k} C^{n - k} (x - x_{0})^{n - k} = \frac{C^{k + 1} }{(1 - |x - x_{0}| C)^{k}},
	\]
	whenever $|x - x_{0}| C < 1$. By the mean value theorem for divided differences it follows that we get required bound for some neighbourhood of $x_{0}$ and consequently, by compactness for any closed subinteval of $(a, b)$.
\end{proof}

Of course, we could just as well replace the closed inteval by any compact subset of $(a, b)$. The previous result is some kind of relative of \ref{bounded_div}. Also theorem \ref{k-tone_smooth} has rather interesting relative.

\begin{lause}[Bernstein's little theorem]
	If $f : (a, b) \to \R$ is $k$-tone for every $k \geq 0$, then $f$ is real-analytic on $(a, b)$.
\end{lause}
\begin{proof}
	We prove that the conditions of the theorem \ref{div_anal} are satisfied. Pick any $a < x_{0} < x < b$. Now for any $n \geq 0$ we have
	\[
		f(x) = \sum_{k = 0}^{n - 1} \frac{f^{(k)}(x_{0})}{k!}(x - x_{0})^{k} + \frac{f^{(n)}(x_{0})}{n!} (x - x_{0})^{n} + [x, x_{0}, x_{0}, \ldots, x_{0}]_{f} (x - x_{0})^{n + 1}.
	\]
	Note that all the terms on the right-hand side are non-negative, and hence
	\[
		0 \leq \frac{f^{(n)}(x_{0})}{n!} \leq f(x) (x - x_{0})^{-n}.
	\]
	Now given any interval $[c, d] \subset (a, b)$ we can make such estimate uniform over $x_{0} \in [c, d]$ simply by picking $x \in (d, b)$, and we are done.
\end{proof}

We could further conclude that $f$ in the previous theorem extends to a complex analytic function to $\D(a, |b - a|)$.

Usually by Bernstein's little theorem one means slightly weaker statement: if $f : (0, \infty) \to \R$ is smooth such that $(-1)^{n} f^{(n)}(t) \geq 0$ for any $t > 0$, then $f$ extends to analytic function to right half-plane. This is readily implied by the previous observation. Latter version has however a considerable strenghtening.

\begin{lause}[Bernstein's big theorem]
	If $f : (0, \infty) \to \R$ is smooth such that $(-1)^{n} f^{(n)}(t) \geq 0$ for any $t > 0$, then $f$ is Laplace transform of a radon measure $\mu$ on $[0, \infty)$, that is we have
	\[
		f(x) = \int_{0}^{\infty} e^{-x t} d \mu(t)
	\]
	for every $x > 0$.
\end{lause}

We will postpone the proof.

TODO:

\begin{itemize}
	\item Mean value theorem, coefficient of the interpolating polynomial
	\item Basic properties, product rule.
	\item $k$-tone functions, smoothness, and representation
	\item Majorization, Jensen and Karamata inequalities, generalizations, and corollaries concerning spectrum and trace functions. Schur-Horn conjectures and Honey-Combs
	\item Tohoku contains nice proof of Lidskii inequality
	\item How to understand the inequalities arising from $k$-tone functions: is there nice way to parametrize the tuples coming from the $k$-majorization.
	\item For $k = 3$ and $3$ numbers, it's all about the biggers number: one with the largest largest number dominates.
	\item The previous probably generalizes: for $k$-tone functions and $k$ numbers on both sides, with all polynomials of degree less than $k$ vanishing on both tuples, one with largest largest value dominates, or equivalently, it's all about the constant term. This is clearly necessary, by is it also sufficient? Should be: express the whole thing as an integral, differentiate with respect to the constant term, and finally interpret as a divided difference.
	\item What if we add more terms: is there simple characterization? Why have similar integral representation, and can probably differentiate: Maybe not, or one has to be really careful. Is there characterization with linear inequalities (in addition to the equalities)?
	\item Peano Kernels: Smoothness properties, Bernstein (?) polynomials as examples.
	\item Opitz formula
	\item Regularization techniques
	\item Notion of midpoint-convexity should generalize by regularization techniques.
	\item Should Legendre transform generalize to higher orders?  For smooth enough functions probably with derivatives being inverses of each other, but what is the correct definition? And is it of any use? Maybe differentiating $k - 2$ times and then having similar characterization. Is there higher order duality?
	\item Is there elementary transformations for $k$-tone Karamata?
	\item Divided-difference series for entire functions (Newton expansion)? For analytic function? When does it converge? When does it converge to the right function?
	\item Given domain $U \subset \C$ and analytic function $f : U \to \C$, determine all subsets $V \subset U$ such that there exists Newton series with some sequence $x_{1}, x_{2}, \ldots$ converging in $V$. This is very much related to logarithmic potentials and subharmonic functions: sequence, if say bounded for starters, corresponds to a radon measure. Indeed, take weak limit of radon measures averaged exprimental measures of first elements in the sequence, if the limit exists (if not...). Now if $f = \frac{1}{z}$ for starters, we have the logarithmic potential $U(z)$ and the Newton series converges whenever $U(z) < U(0)$.
	\item Harnack-type inequalities for derivatives of Pick functions?
	\item Smooth function is in $P(0, \infty)$ if it's negative of Laplace transform of Laplace transform of a measure on $[0, \infty)$?
	\item Are there better bounds for theorem \ref{bounded_div}?
\end{itemize}
