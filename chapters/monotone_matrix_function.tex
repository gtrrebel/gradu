\chapter{Monotone and Convex matrix functions}

We already introduced monotone and convex matrix functions in the introduction, but now that we have properly defined and discussed underlying structures we should take a deeper look. As mentioned, monotone and convex matrix functions are sort of generalizations for the standard properties of reals, and this is why we should undestand which of the phenomena for the real functions carry to matrix functions and which do not.

We will start with the matrix monotone functions; much of the discussion carries quite directly to the convex case.

\section{Basic properties of the matrix monotone functions}

We first state the definition.

\begin{maar}
	Let $(a,b) \subset \R$ be an open, possibly unbounded interval and $n$ positive integer. We say that $f : (a, b) \to \R$ is $n$-monotone or matrix monotone of order $n$, if for any $A, B \in \H^{n}_{(a, b)}$, such that $A \leq B$ we have $f(A) \leq f(B)$.
\end{maar}

We will denote the space of $n$-monotone functions on open interval $(a, b)$ by $P_{n}(a, b)$. One immediately sees that that all the matrix monotone functions are monotone as real functions.

\begin{prop}
	If $f \in P_{n}(a, b)$, $f$ is increasing.
\end{prop}
\begin{proof}
	Take any $a < x \leq y < b$. Now for $xI, yI \in \H^{n}_{(a, b)}$ we have $x I \leq y I$ so by definition
	\[
		f(x) I = f(xI) \leq f(y I) = f(y) I,
	\]
	from which it follows that $f(x) \leq f(y)$. This is what we wanted.
\end{proof}

Actually, increasing functions have simple and expected role in $n$-monotone matrices.

\begin{prop}
	Let $(a, b)$ be an open interval and $f : (a, b) \to \R$. Then the following are equivalent:
	\begin{enumerate}[(i)]
		\item $f$ is increasing.
		\item $f \in P_{1}(a, b)$.
		\item For any positive integer $n$ and commuting $A, B \in \H^{n}_{(a, b)}$ such that $A \leq B$ we have $f(A) \leq f(B)$ .
	\end{enumerate}
\end{prop}
\begin{proof}
	TODO
\end{proof}

The equivalence of the first two is almost obvious and from this point on we shall identify $1$-monotone and increasing functions. But the third point is very important: it is exactly the non-commutative nature which makes the classes of higher order interesting.

Let us then have some examples.

\begin{prop}
	For any positive integer $n$, open interval $(a, b)$ and $\alpha, \beta \in \R$ such that $\alpha \geq 0$ we have that $(x \mapsto \alpha x + \beta) \in P_{n}(a, b)$.
\end{prop}
\begin{proof}
	Assume that for $A, B \in \H_{(a, b)}$ we have $A \leq B$. Now
	\[
		f(B) - f(A) = (\alpha B + \beta I) - (\alpha A + \beta I) = \alpha (B - A).
	\]
	Since by assumption $B - A \geq $ and $\alpha \geq 0$, also $\alpha (B - A) \geq 0$, so by definition $f(B) \geq f(A)$. This is exactly what we wanted.
\end{proof}

That was easy. It's not very easy to come up with other examples, though. Most of the common monotone functions fail to be matrix monotone. Let's try some non-examples.

\begin{prop}
	Function $(x \mapsto x^2)$ is not $n$-monotone for any $n \geq 2$ and any open interval $(a, b) \subset \R$.
\end{prop}
\begin{proof}
	Let us first think what goes wrong with the standard proof for the case $n = 1$.

	Note that if $A \leq B$,
	\[
		B^2 - A^2 = (B - A) (B + A)
	\]
	is positive as a product of two positive matrices (real numbers).

	There are two fatal flaws here when $n > 1$.
	\begin{itemize}
		\item $(B - A) (B + A) = B^2-A^2 + (B A - A B)$, not $B^2 - A^2$.
		\item Product of two positive matrices need not be positive.
	\end{itemize}
	Note that both of these objections result from the non-commutativity and indeed, both would be fixed should $A$ and $B$ commute.

	Let's write $B = A + H$ ($H \geq 0$). Now we are to investigate
	\[
		(A + H)^2-A^2 = A H + H A + H^2.
	\]
	Note that $H^2 \geq 0$, but as we have seen in TODO, $A H + H A$ need not be positive! Also, if $H$ is small enough, $H^2$ is negligible compared to $AH + HA$. We are ready to formulate our proof strategy: find $A \in \H^{n}_{a, b}$ and $\Hp^{n}$ such that $A H + H A \ngeq 0$. Then choose parameter $t > 0$ so small that $A + t H \in \H^{n}(a,b)$ and
	\[
		(A + t H)^2 - A^2 = t (A H + H A + t H^2) \ngeq 0
	\]
	and set the pair $(A, A + t H)$ as the counterexample.

	TODO
\end{proof}

In a similar manner one could show the similar statement for the functions $(x \mapsto x^k)$.

At this point several other important properties of the matrix monotone functions should be clear.

\begin{prop}
	For any positive integer $n$ and open interval $(a, b)$ the set $P_{n}(a, b)$ is a convex cone, i.e. it is closed under taking summation and multiplication by non-negative scalars.
\end{prop}

\begin{proof}
	This is easy: closedness under summation and scalar multiplication with non-negative scalars correspond exaclty to the same property of positive matrices.
\end{proof}

We should be a bit careful though. As we saw with the square function example, product of two $n$-monotone functions need not be n-monotone in general, even if they are both positive functions; similar statement holds for increasing functions. Similarly, taking maximums doesn't preserve monotonicity.

\begin{prop}
	Maximum of two $n$-monotone functions need not be $n$-monotone for $n \geq 2$.
\end{prop}
\begin{proof}
	Again, let's think what goes wrong with the standard proof for $n = 1$.

	Fix open interval $(a, b)$, positive integer $n \geq 2$ and two functions $f, g \in P^{n}(a, b)$. Take any two $A, B \in \H_{(a, b)}^{n}$ with $A \leq B$. Now $f(A) \leq f(B) \leq \max(f, g)(B)$ and $f(A) \leq f(B) \leq \max(f, g)(B)$. It follows that
	\[
		\max(f, g)(A) = \max(f(A), g(A)) \leq \max(f, g)(B),
	\]
	as we wanted.

	Here the flaw is in the expression $\max(f(A), g(A))$: what is maximum of two matrices? This is an interesting question and we will come back to it a bit later, but it turns out that however you try to define it, you can't satisfy the above inequality.

	We still need proper counterexamples though. Let's try $f \equiv 0$ and $g = \id$. So far the only $n$-monotone functions we know are affine functions so that's essentially our only hope for counterexamples.

	TODO
\end{proof}

Similarly we have composition and pointwise limits.

\begin{prop}
	If $f : (a, b) \to (c, d)$ and $g : (c, d) \to \R$ are $n$-monotone, so is $g \circ f : (a, b) \to \R$.
\end{prop}
\begin{proof}
	Fix any $A, B \in \H^{n}_{(a, b)}$ with $A \leq B$. By assumption $f(A) \leq f(B)$ and $f(A), f(B) \in \H^{n}_{(c, d)}$ so again by assumption, $g(f(A)) \leq g(f(B))$, our claim.
\end{proof}

\begin{prop}
	If $n$-monotone functions $f_{i} : (a, b) \to \R$ converge pointwise to $f : (a, b) \to \R$ as $i \to \infty$, also $f$ is $n$-monotone.
\end{prop}
\begin{proof}
	As always, fix $A, B \in \H^{n}_{(a, b)}$ with $A \leq B$. Now by assumption
	\[
		f(B) - f(A) = \lim_{i \to \infty} f_{i}(B) - \lim_{i \to \infty} f_{i}(A) = \lim_{i \to \infty} \left(f_{i}(B) - f_{i}(A)\right) \geq 0,
	\]
	so also $f \in P_{n}(a, b)$.
\end{proof}

We shall be using especially the previous result a lot.

One of the main properties of the classes of matrix monotone functions has still avoided our discussion, namely the relationship between classes of different orders. We already noticed that matrix monotone functions of all orders all monotonic, or $P_{n}(a,b) \subset P_{1}(a, b)$ for any $n \geq 1$. It should not be very surprising that we can make much more precise inclusions.

\begin{prop}
	For any open interval $(a, b)$ and positive integer $n$ we have $P_{n + 1}(a, b) \subset P_{n}(a, b)$.
\end{prop}
\begin{proof}
	TODO
\end{proof}

One might ask whether these inclusions are strict. It turns out they are, as long as our interval is not the whole $\R$. We will come back to this.

There are also more trivial inclusions: $P_{n}(a, b) \subset P_{n}(c, d)$ for any $(a, b) \supset (c, d)$. More interval, more matrices, more restrictions, less functions. To be precise, we only allowed functions with domain $(a, b)$ to the class $P_{n}(a, b)$, so maybe one should say instead something like: if $(a, b) \supset (c, d)$ and $f \in P_{n}(a, b)$, then also $\restr{f}{(c, d)} \in P_{n}(c, d)$. We will try not to worry too much about these technicalities.

\section{Pick functions are monotonic}
One of the main reasons for introducing Pick functions is that they are monotone of all orders. After understanding real valued monotone functions, it should be clear how to prove the previous statement: as Pick functions are essentially sums of functions of the form $x \mapsto \frac{1}{\lambda - x}$, we should verify that these are monotone. Also $\lambda$ should not have effect on anything so we only need the following. But the increasing nature of the function of $x \mapsto -\frac{1}{x}$ is something we know already. We have hence proved 
\begin{lause}
	If $f \in P(a, b)$, then $f \in P_{n}(a, b)$ for any $n \geq 1$.
\end{lause}
This is obviously why we chose the notation $P_{n}$ for classes of matrix monotone functions.

\section{Monotonicity and derivative}

As in the real case, also in the matrix world we may characterize monotonicity with derivatives.

\begin{lause}
	Let $f \in C^{1}(a, b)$ and $n \geq 1$. Then the following are equivalent:
	\begin{enumerate}[(i)]
	\item For any $A \in \H^{n}_{(a, b)}$ and $H \geq 0$ we have
	\[
		D^{1}_{n}f_{A}(H) \geq 0.
	\]
	\item For any $A \in \H^{n}_{(a, b)}$ and $P$ one dimensional (orthogonal) projection we have
	\[
		D^{1}_{n}f_{A}(P) \geq 0.
	\]
	\item For any $A \in \H^{n}_{(a, b)}$, $H \geq 0$ and $v \in V$ the map
	\[
		t \mapsto \langle f(A + t H) v, v \rangle
	\]
	is increasing.
	\item For any $A \in \H^{n}_{(a, b)}$, $P$ one dimensional (orthogonal) projection and $v \in V$ the map
	\[
		t \mapsto \langle f(A + t P) v, v \rangle
	\]
	is increasing.
	\end{enumerate}
\end{lause}
\begin{proof}
	TODO
\end{proof}

We already noticed that we can express $D^{1}_{n}f_{A}(H) = ([\lambda_{i}, \lambda_{j}]_{f})_{1 \leq i, j \leq n}\circ H$, where Hadamard product is taken along eigenbasis of $A$. We can however make the following simple observation:

\begin{lem}
	Let $A \in \H$. Then $A \geq 0$, if and only if $A \circ B$ for every $B \geq 0$.
\end{lem}

\begin{proof}
	If the Hadamard product is along $(e_{i})_{1 \leq i \leq n}$, we have $A = A \circ \left(\sqrt{n} P_{\frac{1}{\sqrt{n}}\sum_{1 \leq i \leq n} e_{i}} \right)$, and hence have the ``if". Note that for only if we only need to verify the inequality for $A$ and $B$ both one dimensional projections. But now one easily sees that $A \circ B$ is non-negative multiple of projection.
\end{proof}

We hence have the following characterization.

\begin{lause}
	$f \in P_{n}(a, b)$, if and only if the matrix
	\begin{align*}
		\left([\lambda_{i}, \lambda_{j}]_{f}\right)_{i, j} \geq 0
	\end{align*}
	for any $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n} \in (a, b)$.
\end{lause}



TODO:
\begin{itemize}
	\item Examples
	\item Pick functions are monotone
	\item Heaviside function
	\item Trace inequalities: if $f$ is monotone/convex then $\tr f$ is monotone/convex. Proof idea: we may write $\tr f$ as a limit of finite sum of translations of Heaviside functions (monotone case) or absolute values (convex case), so its sufficient to prove the claim for these functions. For monotone case it hence suffices to prove that if $A \leq B$, $B$ has at least as many non-negative eigenvalues as $A$. But this is clear by subspace characterization of non-negative eigenvalues. For convex case, it suffices to prove that $\tr |A| + \tr |B| \geq \tr |A + B|$ for any $A, B \in \H^{n}(a, b)$. For this, note that if $(e_{i})_{i = 1}^{n}$ is eigenbasis of $A + B$, we have
	\begin{eqnarray*}
		\tr |A + B| &=& \sum_{i = 1}^{n} \langle |A + B| e_{i}, e_{i} \rangle \\
		= \sum_{i = 1}^{n} \left|\langle (A + B) e_{i}, e_{i} \rangle \right| &\leq& \sum_{i = 1}^{n} \left|\langle A e_{i}, e_{i} \rangle \right| + \sum_{i = 1}^{n} \left|\langle B e_{i}, e_{i} \rangle \right| \\
		\leq \sum_{i = 1}^{n} \langle |A| e_{i}, e_{i} \rangle + \sum_{i = 1}^{n} \langle |B| e_{i}, e_{i} \rangle &=& \tr |A| + \tr |B|
	\end{eqnarray*}
	\item What about trace inequalities for $k$-tone functions? Eigen-package seems to find a counterexample for $6$-tone functions and $n = 2$, but it's hard to see if there's enough numerical stability. At divided differences of polynomials vanish. First non-trivial question would be:
	If $A_{j} = A + j H$ for $0 \leq j \leq 3$ and $H \geq 0$. Then is it necessarily the case that
	\[
		\tr \left(A_{3} |A_{3}| - 3 A_{2}|A_{2}| + 3 A_{1} |A_{1}| - A_{0} |A_{0}| \right) \geq 0?
	\]
	This would imply that $3$-tone functions would lift to trace $3$-tone functions. Maybe expressing this as a contour integral from $-i \infty \to i \infty$ a same tricks as in the paper. First projection case: $H$ is projection. Or: approximate by integrals of heat kernels. It should be sufficient to proof things for $k$-fold integrals or heat kernel, or by scaling just for gaussian function.
	\item How is the previous related to the $|\cdot|$ not being operator-convex: quadratic form inequality for eigenvectors is not enough.
	\item The previous also implies that
	\[
		f(Q_{A}(v)) \leq Q_{f(A)}(v)
	\]
	for any convex $f$. Using this and Minkowski one sees that $p$-schatten norms are indeed norms.
	\item For $f, g$ generalization (Look at $h(X) = g (\tr f(X))$) we need that $f$ is convex. What else? $h$ is convex if it is convex for diagonalizable matrices and $f$ is convex and $g$ increasing. For the diagonalizable maps it is sufficient that $f$ is increasing and $g = f^{-1}$ and $\log \circ f \circ \exp$ is convex.
	\item Von Neumann trace inequality, more trace inequalities.
	\item On Generalizations of Minkowski's Inequality in the Form of a Triangle Inequality, Mulholland
	\item Positive derivative
	\item Smoothness properties
	\item Characterizations
\end{itemize}
