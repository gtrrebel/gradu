\chapter{Monotone matrix functions}

We already introduced monotone matrix functions in the introduction, but now that we have properly defined and discussed underlying structures we should take a deeper look. As mentioned, monotone matrix functions are sort of generalizations for the standard properties of reals, and this is why we should undestand which of the phenomena for the real functions carry to matrix functions and which do not.

\section{Basic properties}

We first state the definition.

\begin{maar}
	Let $(a,b) \subset \R$ be an open, possibly unbounded interval and $n$ positive integer. We say that $f : (a, b) \to \R$ is $n$-monotone or matrix monotone of order $n$, if for any $A, B \in \H^{n}_{(a, b)}$, such that $A \leq B$ we have $f(A) \leq f(B)$.
\end{maar}

We will denote the space of $n$-monotone functions on open interval $(a, b)$ by $P_{n}(a, b)$. One immediately sees that that all the matrix monotone functions are monotone as real functions.

\begin{prop}
	If $f \in P_{n}(a, b)$, $f$ is increasing.
\end{prop}
\begin{proof}
	Take any $a < x \leq y < b$. Now for $xI, yI \in \H^{n}_{(a, b)}$ we have $x I \leq y I$ so by definition
	\[
		f(x) I = f(xI) \leq f(y I) = f(y) I,
	\]
	from which it follows that $f(x) \leq f(y)$. This is what we wanted.
\end{proof}

Actually, increasing functions have simple and expected role in $n$-monotone matrices.

\begin{prop}
	Let $(a, b)$ be an open interval and $f : (a, b) \to \R$. Then the following are equivalent:
	\begin{enumerate}[(i)]
		\item $f$ is increasing.
		\item $f \in P_{1}(a, b)$.
		\item For any positive integer $n$ and commuting $A, B \in \H^{n}_{(a, b)}$ such that $A \leq B$ we have $f(A) \leq f(B)$ .
	\end{enumerate}
\end{prop}
\begin{proof}
	TODO
\end{proof}

The equivalence of the first two is almost obvious and from this point on we shall identify $1$-monotone and increasing functions. But the third point is very important: it is exactly the non-commutative nature which makes the classes of higher order interesting.

Let us then have some examples.

\begin{prop}
	For any positive integer $n$, open interval $(a, b)$ and $\alpha, \beta \in \R$ such that $\alpha \geq 0$ we have that $(x \mapsto \alpha x + \beta) \in P_{n}(a, b)$.
\end{prop}
\begin{proof}
	Assume that for $A, B \in \H_{(a, b)}$ we have $A \leq B$. Now
	\[
		f(B) - f(A) = (\alpha B + \beta I) - (\alpha A + \beta I) = \alpha (B - A).
	\]
	Since by assumption $B - A \geq $ and $\alpha \geq 0$, also $\alpha (B - A) \geq 0$, so by definition $f(B) \geq f(A)$. This is exactly what we wanted.
\end{proof}

That was easy. It's not very easy to come up with other examples, though. Most of the common monotone functions fail to be matrix monotone. Let's try some non-examples.

\begin{prop}
	Function $(x \mapsto x^2)$ is not $n$-monotone for any $n \geq 2$ and any open interval $(a, b) \subset \R$.
\end{prop}
\begin{proof}
	Let us first think what goes wrong with the standard proof for the case $n = 1$.

	Note that if $A \leq B$,
	\[
		B^2 - A^2 = (B - A) (B + A)
	\]
	is positive as a product of two positive matrices (real numbers).

	There are two fatal flaws here when $n > 1$.
	\begin{itemize}
		\item $(B - A) (B + A) = B^2-A^2 + (B A - A B)$, not $B^2 - A^2$.
		\item Product of two positive matrices need not be positive.
	\end{itemize}
	Note that both of these objections result from the non-commutativity and indeed, both would be fixed should $A$ and $B$ commute.

	Let's write $B = A + H$ ($H \geq 0$). Now we are to investigate
	\[
		(A + H)^2-A^2 = A H + H A + H^2.
	\]
	Note that $H^2 \geq 0$, but as we have seen in TODO, $A H + H A$ need not be positive! Also, if $H$ is small enough, $H^2$ is negligible compared to $AH + HA$. We are ready to formulate our proof strategy: find $A \in \H^{n}_{a, b}$ and $\Hp^{n}$ such that $A H + H A \ngeq 0$. Then choose parameter $t > 0$ so small that $A + t H \in \H^{n}(a,b)$ and
	\[
		(A + t H)^2 - A^2 = t (A H + H A + t H^2) \ngeq 0
	\]
	and set the pair $(A, A + t H)$ as the counterexample.

	TODO
\end{proof}

In a similar manner one could show the similar statement for the functions $(x \mapsto x^k)$.

At this point several other important properties of the matrix monotone functions should be clear.

\begin{prop}
	For any positive integer $n$ and open interval $(a, b)$ the set $P_{n}(a, b)$ is a convex cone, i.e. it is closed under taking summation and multiplication by non-negative scalars.
\end{prop}

\begin{proof}
	This is easy: closedness under summation and scalar multiplication with non-negative scalars correspond exaclty to the same property of positive matrices.
\end{proof}

We should be a bit careful though. As we saw with the square function example, product of two $n$-monotone functions need not be n-monotone in general, even if they are both positive functions; similar statement holds for increasing functions. Similarly, taking maximums doesn't preserve monotonicity.

\begin{prop}
	Maximum of two $n$-monotone functions need not be $n$-monotone for $n \geq 2$.
\end{prop}
\begin{proof}
	Again, let's think what goes wrong with the standard proof for $n = 1$.

	Fix open interval $(a, b)$, positive integer $n \geq 2$ and two functions $f, g \in P^{n}(a, b)$. Take any two $A, B \in \H_{(a, b)}^{n}$ with $A \leq B$. Now $f(A) \leq f(B) \leq \max(f, g)(B)$ and $f(A) \leq f(B) \leq \max(f, g)(B)$. It follows that
	\[
		\max(f, g)(A) = \max(f(A), g(A)) \leq \max(f, g)(B),
	\]
	as we wanted.

	Here the flaw is in the expression $\max(f(A), g(A))$: what is maximum of two matrices? This is an interesting question and we will come back to it a bit later, but it turns out that however you try to define it, you can't satisfy the above inequality.

	We still need proper counterexamples though. Let's try $f \equiv 0$ and $g = \id$. So far the only $n$-monotone functions we know are affine functions so that's essentially our only hope for counterexamples.

	TODO
\end{proof}

Similarly we have composition and pointwise limits.

\begin{prop}
	If $f : (a, b) \to (c, d)$ and $g : (c, d) \to \R$ are $n$-monotone, so is $g \circ f : (a, b) \to \R$.
\end{prop}
\begin{proof}
	Fix any $A, B \in \H^{n}_{(a, b)}$ with $A \leq B$. By assumption $f(A) \leq f(B)$ and $f(A), f(B) \in \H^{n}_{(c, d)}$ so again by assumption, $g(f(A)) \leq g(f(B))$, our claim.
\end{proof}

\begin{prop}
	If $n$-monotone functions $f_{i} : (a, b) \to \R$ converge pointwise to $f : (a, b) \to \R$ as $i \to \infty$, also $f$ is $n$-monotone.
\end{prop}
\begin{proof}
	As always, fix $A, B \in \H^{n}_{(a, b)}$ with $A \leq B$. Now by assumption
	\[
		f(B) - f(A) = \lim_{i \to \infty} f_{i}(B) - \lim_{i \to \infty} f_{i}(A) = \lim_{i \to \infty} \left(f_{i}(B) - f_{i}(A)\right) \geq 0,
	\]
	so also $f \in P_{n}(a, b)$.
\end{proof}

We shall be using especially the previous result a lot.

One of the main properties of the classes of matrix monotone functions has still avoided our discussion, namely the relationship between classes of different orders. We already noticed that matrix monotone functions of all orders all monotonic, or $P_{n}(a,b) \subset P_{1}(a, b)$ for any $n \geq 1$. It should not be very surprising that we can make much more precise inclusions.

\begin{prop}
	For any open interval $(a, b)$ and positive integer $n$ we have $P_{n + 1}(a, b) \subset P_{n}(a, b)$.
\end{prop}
\begin{proof}
	TODO
\end{proof}

One might ask whether these inclusions are strict. It turns out they are, as long as our interval is not the whole $\R$. We will come back to this.

There are also more trivial inclusions: $P_{n}(a, b) \subset P_{n}(c, d)$ for any $(a, b) \supset (c, d)$. More interval, more matrices, more restrictions, less functions. To be precise, we only allowed functions with domain $(a, b)$ to the class $P_{n}(a, b)$, so maybe one should say instead something like: if $(a, b) \supset (c, d)$ and $f \in P_{n}(a, b)$, then also $\restr{f}{(c, d)} \in P_{n}(c, d)$. We will try not to worry too much about these technicalities.

\section{Pick functions are monotonic}
One of the main reasons for introducing Pick functions is that they are monotone of all orders. After understanding real valued monotone functions, it should be clear how to prove the previous statement: as Pick functions are essentially sums of functions of the form $x \mapsto \frac{1}{\lambda - x}$, we should verify that these are monotone. Also $\lambda$ should not have effect on anything so we only need the following. But the increasing nature of the function of $x \mapsto -\frac{1}{x}$ is something we know already. We have hence proved 
\begin{lause}
	If $f \in P(a, b)$, then $f \in P_{n}(a, b)$ for any $n \geq 1$.
\end{lause}
This is obviously why we chose the notation $P_{n}$ for classes of matrix monotone functions.

\section{Derivative and Loewner's characterization}

As in the real case, also in the matrix world we may characterize monotonicity with derivatives.

\begin{lause}
	Let $f \in C^{1}(a, b)$ and $n \geq 1$. Then the following are equivalent:
	\begin{enumerate}[(i)]
	\item For any $A \in \H^{n}_{(a, b)}$ and $H \geq 0$ we have
	\[
		D^{1}_{n}f_{A}(H) \geq 0.
	\]
	\item For any $A \in \H^{n}_{(a, b)}$ and $P$ one dimensional (orthogonal) projection we have
	\[
		D^{1}_{n}f_{A}(P) \geq 0.
	\]
	\item For any $A \in \H^{n}_{(a, b)}$, $H \geq 0$ and $v \in V$ the map
	\[
		t \mapsto \langle f(A + t H) v, v \rangle
	\]
	is increasing.
	\item For any $A \in \H^{n}_{(a, b)}$, $P$ one dimensional (orthogonal) projection and $v \in V$ the map
	\[
		t \mapsto \langle f(A + t P) v, v \rangle
	\]
	is increasing.
	\end{enumerate}
\end{lause}
\begin{proof}
	TODO
\end{proof}

We already noticed that we can express $D^{1}_{n}f_{A}(H) = ([\lambda_{i}, \lambda_{j}]_{f})_{1 \leq i, j \leq n}\circ H$, where Hadamard product is taken along eigenbasis of $A$. We can however make the following simple observation:

\begin{lem}
	Let $A \in \H$. Then $A \geq 0$, if and only if $A \circ B$ for every $B \geq 0$.
\end{lem}

\begin{proof}
	If the Hadamard product is along $(e_{i})_{1 \leq i \leq n}$, we have $A = A \circ \left(\sqrt{n} P_{\frac{1}{\sqrt{n}}\sum_{1 \leq i \leq n} e_{i}} \right)$, and hence have the ``if". Note that for only if we only need to verify the inequality for $A$ and $B$ both one dimensional projections. But now one easily sees that $A \circ B$ is non-negative multiple of projection.
\end{proof}

We hence have the following characterization.

\begin{lause}
	$f \in P_{n}(a, b) \cap C^{1}(a, b)$, if and only if the matrix
	\begin{align}\label{Loewner_matrix}
		\left([\lambda_{i}, \lambda_{j}]_{f}\right)_{i, j} \geq 0
	\end{align}
	for any $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n} \in (a, b)$.
\end{lause}

This is the original characterization by Loewner, and it is pretty much just saying that function is matrix monotone if its (matrix) derivative is positive. The matrix \ref{Loewner_matrix} is called, appropriately, Loewner matrix (of function $f$ on points $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$). Using the characterization it is in general not very easy to check that the function is $n$-monotone: we would have to check positivity of the matrix for any tuple on the interval. Also the characterization is not local one: in order to check monotonicity we need to know the behaviour on the whole interval. This is just a reflection of the fact that the space in which we are working on, space of real maps, is itself in a way spread around the interval.

\section{Local characterization}

It nevertheless turns out that $n$-monotonicity is a local property.

\begin{prop}
	For any $n \geq 1$, $P_{n}$ is a local property meaning that whenever $f \in P_{n}(a, b)$ and $P_{n}(c, d)$ for some $a < c < b < d$, then also $f \in P_{n}(a, d)$.
\end{prop}

The reason for this is hidden in the Loewner matrix.

Note that Loewner matrix is essentially something we saw before: it is just a Pick matrix when all the points are on the real line. We observed before that positivity of the Pick matrix was some kind of manifestation of the strength of the Cauchy's integral formula. Namely, if $f$ happens to analytic, in some suitable set, we can write
\begin{align*}
	\left([\lambda_{i}, \lambda_{j}]_{f}\right)_{i, j} = \frac{1}{2 \pi i}\int_{\gamma} \frac{f(z)}{(z - \lambda_{i})(z - \lambda_{j})} dz.
\end{align*}
Now the positivity the matrix means that for any $c_{1}, c_{2}, \ldots, c_{n} \in \C$ the quantity
\begin{align*}
	\sum_{1 \leq i, j \leq n} c_{i} \overline{c_{j}} [\lambda_{i}, \lambda_{j}]_{f} = \frac{1}{2 \pi i}\int_{\gamma}f(z) \left(\sum_{1 \leq i \leq n} \frac{c_{i}}{z - \lambda_{i}}\right) \left( \sum_{1 \leq i \leq n} \frac{\overline{c_{i}}}{z - \lambda_{i}}\right)dz.
\end{align*}
But here's the trick: we may write
\begin{align*}
	\sum_{1 \leq i \leq n} \frac{c_{i}}{z - \lambda_{i}} = \frac{q(z)}{\prod_{1 \leq i \leq n} (z - \lambda_{i})} = \frac{q(z)}{p_{\Lambda}(z)}
\end{align*}
for some polynomial of degree less than $n$, and indeed, if the $\lambda$'s are distinct there's a one-to-one correspondence between polynomials $q$ and the $\lambda$'s. It follows that we may rewrite the integral as
\begin{align*}
\frac{1}{2 \pi i}\int_{\gamma}f(z) \frac{q(z) \overline{q(\overline{z})}}{p_{\Lambda}(z)^2}dz.
\end{align*}
Note that $z \mapsto q(z)\overline{q(\overline{z})})$ is a polynomial of degree at most $(2 n - 2)$ non-negative on the real line. Easy application of the Fundamental theorem of algebra reveals that all such polynomials are actually of the previous form. Write $h(z) = q(z)\overline{q(\overline{z})})$.

Finally note that resulting expression,
\begin{align*}
\frac{1}{2 \pi i}\int_{\gamma}f(z) \frac{h(z)}{p_{\Lambda}(z)^2}dz
\end{align*}
is nothing but the divided difference of the function $f h$ at points $\lambda_{1}, \lambda_{1}, \lambda_{2}, \lambda_{2}, \ldots, \lambda_{n}, \lambda_{n}$. By \ref{bootstrap_lemma} this extends to all $C^{1}(a, b)$. We would like to conclude that $f h$ is $(2 n - 1)$-tone, but unfortunately we only know the non-negativity of the divided differences of order $(2 n - 1)$ special sets of tuples. It however turns out that this is enough, as can be seen by using the same trick as in the proof of theorem \ref{bounded_div}.

\begin{lem}
	Let $k$ and $n$ be positive integers and $d_{1}, d_{2}, \ldots, d_{m}$ be positive integers with $d_{1} + d_{2} + \ldots + d_{m} = k + 1$. Assume that $n \geq \left(\max_{1 \leq i \leq m} d_{i} \right) - 1$. Let $f \in C^{n}(a, b)$. Then the following are equivalent.
	\begin{enumerate}[(i)]
		\item $f$ is $k$-tone.
		\item For any $a < x_{1} < x_{2} < \ldots < x_{m} < b$ we have
		\begin{align*}
			[x_{1}, x_{1}, \ldots, x_{1}, x_{2}, \ldots, x_{2}, \ldots, x_{m}, \ldots, x_{m}]_{f} \geq 0
		\end{align*}
		where $x_{i}$ appears $d_{i}$ times.
	\end{enumerate}
\end{lem}

\begin{proof}
	$(i) \Rightarrow (ii)$ is clear.

	For the other direction let's take any $a < y_{1} < y_{2} < \ldots < y_{n} < y_{n + 1} < b$. We need to prove that
	\begin{align*}
		[y_{1}, y_{2}, \ldots, y_{k}, y_{k + 1}]_{f} \geq 0.
	\end{align*}
	The idea is to bunch the variables together using the mean value theorem.

	Let's consider the function $g_{1}(x) = [x, y_{d_{1} + 1}, y_{d_{1} + 2}, \ldots, y_{k + 1}]_{f}$. In terms of $g_{1}$ we need to prove that
	\begin{align*}
		[y_{1}, y_{2}, \ldots, y_{d + 1}]_{g_{1}}.
	\end{align*}
	Since $f \in C^{d_{1} - 1}(a, b)$, $g_{1} \in C^{d_{1} - 1}(a, y_{d_{1} + 1})$ and hence by the mean value theorem we have
	\begin{align*}
		[y_{1}, y_{2}, \ldots, y_{d_{1}}]_{g} = [x_{1}, x_{1}, \ldots, x_{1}]_{g_{1}}
	\end{align*}
	for some $a < x_{1} < y_{d + 1}$. Consequently,
	\begin{align*}
		[y_{1}, y_{2}, \ldots, y_{k}, y_{k + 1}]_{f} = [x_{1}, x_{1}, \ldots, x_{1}, y_{d_{1} + 1}, \ldots, y_{k + 1}]_{f}.
	\end{align*}
	Next step is to bunch together the next $d_{2}$ terms: consider now the map $g_{2}(x) = [x_{1}, x_{1}, \ldots, x_{1}, x, y_{d_{1} + d_{2} + 1, \ldots, y_{k + 1}}]_{f}$ and observe that we are to verify that
	\[
		[y_{d_{1} + 1}, \ldots, y_{d_{1} + d_{2}}]_{g_{2}} \geq 0.
	\]
	Again use mean value theorem to replace $y_{d_{1} + 1}, \ldots, y_{d_{1} + d_{2}}$ by $x_{2}$'s.

	One should be bit careful here: the number $x_{1}$ certainly depends on all the $y$'s, so once we have fixed it we can't say that
	\begin{align*}
		[y_{d_{1} + 1}', y_{d_{1} + 2}', \ldots, y_{d_{1} + d_{2}}']_{g_{2}} = [y_{1}, \ldots, y_{d_{1}}, y_{d_{1} + 1}', \ldots, y_{d_{1} + d_{2}}', y_{d_{1} + d_{2} + 1}, \ldots, y_{k + 1}]_{f},
	\end{align*}
	for instance, anymore. This is of course not a problem.

	Making $m$ steps of the previous form we finally find numbers $x_{1}, x_{2}, \ldots, x_{m}$ such that
	\begin{align*}
		[y_{1}, y_{2}, \ldots, y_{k}, y_{k + 1}]_{f} = [x_{1}, x_{1}, \ldots, x_{1}, x_{2}, \ldots, x_{2}, \ldots, x_{m}, \ldots, x_{m}]_{f} \geq 0
	\end{align*}
	and we are done.
\end{proof}

We have hence the following.

\begin{lause}
	Let $n \geq 1$ and $f \in C^{1}(a, b)$. Then $f \in P_{n}(a, b)$, if and only if $f h$ is $(2 n - 1)$-tone whenever $h$ is polynomial of degree at most $(2 n - 2)$, non-negative on real line. 
\end{lause}

This result has a curious corollary.

\begin{kor}\label{k_tone_cor}
	If $n \geq 1$ and $f h$ is $(2 n + 1)$:tone for every polynomial $h$ of degree at most $2 n$, then $f h$ is $(2 n - 1)$-tone for every polynomial $h$ of degree at most $(2 n - 2)$. In particular $f$ is $k$-tone for every $k = 1, 3, 5, \ldots, 2n - 3, 2 n - 1, 2n + 1$.
\end{kor}
Although we strictly speaking only proved the result for $f \in C^{1}$, it holds true without extra assumptions, as can be seen with the following alternate proof.

\begin{proof}[Proof of corollary \ref{k_tone_cor}]
	Take any $h$, a polynomial of at most $(2 n - 2)$ non-negative on real axis and points $a < \lambda_{1} < \lambda_{2} < \ldots < \lambda_{2 n} < b$. We should prove that
	\begin{align*}
		[\lambda_{1}, \lambda_{2}, \ldots, \lambda_{2 n}]_{f h} \geq 0.
	\end{align*}
	By the assumption, for any $c_{1}, c_{2} \in \R$ and $a < t_{1}, t_{2} < b$ we have
	\begin{align*}
		[\lambda_{1}, \lambda_{2}, \ldots, \lambda_{2 n}, t_{1}, t_{2}]_{f h (c_{1} (\cdot - t_{1}) + c_{2} (\cdot - t_{2}))^2} \geq 0.
	\end{align*}
	Expanding this using linearity and Leibniz rule leads to
	\begin{align*}
		0 &\leq (c_{1} + c_{2})^{2} [\lambda_{1}, \lambda_{2}, \ldots, \lambda_{2 n}]_{f h} \\
		&+ c_{1}^2 (t_{2} - t_{1}) [\lambda_{1}, \lambda_{2}, \ldots, \lambda_{2 n}, t_{2}]_{f h} \\
		&+ c_{2}^2 (t_{1} - t_{2}) [\lambda_{1}, \lambda_{2}, \ldots, \lambda_{2 n}, t_{1}]_{f h}.
	\end{align*}
	We would like to choose $c_{1}$ and $c_{2}$ such that sum of the last two terms is non-positive, but $c_{1} + c_{2} \neq 0$: then $[\lambda_{1}, \lambda_{2}, \ldots, \lambda_{2 n}]_{f h}$ would have to be non-negative. By choosing $c_{1}$ and $c_{2}$ non-negative this can be done if can make any of the two last terms negative. Now if $[\lambda_{1}, \lambda_{2}, \ldots, \lambda_{2 n}, t_{1}]_{f h} \neq 0$ for some $t_{1}$, this is easily done. If $[\lambda_{1}, \lambda_{2}, \ldots, \lambda_{2 n}, t_{1}]_{f h} = 0$ for any $t_{1}$, then the last two terms vanish anyway, so we are again done.

	Final claim follows by setting $h \equiv 1$.
\end{proof}

\section{Main Theorem}

Finally, one might ask whether we can get rid of $C^{1}$-assumption, and it turns out that we can.

\begin{lause}
	Let $n \geq 1$. Then $f \in P_{n}(a, b)$, if and only if $f h$ is $(2 n - 1)$-tone whenever $h$ is polynomial of degree at most $(2 n - 2)$, non-negative on real line. 
\end{lause}
\begin{proof}
	For the version with extra assumption, the starting point was to take derivative of the matrix function. Although we now cannot do that, we can try to replicate the proof otherwise.

	Instead of proving that
	\begin{align*}
		[\lambda_{1}, \lambda_{1}, \lambda_{2}, \lambda_{2}, \ldots, \lambda_{n}, \lambda_{n}]_{f h} \geq 0
	\end{align*}
	for any $a < \lambda_{1}, \lambda_{2}, \ldots, \lambda_{n - 1}, \lambda_{n} < b$, we should prove that
	\begin{align*}
		[\lambda_{1}, \lambda_{2}, \lambda_{3}, \lambda_{4}, \ldots, \lambda_{2n - 1}, \lambda_{2 n}]_{f h} \geq 0.
	\end{align*}
	$\lambda$'s should eigenvalues of some map, but now there are $2 n$ of them. Natural guess would be that they are eigenvalues of two maps, $A$ and $B$.

	But now everything starts to make sense: whenever $A$, $B$ with $A \leq B$ and $v \in V$ the quantity
	\begin{align*}
		\langle (f(B) - f(A)) v, v \rangle
	\end{align*}
	is non-negative. On the other hand this can be expanded as some kind of linear combination of values of $f$ at eigenvalues of $A$ and $B$. Same is true for the divided differences, so there might be change to choose $A$, $B$ and $v$ such that
	\begin{align*}
		\langle (f(B) - f(A)) v, v \rangle = [\lambda_{1}, \lambda_{2}, \lambda_{3}, \lambda_{4}, \ldots, \lambda_{2n - 1}, \lambda_{2 n}]_{f h}.
	\end{align*}
	Moreover should we find some kind of correspondence between triplets $(A, B, v)$ and pairs $((\lambda_{i})_{i = 1}^{2 n}, h)$, we would be done. This is the content of the main lemma.

	\begin{lem}\label{main_lemma}
		If $a < \lambda_{1} < \lambda_{2} < \ldots < \lambda_{2 n - 1} < \lambda_{2 n} < b$ and $h$ is polynomial of degree at most $(2 n - 2)$ non-negative on the real line, we may find $a I < A \leq  B < b I$ and $v \in V$ such that
		\begin{align*}
			\langle (f(B) - f(A)) v, v \rangle = [\lambda_{1}, \lambda_{2}, \lambda_{3}, \lambda_{4}, \ldots, \lambda_{2n - 1}, \lambda_{2 n}]_{f h}
		\end{align*}
		for any $f : (a, b) \to \R$.

		Conversely, if $a I < A \leq B < b I$ such that $B - A = c P_{w}$ for some $c > 0$ and $w$ not othogonal to any eigenvector of $A$, then there exists $a < \lambda_{1} < \lambda_{2} < \ldots < \lambda_{2 n - 1} < \lambda_{2 n} < b$ and polynomial $h$ of degree at most $(2 n - 2)$, non-negative on the real line such that for any $f : (a, b) \to \R$ we have
		\begin{align*}
			\langle (f(B) - f(A)) v, v \rangle = [\lambda_{1}, \lambda_{2}, \lambda_{3}, \lambda_{4}, \ldots, \lambda_{2n - 1}, \lambda_{2 n}]_{f h}.
		\end{align*}
	\end{lem}

	Before proving the lemma we show how it implies the theorem.

	Assume first that $f \in P_{n}(a, b)$. We need to prove that $f h$ is $(2 n - 1)$-tone for any $h$ polynomial of degree at most $(2 n - 2)$ non-negative on real line. But any divided difference of such $f h$ can be expressed by the main lemma \ref{main_lemma} as $\langle (f(B) - f(A)) v, v \rangle$ for some $aI < A \leq B < bI$, and the previous is non-negative by the assumption.

	Conversely, assume that $f h$ is $(2 n - 1)$-tone for any suitable $h$ and take any $A \leq B$. Write $B - A = \sum_{i = 1}^{n} c_{i} P_{v_{i}}$ for some $c_{i} \geq 0$. To prove that $f(B) - f(A) \geq 0$ we simply need to prove that $f(A + \sum_{i = 1}^{k} c_{i} P_{v_{i}}) - f(A + \sum_{i = 1}^{k - 1} c_{i} P_{v_{i}}) \geq 0$ for any $1 \leq k \leq n$, as $f(B) - f(A)$ is sum of such terms. We may hence assume that $B - A$ is positive multiple of one dimensional projection.

	We may also assume that if $B - A = c P_{w}$, then $w$ is not orthogonal to any eigenvector of $A$. Indeed, if this would be the case, we could decompose $V = \vspan\{v_{1}\} \oplus V'$, where $v_{1}$ is the eigenvector, and factorize $A = \arestr{A}{\vspan\{v_{1}\}} \oplus \arestr{A}{V'}$ and $P_{w} = 0 \oplus \arestr{(P_{w})}{V'}$. But now checking that $f(B) - f(A) \geq 0$ boils down to checking that $f(\arestr{B}{V'}) - f(\arestr{A}{V'}) \geq 0$, which would follow if we could prove that $f \in P_{n - 1}(a, b)$. But this follows if we induct on $n$ in the main statement and use lemma \ref{k_tone_cor}.

	Finally in the this, by the lemma \ref{main_lemma} we may find $a < \lambda_{1} < \lambda_{2} < \ldots < \lambda_{2 n - 1} < \lambda_{2 n} < b$ such that
	\begin{align*}
		\langle (f(B) - f(A)) v, v \rangle = [\lambda_{1}, \lambda_{2}, \lambda_{3}, \lambda_{4}, \ldots, \lambda_{2n - 1}, \lambda_{2 n}]_{f h} \geq 0
	\end{align*}
	and we are finally done.

	In the ``if"-direction we could alternatively make use of the continuity of $f$, which is guaranteed by the lemma \ref{k-tone_smooth}

\end{proof}

Let us then complete proof by proving the lemma \ref{main_lemma}.

\begin{proof}[Proof of lemma \ref{main_lemma}]
	The proof is based on lemma TODO.
\end{proof}

TODO:
\begin{itemize}
	\item Examples
	\item Pick functions are monotone
	\item Heaviside function
	\item Trace inequalities: if $f$ is monotone/convex then $\tr f$ is monotone/convex. Proof idea: we may write $\tr f$ as a limit of finite sum of translations of Heaviside functions (monotone case) or absolute values (convex case), so its sufficient to prove the claim for these functions. For monotone case it hence suffices to prove that if $A \leq B$, $B$ has at least as many non-negative eigenvalues as $A$. But this is clear by subspace characterization of non-negative eigenvalues. For convex case, it suffices to prove that $\tr |A| + \tr |B| \geq \tr |A + B|$ for any $A, B \in \H^{n}(a, b)$. For this, note that if $(e_{i})_{i = 1}^{n}$ is eigenbasis of $A + B$, we have
	\begin{eqnarray*}
		\tr |A + B| &=& \sum_{i = 1}^{n} \langle |A + B| e_{i}, e_{i} \rangle \\
		= \sum_{i = 1}^{n} \left|\langle (A + B) e_{i}, e_{i} \rangle \right| &\leq& \sum_{i = 1}^{n} \left|\langle A e_{i}, e_{i} \rangle \right| + \sum_{i = 1}^{n} \left|\langle B e_{i}, e_{i} \rangle \right| \\
		\leq \sum_{i = 1}^{n} \langle |A| e_{i}, e_{i} \rangle + \sum_{i = 1}^{n} \langle |B| e_{i}, e_{i} \rangle &=& \tr |A| + \tr |B|
	\end{eqnarray*}
	\item What about trace inequalities for $k$-tone functions? Eigen-package seems to find a counterexample for $6$-tone functions and $n = 2$, but it's hard to see if there's enough numerical stability. At divided differences of polynomials vanish. First non-trivial question would be:
	If $A_{j} = A + j H$ for $0 \leq j \leq 3$ and $H \geq 0$. Then is it necessarily the case that
	\[
		\tr \left(A_{3} |A_{3}| - 3 A_{2}|A_{2}| + 3 A_{1} |A_{1}| - A_{0} |A_{0}| \right) \geq 0?
	\]
	This would imply that $3$-tone functions would lift to trace $3$-tone functions. Maybe expressing this as a contour integral from $-i \infty \to i \infty$ a same tricks as in the paper. First projection case: $H$ is projection. Or: approximate by integrals of heat kernels. It should be sufficient to proof things for $k$-fold integrals or heat kernel, or by scaling just for gaussian function.
	\item How is the previous related to the $|\cdot|$ not being operator-convex: quadratic form inequality for eigenvectors is not enough.
	\item The previous also implies that
	\[
		f(Q_{A}(v)) \leq Q_{f(A)}(v)
	\]
	for any convex $f$. Using this and Minkowski one sees that $p$-schatten norms are indeed norms.
	\item For $f, g$ generalization (Look at $h(X) = g (\tr f(X))$) we need that $f$ is convex. What else? $h$ is convex if it is convex for diagonalizable matrices and $f$ is convex and $g$ increasing. For the diagonalizable maps it is sufficient that $f$ is increasing and $g = f^{-1}$ and $\log \circ f \circ \exp$ is convex.
	\item Von Neumann trace inequality, more trace inequalities.
	\item On Generalizations of Minkowski's Inequality in the Form of a Triangle Inequality, Mulholland
	\item Positive derivative
	\item Smoothness properties
	\item Characterizations
	\item There should nice proof for Loewner theorem, like the blog post for Bernstein's big theorem.
\end{itemize}
