\chapter{Matrix monotone functions -- part 1}

\begin{maar}\label{n_mon_maar}
	We say that $f : (a, b) \to \R$ is $n$-monotone or matrix monotone of order $n$, if for any ($n \times n$ matrices) $A, B \in \H_{(a, b)}$, such that $A \leq B$ we have $f(A) \leq f(B)$.
\end{maar}

We will denote the space of $n$-monotone functions on open interval $(a, b)$ by $P_{n}(a, b)$. We will also denote
\begin{align*}
	P_{\infty}(a, b) = \bigcap_{n \geq 1} P_{n}(a, b).
\end{align*}

By Theorem \ref{positive_machine} we have

\begin{prop}
	The sets $P_{n}(a, b)$ and $P_{\infty}(a, b)$ are closed convex cones.
\end{prop}

Note that in the notation $P_{n}(a, b)$ we don't specify the space $V$; it doesn't matter.

\begin{prop}
	If $\dim(V) = \dim(V')$, then $f$ is $n$-monotone in $V$, if and only if it is $n$-monotone in $V'$.
\end{prop}
\begin{proof}
	The reason is rather clear: inner product spaces of same dimension are isometric.
\end{proof}

\section{Examples}

\begin{prop}
	If $\alpha \geq 0$ and $\beta \in \R$ we have $(x \mapsto \alpha x + \beta) \in P_{n}(a, b)$.
\end{prop}
\begin{proof}
	Assume that for $A, B \in \H_{(a, b)}$ we have $A \leq B$. Now
	\begin{align*}
		f(B) - f(A) = (\alpha B + \beta I) - (\alpha A + \beta I) = \alpha (B - A).
	\end{align*}
	Since by assumption $B - A \geq $ and $\alpha \geq 0$, also $\alpha (B - A) \geq 0$, so by definition $f(B) \geq f(A)$. This is exactly what we wanted.
\end{proof}

\begin{prop}
	If $0 \notin (a, b)$, we have $(x \mapsto -x^{-1}) \in P_{n}(a, b)$.
\end{prop}
\begin{proof}
	The result follows easily from Theorem \ref{inverse_decreasing}.
\end{proof}

By the previous proposition also $(x \mapsto (\lambda - x)^{-1}) \in P_{n}(a, b)$ for any $\lambda \notin (a, b)$, so by the cone property
\begin{align}\label{finite_pick}
	x \mapsto \alpha x + \beta + \sum_{i = 1}^{m} \frac{t_{i}}{\lambda_{i} - x} \in P_{n}(a, b)
\end{align}
for any $\alpha, t_{1}, t_{2}, \ldots, t_{m} \geq 0$ and $\beta, \lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}$ where $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m} \not\in (a, b)$.

\section{Basic properties}

Below we collect many natural properties of the cones $P_{n}(a, b)$.

\begin{prop}
	Let $f : (a, b) \to \R$. Then the following are equivalent:
	\begin{enumerate}[(i)]
		\item $f$ is increasing.
		\item $f \in P_{1}(a, b)$.
		\item For any positive integer $n$ and commuting $A, B \in \H_{(a, b)}$ such that $A \leq B$ we have $f(A) \leq f(B)$ .
	\end{enumerate}
\end{prop}
\begin{proof}
	$(ii) \Rightarrow (i)$: Take any $a < x \leq y < b$. Now for $xI, yI \in \H_{(a, b)}$ we have $x I \leq y I$ so by definition
	\begin{align*}
		f(x) I = f(xI) \leq f(y I) = f(y) I,
	\end{align*}
	from which it follows that $f(x) \leq f(y)$.

	$(iii) \Rightarrow (ii)$: All $1 \times 1$ matrices commute.
	

	$(i) \Rightarrow (iii)$: If $A \leq B$ and $A$ and $B$ commute, by Theorem \ref{commuting_real_maps} we may write $A = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}}$ and $B = \sum_{i = 1}^{n} \lambda_{i}' P_{v_{i}}$ for some $\lambda_{1}, \ldots, \lambda_{n}, \lambda'_{1}, \ldots, \lambda'_{n} \in \R$ and $v_{1}, v_{2}, \ldots, v_{n}$, orthonormal basis of $V$, with $\lambda_{i} \leq \lambda'_{i}$. But now $f(A) = \sum_{i = 1}^{n} f(\lambda_{i}) P_{v_{i}}$ and $f(B) = \sum_{i = 1}^{n} f(\lambda'_{i}) P_{v_{i}}$ so
	\begin{align*}
		f(B) - f(A) = \sum_{i = 1}^{n} (f(\lambda'_{i}) - f(\lambda_{i})) P_{v_{i}}.
	\end{align*}
	But this is positive, since, as $f$ is increasing, the numbers $f(\lambda'_{i}) - f(\lambda_{i})$ is non-negative.
\end{proof}

\begin{prop}
	If $f : (a, b) \to (c, d)$ and $g : (c, d) \to \R$ are $n$-monotone, so is $g \circ f : (a, b) \to \R$.
\end{prop}
\begin{proof}
	Fix any $A, B \in \H_{(a, b)}$ with $A \leq B$. By assumption $f(A) \leq f(B)$ and $f(A), f(B) \in \H_{(c, d)}$ so again by assumption, $g(f(A)) \leq g(f(B))$, our claim.
\end{proof}

\begin{prop}
	We have $P_{n + 1}(a, b) \subset P_{n}(a, b)$.
\end{prop}
\begin{proof}
	Take $A, B \in \H_{(a,b)}(V)$ with $A \leq B$. For any $c \in (a, b)$ we have $(A \oplus c), (B \oplus c) \in \H(V \oplus \C)$ and $(A \oplus c) \leq (B \oplus c)$. Consequently, if $f \in P_{n + 1}(a, b)$, we have
	\begin{align*}
		f_{V}(A) \oplus f(c) = f_{V \oplus \C}(A \oplus c) \leq f_{V \oplus \C}(B \oplus c) = f_{V}(B) \oplus f(c),
	\end{align*}
	which implies that $f(A) \leq f(B)$.
\end{proof}

It turns out that these inclusions are strict, as long as our interval is not the whole $\R$.

There are also more trivial inclusions: $P_{n}(a, b) \subset P_{n}(c, d)$ for any $(a, b) \supset (c, d)$. Bigger interval, more matrices, more restrictions, fewer functions. To be precise, one should say that if $(a, b) \supset (c, d)$ and $f \in P_{n}(a, b)$, then also $\restr{f}{(c, d)} \in P_{n}(c, d)$.

\section{Failures}

Most of the common monotone functions fail to be matrix monotone. Let's try some non-examples.

\begin{prop}
	Function $(x \mapsto x^2)$ is not $n$-monotone for any $n \geq 2$ on $(0, \infty)$.
\end{prop}
\begin{proof}
	Let us first think what goes wrong with the standard proof for the case $n = 1$.

	Note that if $A \leq B$,
	\begin{align*}
		B^2 - A^2 = (B - A) (B + A)
	\end{align*}
	is positive as a product of two positive matrices (real numbers).

	There are two fatal flaws here when $n > 1$.
	\begin{itemize}
		\item $(B - A) (B + A) = B^2-A^2 + (B A - A B)$, not $B^2 - A^2$.
		\item Product of two positive matrices need not be positive.
	\end{itemize}
	Note that both of these objections result from the non-commutativity and indeed, both would be fixed should $A$ and $B$ commute.

	Let's write $B = A + H$ ($H \geq 0$). Now we are to investigate
	\begin{align*}
		(A + H)^2-A^2 = A H + H A + H^2.
	\end{align*}
	While $H^2 \geq 0$, as we have noticed in proposition \ref{symmetric_projection}, $A H + H A$ need not be positive! Also, if $H$ is small enough, $H^2$ is negligible compared to $AH + HA$. So simply pick $A, H \geq 0$ such that $A H + H A \not\geq 0$: for small enough $t > 0$ we have $(A + t H)^2 - A^2 \not\geq 0$. To be entirely honest, we only gave examples of such $A$ and $H$ with $\rank(A) = 1$, so $A \notin \H_{(0, \infty)}$. This deficit is however easily fixed by looking at $A_{\varepsilon} = A + \varepsilon I$ for $\varepsilon > 0$.
\end{proof}

By a bit more careful arguments one could show that $(x \mapsto x^2)$ is not $n$-monotone for any $n \geq 2$ on any open interval.

As a corollary with get

\begin{kor}\label{chimon}
	The function $\chi_{(0, \infty)}$ is not $n$-monotone for any $n \geq 2$.
\end{kor}

\begin{proof}
	If $\chi_{x > 0}$ were $n$-monotone so would be
	\begin{align*}
		x^2 = \int_{0}^{\infty} 2 t \chi_{(t, \infty)}(x) dt.
	\end{align*}
\end{proof}

The function $\chi_{(0, \infty)}$ is in some sense canonical counterexample: every increasing function is more or less positive linear combination of its tranlates, so if monotone functions are not all matrix monotone, the reason is that it is not matrix monotone. For this reason we should really understand why it is not $n$-monotone for any $n > 1$.

The idea is the following: we are going to take $n = 2$ and construct $A, B \in \H(V)$ with the following properties:
\begin{enumerate}
	\item $A \leq B$
	\item $A$ and $B$ have both exactly one positive eigenvalue
	\item $A$ and $B$ don't commute
\end{enumerate}
If we can do this, $A$ and $B$ work as counterexamples. Indeed then $\chi_{(0, \infty)}(A) = P_{v_{1}}$ and $\chi_{(0, \infty)}(B) = P_{w_{1}}$ where eigenvectors $v_{1}$ and $w_{1}$ are eigenvectors of $A$ and $B$ corresponding to positive eigenvalues. Since $A$ and $B$ don't commute, $v_{1} \neq w_{1}$, and hence $\chi_{(0, \infty)}(A) \not\leq \chi_{(0, \infty)}(B)$ by \ref{projection_order}.

Constructing such pair is very easy: just take $A$ with eigenvalues $-1$ and $1$ and consider $B$ of the form $A + t H$ for some $H \geq 0$, $t > 0$ and such that $A$ and $H$ do not commute. For small enough $t$ all of the conditions are easily satisfied.

As we saw with the square function example, product of two $n$-monotone functions need not be $n$-monotone in general, even if they are both positive functions. Similarly, taking maximums doesn't preserve monotonicity.

\begin{prop}
	Maximum of two $n$-monotone functions need not be $n$-monotone for $n \geq 2$.
\end{prop}
\begin{proof}
	Again, let's think what goes wrong with the standard proof for $n = 1$.

	Take $n \geq 2$, $f, g \in P_{n}(a, b)$ and $A, B \in \H_{(a, b)}$ with $A \leq B$. We have $f(A) \leq f(B) \leq \max(f, g)(B)$ and $f(A) \leq f(B) \leq \max(f, g)(B)$. It follows that
	\begin{align*}
		\max(f, g)(A) = \max(f(A), g(A)) \leq \max(f, g)(B),
	\end{align*}
	as we wanted.

	Here the flaw is in the expression $\max(f(A), g(A))$: what is maximum of two matrices? It turns out that however one tries to define it, the above inequality doesn't work.

	For counterexamples take $f \equiv 0$ and $g = \id$: it's easy to see that we can take same pair as with $\chi_{(0, \infty)}$ as our counterexample.
\end{proof}

The maximum problem is not too bad and maybe it's more of a pleasent surprise anyway that it holds for increasing functions. But there is a very fundamental problem hidden in the square example.

\begin{prop}
	Let $n \geq 2$. Then there exists no $\alpha > 0$, and no open interval $(a, b) \subset \R$ such that $\alpha x + x^{2} \in P_{n}(a, b)$.
\end{prop}
\begin{proof}
	Adding linear term just means translating domain and codomain, which is not going to help: $x^2 + \alpha x = (x + \frac{\alpha}{2})^2 - \frac{\alpha^2}{4}$.
\end{proof}

Why is this bad? If $f : (a, b) \to \R$ is not too bad (say Lipschitz), for large enough $\alpha$ the function defined by $g(x) = f(x) + \alpha x$ is increasing. But we can't necessarily do the same thing in the matrix setting even for smooth or analytic functions. Although this might not be such a big surprise or a bad thing in the first place, it is worthwhile to investigate the underlying reason.

Let $f(x) = \alpha x + x^2$ and take $A, H \geq 0$. As observed earlier, we have
\begin{align*}
	\lim_{t \to 0} \frac{f(A + t H) - f(A)}{t} &= \alpha H \\
	&+ H A + A H\\
\end{align*}
In the real setting we could just increase $\alpha$ to make the previous expression positive. In the matrix setting there is a problem: note that if $H$ is of rank $1$, increasing $\alpha$ means ``increasing the right-hand side only to one direction". Now, if the right-hand side is not positive map in the first place, it might be non-positive in a big subspace, so rank $1$ machinery is not going to save the day. Note also that even if we let $A \to 0$ (look at matrix function at $0$), the situation isn't a priori better.

On the other hand if $n = 2$, for instance, there is not too much room for things to go south. We still, a priori, can't guarantee positivity with $\alpha$, but adding also something extra, say $\beta x^3$ for some $\beta > 0$ might work. In the end, there isn't too much space in the $2$-dimensional space.

When $n$ gets larger we have more and more space to worry about, so we should start worrying about more and more Taylor coefficients.

This leads us to expect two things:

\begin{enumerate}
	\item Larger the dimension $n$, the more Taylor coefficients should be under some kind of control.
	\item For fixed $n$ we can (at least locally) guarantee $n$-monotonicity by controlling certain number of first Taylor coefficients.
\end{enumerate}

\section{Heuristics}

\subsection{Taylor coefficients}

Let us try to push the ideas of the previous section further. Take entire $f(x) = \sum_{k = 0}^{\infty} a_{k} x^{k}$ and assume that $0 \in (a, b)$. Note that as for any $k > 0$
\begin{align*}
	(A + B)^k = \sum_{i = 0}^{k} \sum_{\atop{j_{0}, \ldots, j_{i} \geq 0}{j_{0} + \ldots + j_{i} = k - i}} A^{j_{0}} B A^{j_{1}} B \cdots B A^{j_{k}},
\end{align*}
we have
\begin{align*}
	\lim_{t \to 0} \frac{f(A + t H) - f(A)}{t} = \sum_{i = 1}^{\infty} a_{i} \sum_{j = 0}^{i - 1} A^{j} H A^{i - 1 - j}.
\end{align*}
If $f \in P_{\infty}(a, b)$, this expression should be positive for $A \in \H_{(a, b)}$ and $H \geq 0$. Let us denote
\begin{align*}
	D f_{A}(H) := \lim_{t \to 0} \frac{f(A + t H) - f(A)}{t},
\end{align*}
derivative of a matrix function at $A$ in the direction $H$. As the derivative is linear in $H$, it suffices to consider the case $\rank(H) = 1$. Let's say $H = v v^{*}$ for some $v \in V$ and take any $w \in V$. Now we should have
\begin{align*}
	\langle D f_{A}(H) w, w \rangle &= \sum_{i = 1}^{\infty} a_{i} \sum_{j = 0}^{i - 1} \langle A^{j} H A^{i - 1 - j} w, w \rangle \\
	&= \sum_{i = 1}^{\infty} a_{i} \sum_{j = 0}^{i - 1} \langle  A^{i - 1 - j} w, v \rangle \langle A^{j} v, w \rangle \\
	&\geq 0
\end{align*}
for any $v, w \in V$. Write $c_{j} = \langle A^{j} v, w \rangle$ and observe that $\langle  A^{j} w, v \rangle = \overline{\langle A^{j} v, w \rangle} = \overline{c_{j}}$. It follows that
\begin{align*}
	\sum_{i = 1}^{\infty} a_{i} \sum_{j = 0}^{i - 1} \overline{c_{i - 1 - j}} c_{j} = \sum_{i, j \geq 0} a_{i + j + 1} c_{i} \overline{c_{j}} \geq 0
\end{align*}
for some kind of sequences $(c_{i})_{i = 1}^{\infty}$. This implies that if the infinite matrix $(a_{i + j + 1})_{i, j \geq 0}$ is positive, then $f$ is matrix monotone. What about the converse?

It is not very hard to see that
\begin{align*}
c_{j} = \sum_{i = 1}^{n} t_{i} \lambda_{i}(A)^{j}
\end{align*}
for some $t_{1}, t_{2}, \ldots, t_{n} \in \C$. Conversely, if the eigenvalues of $A$ are simple, we can control first $n$ terms of $(c_{i})_{i = 1}^{\infty}$ by choice of $v$ and $w$. Indeed: this is just saying that the matrix $(\lambda_{j}(A)^{i - 1})_{i, j}^{n}$ has full column rank, which follows from the fact that its row rank is full by polynomial interpolation. This implies the following:

\begin{prop}
	If $f$ is a polynomial with $2 \leq \deg(f) \leq n$, then $f \notin P_{n}(a, b)$.
\end{prop}
\begin{proof}
	Observe that if the matrix
	\begin{align*}
		\begin{bmatrix}
			a_{1} & a_{2} & \ldots & a_{n - 1} & a_{n} \\
			a_{2} & a_{3} & \ldots & a_{n} & 0 \\
			\vdots & \vdots & \ddots & 0 & 0 \\
			a_{n - 1} & a_{n} & \ldots & 0 & 0 \\
			a_{n} & 0 & \ldots & 0 & 0 \\
		\end{bmatrix}
	\end{align*}
	is positive, then $a_{2} = a_{3} = \ldots = a_{n} = 0$.
\end{proof}

If $f$ is not polynomial such conclusions are harder to make, as we can only control first $c_{i}$'s. Nevertheless, also the converse holds.

\begin{lause}\label{heuristic_loewner}
	$f \in P_{\infty}(a, b)$, if and only if $f$ is analytic and the infinite matrix $(a_{i + j - 1})_{i, j \geq 1}$, where $a_{k} = f^{(k)}(x)/k!$ and $x \in (a, b)$, is positive for any $x \in (a, b)$.
\end{lause}

As one would hope, there's corresponding result for the classes $P_{n}(a, b)$.

\begin{lause}\label{heuristic_main}
	$f \in P_{n}(a, b)$, if and only if $(a_{i + j - 1})_{1 \leq i, j \leq n}$, where $a_{k} = f^{(k)}(x)/k!$ and $x \in (a, b)$, is positive for any $x \in (a, b)$.
\end{lause}

Only now there's a problem: it turns out that function in $P_{n}(a, b)$ need not be analytic, or even $(2 n - 1)$ times differentiable, so the condition should be understood in the distributional sense.

Theorem \ref{heuristic_loewner} is an answer question \ref{question_1}, and it is one of the two main theorems of this thesis.

\subsection{Main argument}

Let us try to prove the ``only if" -directions of these results modulo regularity issues.

\begin{proof}[Proof ``sketch" of the ``only if" of \ref{heuristic_main}]
	Denote the matrix in question by $M (= M_{n}(x, f))$. We may w.l.o.g. take $x = 0 \in (a, b)$. The idea is the following: we know that $\langle D f_{A} (H) w, w \rangle$ is positive for any $A \in \H_{(a, b)}, H \geq 0$ and $w \in V$ and should prove that $\langle M v, v \rangle \geq 0$ for any $v \in \C^{n}$. Best we could hope for is that
	\begin{align*}
		\lim_{\varepsilon \to 0} \langle D f_{A_{\varepsilon}} (H_{\varepsilon}) w_{\varepsilon}, w_{\varepsilon} \rangle = \langle M v, v \rangle
	\end{align*}
	for some $H_{\varepsilon} \geq 0$, $w_{\varepsilon} \in V$, $A_{\varepsilon} \in \H_{(a, b)}$, with $\lim_{\varepsilon \to 0} A_{\varepsilon} = 0$. This works.

	To find $H_{\varepsilon}, w_{\varepsilon}, A_{\varepsilon}$, we change the point of view. Recall that if $f$ is entire we have
	\begin{align*}
		f(A) = \frac{1}{2 \pi i}\int_{\gamma} (z I - A)^{-1} f(z) dz.
	\end{align*}
	Now we can write
	\begin{align*}
		\frac{f(A + t H) - f(A)}{t} &= \frac{1}{2 \pi i}\int_{\gamma} \frac{(z I - A - t H)^{-1} - (z I - A)^{-1}}{t} f(z) dz \\
		&= \frac{1}{2 \pi i}\int_{\gamma} (z I - A - t H)^{-1} H (z I - A)^{-1} f(z) dz.
	\end{align*}
	Taking $t \to 0$, we find that
	\begin{align*}
		D f_{A}(H) = \frac{1}{2 \pi i}\int_{\gamma} (z I - A)^{-1} H (z I - A)^{-1} f(z) dz.
	\end{align*}
	Next, take $H = v v^{*}$ for $v \in V$ and pick $w \in V$: we have
	\begin{align*}
		\langle D f_{A}(H) w, w \rangle &= \frac{1}{2 \pi i}\int_{\gamma} \langle (z I - A)^{-1} v v^{*} (z I - A)^{-1} w, w \rangle f(z) dz \\
		&= \frac{1}{2 \pi i}\int_{\gamma} \langle (z I - A)^{-1} v, w \rangle \langle (z I - A)^{-1} w, v \rangle f(z) dz.
	\end{align*}
	Note that we can write $\langle (z I - A)^{-1} v, w \rangle = \det(z I - A)^{-1} q(z)$ where $q$ is some polynomial of degree less than $n$. Moreover, if $A$ has $n$ distinct roots and $v$ is not orthogonal to any eigenvector of $A$, varying $w$ gives all such polynomials $q$, so we can rewrite our object as
	\begin{align}\label{loewner_integral}
		\frac{1}{2 \pi i}\int_{\gamma} \det(z I - A)^{-2} q(z) \overline{q(\overline{z})} f(z) dz.
	\end{align}
	Taking $A \to 0$ reveals that
	\begin{align*}
		\frac{1}{2 \pi i}\int_{\gamma} \frac{q(z) \overline{q(\overline{z})}}{z^{2 n}} f(z) dz.
	\end{align*}
	But actually this is all we wanted. Indeed, since by the Cauchy's integral formula we have
	\begin{align*}
		\frac{f^{(k)}(0)}{k!} = \frac{1}{2 \pi i} \int_{\gamma} \frac{f(z) d z}{z^{k + 1}}
	\end{align*}
	for any $k \geq 0$, so we can write
	\begin{align*}
		\sum_{1 \leq i, j \leq n} a_{i + j - 1} c_{i} \overline{c_{j}} &= \sum_{1 \leq i, j \leq n} c_{i} \overline{c_{j}} \frac{1}{2 \pi i} \int_{\gamma} \frac{f(z) d z}{z^{i + j}} \\
		&= \frac{1}{2 \pi i} \int_{\gamma} \left(\sum_{i = 1}^{n} \frac{c_{i}}{z^{i}} \right) \left(\sum_{i = 1}^{n} \frac{\overline{c_{i}}}{z^{i}} \right)f(z) dz \\
		&= \frac{1}{2 \pi i} \int_{\gamma}  \frac{q(z) \overline{q(\overline{z})}}{z^{2 n}} f(z) dz,
	\end{align*}
	where we write $q(z) = \sum_{i = 1}^{n} z^{n - i} c_{i}$.
\end{proof}

\begin{proof}[Proof ``sketch" of the ``only if" of \ref{heuristic_loewner}]
	t.f.i.f. \ref{heuristic_main}.
\end{proof}

Note that the proof gives also alternate interpretation for the matrix $M$. Positivity of $M$ means simply that for $q \in \C_{n - 1}[x]$ the function $f(x) q(x) \overline{q(\overline{x})} = f(x) |q(x)|^2$ has non-negative $(2 n - 1)$'th derivative.

The main problem of the argument is the regularity. While we assume $f$ to be entire mainly for convenience, we need some kind of regularity to make sense of the definition: it is a priori not even clear that functions in $P_{\infty}(a, b)$ should be continuous. Origin of these regularity properties is the topic of the next chapter.

\section{Notes and references}

Matrix monotone functions were first introduced and discussed by Loewner in \cite{Low}, and since then, basic properties thereof have been examined in numerous sources. While this thesis doesn't follow any particular text, the book by Donoghue \cite{Don} and lecture notes by Hiai \cite{Hiai} have served as bodies of endless inspiration.

Maximum of real maps is discussed in detail in \cite{Ando2}.

Loewner himself characterized the $n$-monotone functions on $(a, b)$ as functions for which the matrix
\begin{align*}
	\begin{bmatrix}
		f'(\lambda_{1}) & \frac{f(\lambda_{1}) - f(\lambda_{2})}{\lambda_{1} - \lambda_{2}} & \cdots & \frac{f(\lambda_{1}) - f(\lambda_{n})}{\lambda_{1} - \lambda_{n}} \\
		\frac{f(\lambda_{2}) - f(\lambda_{1})}{\lambda_{2} - \lambda_{1}} & f'(\lambda_{2}) & \cdots & \frac{f(\lambda_{2}) - f(\lambda_{n})}{\lambda_{2} - \lambda_{n}} \\
		\vdots & \vdots & & \vdots \\
		\frac{f(\lambda_{n}) - f(\lambda_{1})}{\lambda_{n} - \lambda_{1}} & \frac{f(\lambda_{b}) - f(\lambda_{2})}{\lambda_{n} - \lambda_{2}} & \cdots & f'(\lambda_{n}) 
	\end{bmatrix},
\end{align*}
called Loewner matrix, is positive for any $a < \lambda_{1}, \ldots, \lambda_{n} < b$. It's not hard to show that positivity of (\ref{loewner_integral}) is essentially a reformulation of this condition. Theorems \ref{heuristic_main} and \ref{heuristic_loewner} were first observed by Dobsch (a student of Loewner) in \cite{Dob}, but they were not rigorously proven until \cite{Don}.



