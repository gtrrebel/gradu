\chapter{Representations}

Over the course of this thesis we have mentioned various representations results of the following form:

\[
f(x) = \int h_{t}(x) d \mu (t),
\]
or
\[
f = \int h_{t} d \mu(t).
\]
Here $\mu$ is Borel measure on some set and $f$ and $h_{t}$'s are functions of some kind. Functions $h_{t}$ should be thought of some kind of basis functions. Although such results have been hardly used, one cannot just leave them unmentioned.

Most of the representation results in the thesis can be understood in terms of Choquet theory. The idea is the following: the sets we are concerned with are convex and the functions $h_{t}$ the extreme points of these convex set. Extreme points are the points that can't be expressed as a non-trivial linear combination of points in the set.

Now if one has say compact convex set in $K \subset \R^{n}$, $K$ should be roughly given by its boundary: compact convex sets are equal if and only they have same boundary. But more is true: every point in $K$ can be expressed as a convex combination of extreme points of $K$ (actually of at most $n + 1$ extreme points).

Same holds true much more generally, in infinite dimensional spaces.

\section{Basis $k$-tone functions}

We noticed that $k$-tone functions correspond more or less to functions with non-negative $k$'th derivative. In other words, $k$-tone functions should be $k$-fold integrals of positive functions, at least in sufficiently smooth setting. For instance $f : (a, b) \to \R$ is increasing and smooth if and only if it's of the form
\begin{align}\label{increasing_repr}
	f(x) = \int_{x_{0}}^{x} \rho(t) dt
\end{align}
for some positive $\rho \in C^{\infty}(a, b)$ and $x_{0} \in (a, b)$, up to a constant at least. For non-smooth case, we could require $\rho$ only to be a positive $L^{1}$-function: this gives us absolutely continuous increasing functions. If we further drop $\rho$ but replace the Lebesgue measure by an arbitrary Radon measure $\mu$, we get every right-continuous increasing function. Measuretheoretically these are already all the increasing functions, although we miss some functions like $\chi_{(0, \infty)}$.

If $\mu = \delta_{0}$, for instance, $f = \chi_{[0, \infty)}$. One could think that $\delta_{0}$ gives a jump for $f$ at $0$. More generally, if $\mu$ is positive linear combination if $m$ (distinct) Dirac deltas, $f$ is a function with $m$ jumps. Now every Radon measure is a weak limit of positive linear combination Dirac deltas, so every increasing function is limit of finite sums of jump functions, at least in some weak sense.

This is fact is actually contained in \ref{increasing_repr}: we may rewrite
\[
	f(x) = \int_{a}^{b} \chi_{[t, \infty)}(x)d\mu(t),
\]
$f$ is essentially sum of functions of the form $\chi_{[t, \infty)}$, again up to a constant. We will call those the basis functions for

The point is: whenever something holds for any step function, it should hold for any increasing function. In this context by ``something" I mean linear inequalities: if $\nu$ is a signed Radon measure such that for any step function $\chi_{[t, \infty)}$ we have
\[
	\int \chi_{[x, \infty)}(t) d \nu(t),
\]
then also
\[
	\int f(t) d \nu(t)
\]
for any increasing function. Actually we should also require that $\int d \nu(t) = 0$. I'm being deliberately vague about the domains, they don't really matter too much.

Things get much more interesting when we move to $k$-tone functions of higher order. For $k$-tone functions, i.e. convex functions we can make similar statements.

We can write any (smooth enough) convex function in the form
\[
	f(x) = \int_{x_{0}}^{x} \int_{x_{0}}^{x_{1}} \rho(t) dt dx_{1},
\]
at least up to a constant and linear term. By simple partial integration this can be rewritten as
\[
	f(x) = \int_{x_{0}}^{x} (x - t) \rho(t) d t,
\]
or even, better, as
\[
	f(x) = \int_{a}^{b} (x - t)_{+} \rho(t) dt,
\]
where $(x - t)_{+}$ denotes $\max(0, x - t)$. What this means is that the functions $(\cdot - t)_{+}$ work as a basis functions for convex functions, up to a affine term. By affine transformation we could equivalently take the functions of the form $|\cdot - t|$ as a basis functions.

Now if a linear equality holds for functions of the form $|x - t|$, it holds for any convex function. So since for any $x_{1}, x_{2}, \ldots, x_{m} \in \R$ we have
\[
	\sum_{1 \leq i \leq m} |x_{i} - t| \geq m |\frac{\sum_{1 \leq i \leq m} x_{i}}{m} - t|,
\]
also for any convex function
\[
	\sum_{1 \leq i \leq m} f(x_{i}) \geq m f\left(\frac{\sum_{1 \leq i \leq m} x_{i}}{m}\right),
\]
Jensen's inequaltity.

\section{Majorization}

Of course there should be a larger family of inequalities which hold for functions of the form $|x - t|$: it turns out that there is a rather simple characterization for such inequalities, by \textit{majorization}.

\begin{maar}
	Let $x = (x_{i})_{i = 1}^{n}$ and $y = (y_{i})_{i = 1}^{n}$ be two sequences of reals. We say that $y$ majorizes $x$, and write $x \prec y$, if TODO
\end{maar}

\begin{lause}[Polya-Hardy-Littlewood-Karamata-Inequality]
	Let $(a, b)$ be an open interval, $n$ positive integer, and $x = (x_{i})_{i = 1}^{n} \in (a, b)^{n}$ and $y = (y_{i})_{i = 1}^{n} \in (a, b)^{n}$. Then the following are equivalent.
	\begin{enumerate}
		\item $x \prec y$
		\item For any real number $t$ we have
		\[
			\sum_{1 \leq i \leq n} |x_{i} - t| \leq \sum_{1 \leq i \leq n} |y_{i} - t|.
		\]
		\item For any convex $f : (a, b) \to \R$ we have
		\[
			\sum_{1 \leq i \leq n} f(x_{i}) \leq \sum_{1 \leq i \leq n} f(y_{i}).
		\]
	\end{enumerate}
\end{lause}
\begin{proof}
	TODO
\end{proof}