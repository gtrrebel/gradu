\chapter{Preliminaries}

As mentioned in the introduction, in order to understand matrix monotone and convex functions we first have to undestand positive matrices and matrix functions (and some other things).

\section{Positive matrices}

This section is titled ``positive matrices", although ``positive maps" might be more appropriate title. We are mostly going to deal with finite-dimensional objects, but many of the ideas could be generalized infinite-dimensional settings, where matrices lose their edge. Also, one should always ask whether it really clarifies the situation to introduce concrete matrices: matrices are good at hiding the truly important properties of linear mappings. The words ``matrix" and ``linear map" are used somewhat synonymously, although one should always remember that the former are just special representations for the latter.

\subsection{Motivation}

How should one order matrices? What should we require from ordering anyway?

I could just give you the answer, but instead I try to explain why it is standard in the first place.

We would definitely like to have natural total order on the space of matrices, but it turns out that are no natural choices for that. Partial order is next best thing. Recall that a partial order on a set $X$ is a binary relation $\leq$ on such that
\begin{enumerate}
	\item $x \leq x$ for any $x \in X$.
	\item For any $x, y \in X$ for which $x \leq y$ and $y \leq x$, necessarily $x = y$.
	\item If for some $x, y, z \in X$ we have both $x \leq y$ and $y \leq z$, also $x \leq z$.
\end{enumerate}

The third point is the main point, the first two are just there preventing us from doing something crazy. But we can do better: this partial order on matrices should also respect addition.
\begin{enumerate}
\item[4.] For any $x, y, z \in X$ such that $x \leq y$, we should also have $x + z \leq y + z$.
\end{enumerate}

There's another way to think about this last point. Instead of specifying order among all the pairs, we just say which matrices are positive: matrix is positive if and only it's at least $0$.

If we know all the positive matrices, we know all the ``orderings". To figure out whether $x \leq y$, we just check whether $0 = x - x \leq y - x$, i.e. whether $y - x$ is positive. Also, positive matrices are just differences of the form $y - x$ where $x \leq y$. Now, conditions on the partial order are reflected to the set of positive matrices.
\begin{enumerate}
	\item[1'.] $0$ (zero matrix) is positive.
	\item[2'.] If both $x$ and $-x$ are positive, then $x = 0$.
	\item[3'.] If both $x$ and $y$ are positive, so is their sum $x + y$.
\end{enumerate}
Here $3'$ is kind of combination of $3$ and $4$.

The terminology here is rather unfortunate. Natural ordering of the reals satisfies all of the above with obvious interpretation of positive numbers, which however differs from the standard definition: $0$ is itself positive in our above definition. This is undoubtedly confusing, but what can you do? For real numbers we have total order, so every number is either zero, strictly positive or strictly negative, so when we say non-negative, it literally means ``not negative": we get all the positive numbers and zero. But with partial orders we might get more. So the main reasons why we are using this terminology are
\begin{enumerate}
	\item It's short.
\end{enumerate}
Also, now that we have decided to preserve the word ``positive" for ``at least zero" one might be tempted to preserve ``strictly positive" for ``at least zero, but not zero". We won't do that, we save that phrase for something more important.

Back to business. When we try to define ordering of the matrices, everything of course depends on the ground field. It hardly makes any sense to order matrices over $\F_{p}$: even $1 \times 1$ matrices, namely (canonically) the elements of $\F_{p}$ defy reasonable ordering. But real numbers, for instance, have ordering, so there's a serious change that all real matrices could be ordered.

We will first try to order all real square matrices. (Actually, we won't even try to order non-square matrices.) $1 \times 1$ matrices are easy to order, but as soon one moves to larger matrices, one faces difficult decisions:

\[
	\text{Is }
	\begin{bmatrix}
		1 & 0 \\
		0 & -1 \\
	\end{bmatrix}
	\leq
	\begin{bmatrix}
		0 & 0 \\
		0 & 0 \\
	\end{bmatrix}
	\text{ or }
	\begin{bmatrix}
		1 & 0 \\
		0 & -1 \\
	\end{bmatrix}
	\geq
	\begin{bmatrix}
		0 & 0 \\
		0 & 0 \\
	\end{bmatrix}
	\text{?}
\]

That's okay, we don't necessarily have to order all pairs of matrices. But there are other problems. We would like the ordering of the matrices to be independent of the choice matrix representation. If we flip basis vectors we change the rows and colums of the matrix, so change it's sign! So if we want
\begin{itemize}
	\item Positivity is readable from the matrix with respect to an arbitrary basis, alone.
	\item Positivity doesn't depend on the chosen basis.
\end{itemize}
there is no way to assign sign to the previous matrix.

Let's consider diagonalizable matrices: by change of basis these can be written as diagonal matrices, but these diagonal values, eigenvalues, need not be real anymore. One could try to ignore the complex bases, but how often really is ignoring the complex structure satisfactory. Another approach would be only to consider matrices with real eigenvalues. There's problem here though: sum of two matrices with real eigenvalues need not have real eigenvalues, even if the former are diagonalizable! Sad as it may sound, it shouldn't be too surprising since it's not very clear that eigenvalues should have any reasonable behaviour with respect to addition. Of course, one shouldn't just take my word for it: here are congrete examples:
\[
	MATRIX A and B
\]
First two matrices have eigenvalues TODO, TODO and TODO, TODO, respectively, but their sum has characteristic polynomial TODO, that most definitely has no real zeros.

There is however very special class of matrices, correponding to special class of linear mappings, which satisfy our requirements.
\begin{itemize}
	\item They have real eigenvalues.
	\item They are all diagonalizable.
	\item They are closed under sum, and (real) scalar multiplication.
\end{itemize}

\subsection{Hermitian maps}

Hermitian mappings will be our non-commutative playing ground in which we can define rather natural partial order. Fix any finite-dimensional inner-product space $(V, \langle \cdot, \cdot \rangle )$ over $\R$ or $\C$. For any $v \in V \setminus \{0\}$ we may define the corresponding projection, denoted by $P_{v}$ by setting $P_{v}(x) = \langle x , v \rangle /\langle v, v \rangle v$.

\begin{maar}
	Let $V$ be an finite-dimensional innerproduct space over $\R$ or $\C$. Now set of \textit{Hermitian maps} of $V$, denoted by $\H(V)$, is defined as
	\[
		\vspan \{P_{v} \mid v \in V \setminus \{0\}\},
	\]
	where the span is $\R$-linear, i.e. Hermitian maps are the maps of the form
	\[
		\sum_{i = 1}^{m} \lambda_{i} P_{v_{i}},
	\]
	for some positive integer $m$, $v_{i} \in V$ and $\lambda_{i} \in \R$.
\end{maar}

Also note that since for any non-zero $\alpha$ and $v \in V \setminus \{0\}$ we have $P_{v} = P_{\alpha v}$, we could just as well only allow vectors of norm one in our definition of Hermitian maps. This is of course just to simplify notation. This is not the standard definition, and in a way it's horrible, but it very directly answers our needs. Remember that we want to have diagonalizable maps with real eigenvalues. Projections are very prototypical examples of such maps and since we want real eigenvalues that is simply what we require in the definition (span over $\R$). Lastly, one of course wants to take the span to have any linear structure in the first place.

It is however very surprising that such construction works: basis vectors (projections) satisfy our requirements, but there are no reasons to expect that these would be preserved in addition. This is guaranteed by the following theorem.

\begin{lause}[Spectral theorem]
	Let $V$ be an $n$-dimensional inner-product space over $\R$ or $\C$, and $A \in \H(V)$. Then there exists real numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$ and orthonormal vectors $v_{1}, v_{2}, \ldots, v_{n}$ such that
	\begin{align}\label{spectralrepr}
		A = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}}.
	\end{align}
\end{lause}

It follows that vectors $v_{1}, v_{2}, \ldots, v_{n}$ are eigenvectors of $A$ with corresponding eigenvalues $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$. Indeed, for any $v_{j}$ we have
\begin{align}
	A v_{j} = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}} v_{j} = \sum_{i = 1}^{n} \lambda_{i} \langle v_{j}, v_{i}\rangle v_{i} = \sum_{i = 1}^{n} \lambda_{i} \delta_{ij} v_{i} = \lambda_{j} v_{j}.
\end{align}

\begin{proof}[Proof (of the Spectral theorem)]
	We proof the statement by induction on the dimension of the space $V$. The case $n = 0$ is trivial: the span itself is trivial, and it's very easy to express $0$-mapping as a empty sum.

	Now fix a positive integer $n$. 

	\textit{Step 1.} First note $A$ has at least one eigenvector over $\C$, as every other linear mapping. Let $v_{1}$ be that eigenvector (of lenght one) with corresponding eigenvalue $\lambda_{1}$.

	\textit{Step 2.} Write $A = \sum_{i = 1}^{m} c_{i} P_{u_{i}}$ for $c_{1}, c_{2}, \ldots, c_{m} \in \R$ and $u_{1}, u_{2}, \ldots, u_{m} \in V \setminus \{0\}$ are of norm $1$. Now the definition of the eigenvector and eigenvalue rewrites to
	\[
		A v_{1} = \sum_{i = 1}^{m} c_{i} \langle v_{1}, u_{i}\rangle u_{i} = \lambda_{i} v_{1}.
	\]
	Take inner product with $v_{1}$ from both sides to get
	\[
		\sum_{i = 1}^{m} c_{i} \langle v_{1}, u_{i}\rangle \langle u_{i}, v_{1} \rangle = \lambda_{1} \langle v_{1}, v_{1} \rangle.
	\]
	Since $\langle v_{1}, u_{i}\rangle \langle u_{i}, v_{1} \rangle = |\langle v_{1}, u_{i}\rangle|^{2}$, the left-hand side is real, so is right-hand side, and finally so is $\lambda_{1}$.

	\textit{Step 3.} Now we have found $\lambda_{1}$ and $v_{1}$ as in the theorem statement. The idea is then to factorize $A$ to two parts: projection corresponding to $v_{1}$ and an orthogonal part living in $v_{1}^{\perp}$. If our choices for $\lambda_{1}$ and $v_{1}$ work, anything orthogonal to $v_{1}$ maps to something orthogonal to $v_{1}$. We verify this.

	Take any $v \perp v_{1}$. Now,
	\begin{eqnarray*}
		\langle A v, v_{1} \rangle &=& \langle \sum_{i = 1}^{m} c_{i} \langle v, u_{i}\rangle u_{i}, v_{1} \rangle \\
		= \sum_{i = 1}^{m} c_{i} \langle v, u_{i}\rangle \langle u_{i}, v_{1} \rangle &=& \overline{\sum_{i = 1}^{m} c_{i} \langle v_{1}, u_{i}\rangle \langle u_{i}, v \rangle} \\
		= \overline{\langle \sum_{i = 1}^{m} c_{i} \langle v_{1}, u_{i}\rangle u_{i}, v \rangle} &=& \overline{\langle A v_{1}, v \rangle} \\
		= \overline{\langle \lambda_{1} v_{1}, v \rangle} &=& 0,
	\end{eqnarray*}
	so also $A v \perp v_{1}$. It follows that we get a map $A' : v_{1}^{\perp} \to v_{1}^{\perp}$ which extends to $A$.

	\textit{Step 4.} We would naturally like to use our induction hypothesis for this map, but for that we need it to be of special form: although the map is well defined, there is no reason to expect that we could just pick some of the $u_{i}$:s for our representation. But we can do something else.

	Write $u_{i} = u_{i}' + \alpha_{i} v_{1} = P_{v_{1}^{\perp}} + P_{v_{1}}$. Now if $v \perp v_{1}$, we have
	\begin{eqnarray*}
		A v &=& \sum_{i = 1}^{m} c_{i} \langle v, u_{i}\rangle u_{i} \\
		&=& \sum_{i = 1}^{m} c_{i} \langle v, u_{i}' + \alpha_{i} v_{1}\rangle (u_{i}' + \alpha_{i} v_{1}) \\
		&=& \sum_{i = 1}^{m} c_{i} \langle v, u_{i}'\rangle u_{i}' +  v_{1} \sum_{i = 1}^{m} c_{i} \langle v, u_{i}'\rangle \alpha_{i}.
	\end{eqnarray*}
	Now since $u_{i}' \perp v_{1}$, the latter sum vanishes, and
	\[
		A' = \sum_{i = 1}^{m} c_{i} P_{u_{i}'}.
	\]
	This representation meets our requirements, so we can write $A' = \sum_{i = 2}^{n} \lambda_{i} P_{v_{i}}$, for some $\lambda_{2}, \ldots, \lambda_{n} \in \R$ and $v_{2}, \ldots, v_{n} \in v_{1}^{\perp}$, and adjoining $v_{1}$ and $\lambda_{1}$, we get the required representation.
\end{proof}

It should be noted that we only used one special property of ($\R$ linear combination of) projections $A$ in the proof:
\[
	\langle A v, w \rangle = \overline{\langle A w, v \rangle} = \langle v, A w \rangle,
\]
for any $v, w \in V$. In the first step we didn't use any properties of $A$. In the step 3 this is exactly what we do in the manipulation. Also in the step 2 the idea is to show that
\[
	\lambda_{1} \langle v_{1}, v_{2} \rangle = \langle A v_{1}, v_{1} \rangle \in \R,
\]
but $\langle A v_{1}, v_{1} \rangle = \overline{\langle A v_{1}, v_{1} \rangle}$ so $\langle A v_{1}, v_{1} \rangle \in \R$. Of course here we didn't need the fact that $v_{1}$ is an eigenvector in any way. Step 4 is just simpler: once we have constructed $A'$, it will obviously have the same property as $A$.

In all of the above it doesn't make much difference whether we have $\R$ or $\C$ as the ground field, since the eigenvalues will be anyway real. Especially when one is working over $\R$, one might whether there is a more straightforward way to get through step $1$: get rid of the complex numbers. There is. Another way to find the eigenvector and eigenvalue is to look at the so called \textit{Rayleigh quotient}
\[
	R(A, v) = \frac{\langle A v, v \rangle}{\langle v, v \rangle},
\]
when $v \in V \setminus \{0\}$. This is scale-invariant bounded real quantity so it attains maximum somewhere outside zero, say at $v$. We claim that $v$ is an eigenvector of $A$. Looking at $v_{t} = v + t w$ and differentiating at zero yields
\begin{eqnarray*}
	0 = \frac{d}{dt} R(A, v_{t})\Big|_{t = 0} &=& \frac{(\langle A v, w \rangle + \langle A w, v \rangle) \langle v, v \rangle - (\langle v, w \rangle + \langle w, v \rangle) \langle Av, v \rangle}{\langle v, v \rangle^2} \\
	&=& \frac{\Re\left(\langle A v, w \rangle \langle v, v \rangle -  \langle v, w \rangle \langle Av, v \rangle \right)}{\langle v, v \rangle^2}.
\end{eqnarray*}
We write $A v = \lambda v + v'$ where $v \perp v'$ and set $w = v'$. Now
\begin{eqnarray*}
	0 &=& \frac{\Re\left(\langle \lambda v + v', v' \rangle \langle v, v \rangle -  \langle v, v' \rangle \langle \lambda v + v', v \rangle \right)}{\langle v, v \rangle^2} \\
	&=& \frac{\Re\langle v', v' \rangle \langle v, v \rangle}{\langle v, v \rangle^2},
\end{eqnarray*}
so $v' = 0$ and $v$ is an eigenvector.

\subsection{Self-adjoint maps}

\begin{maar}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an finite-dimensional inner-product space. \textit{Adjoint} of a linear map $A : V \to V$ is a linear map $A^{*} : V \to V$ such that
	\[
		\langle A v, w \rangle = \langle v, A^{*} w \rangle
	\]
	for any $v, w \in V$.
\end{maar}

\begin{maar}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an finite-dimensional inner-product space. A linear map $A : V \to V$ is \textit{self-adjoint} if it is its own adjoint, i.e.
	\[
		\langle A v, w \rangle = \langle v, A w \rangle
	\]
	for any $v, w \in V$.
\end{maar}

Self-adjointness was the good property of projections that made the Spectral theorem work: we got to move mappings from one side of inner-product to another. Generally the mapping changes, and adjoint is what comes out. We gather here many of the important properties of adjoint.

\begin{lause}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an $n$-dimensional inner-product space, $A, B \in \L(V)$ linear, and $\lambda \in \C$. Then
	\begin{enumerate}[i)]
		\item $A$ has an unique adjoint
		\item Matrix of $A^{*}$ with respect to any orthonormal basis is conjugate transpose of matrix of $A$, i.e. $A^{*}_{i, j} = \overline{A_{j, i}}$.
		\item $(A^{*})^{*} = A$
		\item $(A + B)^{*} = A^{*} + B^{*}$
		\item $(\lambda A)^{*} = \overline{\lambda} A^{*}$
		\item $(AB)^{*} = B^{*}A^{*}$.
	\end{enumerate}
\end{lause}
\begin{proof}
	\begin{enumerate}[i)]
		\item Fix any orthonormal basis of $V$, $(e_{i})_{i = 1}^{n}$. Now for any $v, w \in V$
		\begin{eqnarray*}
			\langle A v, w \rangle &=& \langle A \sum_{i = 1}^{n} \langle v, e_{i} \rangle e_{i}, w\rangle\\
			&=& \sum_{i = 1}^{n} \langle A e_{i}, w \rangle \langle v, e_{i} \rangle \\
			&=& \langle v, \sum_{i = 1}^{n} \langle w, A e_{i} \rangle e_{i} \rangle,
		\end{eqnarray*}
		so we should set $A^{*} w = \sum_{i = 1}^{n} \langle w, A e_{i} \rangle e_{i}$. This clearly defines an linear mapping $A^{*}$. Also, if $A$ has two adjoints, their difference $C$ is linear map such that $\langle v, C w \rangle = 0$ for any $v, w \in V$. Setting $v = C w$ we get that $C w = 0$ for any $w \in V$ so the adjoints are equal.
		\item Observe that $A^{*}_{i, j} = \langle A^{*}e_{j}, e_{i} \rangle = \overline{\langle e_{i}, A^{*} e_{j} \rangle} = \overline{ \langle A e_{i}, e_{j} \rangle} = \overline{A_{j, i}}$.
		\item We have for any $v, w \in V$ that $\langle A^{*} v, w \rangle = \overline{\langle w, A^{*} v \rangle} = \overline{\langle A w, v \rangle} = \langle v, A w \rangle$, so $A$ is adjoint of $A^{*}$.
		\item Since $\langle (A + B) v, w \rangle = \langle A v, w \rangle + \langle B v, w \rangle = \langle v, A^{*} w \rangle + \langle v, B^{*} w \rangle = \langle v, (A^{*} + B^{*})w \rangle$, $A^{*} + B^{*}$ is adjoint of $A + B$.
		\item We simply calculate that $\langle \lambda A v, w \rangle = \lambda \langle v, A^{*} w \rangle = \langle v, \overline{\lambda} A^{*}v \rangle$.
		\item Note that $\langle A B v, w \rangle = \langle B v, A^{*}w \rangle = \langle v, B^{*} A^{*} w \rangle$.
	\end{enumerate}
\end{proof}

\begin{huom}
	One way to understand adjoints is to look at a bit more general case. Fix any two finite-dimensional inner-product spaces, $(V, \langle \cdot, \cdot \rangle_{V})$ and $(W, \langle \cdot, \cdot \rangle_{W})$, not necessarily of the same dimension, over, say, $\C$.
	Now given any linear $A : V \to W$, adjoint is a linear map $A^{*} : W \to V$ such that for any $v \in V$ and $w \in W$ we have
	\[
		\langle A v, w \rangle_{W} = \langle v, A^{*} w \rangle_{V}.
	\]
	Again, adjoint exists and it's unique. Fix any $w \in W$. Vector $w$ induces a linear mapping $W \to \C$ by $w' \mapsto \langle w', w \rangle_{W}$. Hence we get a map $T_{W} : W \to W^{*}$ where $V^{*}$ is the dual of $V$. It turns out that $T$ is anti-linear bijection. Anti-linearity and injectivity are easy to check, and since both spaces have same dimension, map has to be also surjection.

	Composing with $A$, $T_{W}$ induces a map $W \to V^{*}$ given by $w \mapsto T_{W}(w) \circ A$. Now we finally compose this with $T^{-1}_{V}$ to get a map $W \to V$, the adjoint. So $A^{*}(w) = T^{-1}_{V}(T_{W}(w) \circ A)$.
\end{huom}

Adjoint behaves in many ways like conjugate.
\begin{itemize}
	\item Notions agree in $1$-dimensional spaces.
	\item Taking adjoint is an anti-linear involution.
	\item Eigenvalues of the adjoint are conjugates of the eigenvalues of the original map.
	\item If $A \in \L(V)$ has an orthonormal eigenbasis, $A^{*}$ has the same eigenbasis with conjugated eigenvalues.
	\item Self-adjoint maps have real eigenvalues.
\end{itemize}

Maps for which the fourth point holds are called \textit{normal}. TODO

As we noticed, if $A \in \L(V)$ is self-adjoint $\langle A v, v \rangle \in \R$ for any $v \in V$. This motivates us to define the quadratic form of $A$, a map $Q_{A} : V \to \R$, by setting $Q_{A}(v) = \langle A v, v \rangle$. Similarly, if $Q_{A}$ is real, $A$ is self-adjoint. Indeed, looking at $Q_{A}(v + t w) = Q_{A}(v) + |t|^2 Q_{A}(w) + \overline{t} \langle A v, w \rangle + t \langle A w, v \rangle$ we see that $\overline{t} \langle A v, w \rangle + t \langle A w, v \rangle \in \R$ for any $v, w \in V$ and $t \in \C$. Still $\overline{t} \langle A v, w \rangle + t \langle A w, v \rangle = \overline{t} \langle A v, w \rangle + t \overline{\langle A v, w \rangle} - t \langle w, A v \rangle   + t \langle A w, v \rangle = \Re(\overline{t} \langle A v, w \rangle) + t (\langle A w, v \rangle  - \langle w, A v \rangle) $ so we must have $\langle A w, v \rangle  = \langle w, A v \rangle$.


















