\chapter{Preliminaries}

As mentioned in the introduction, in order to understand matrix monotone and convex functions we first have to undestand positive matrices and matrix functions (and some other things).

\section{Positive matrices}

This section is titled ``positive matrices", although ``positive maps" might be more appropriate title. We are mostly going to deal with finite-dimensional objects, but many of the ideas could be generalized infinite-dimensional settings, where matrices lose their edge. Also, one should always ask whether it really clarifies the situation to introduce concrete matrices: matrices are good at hiding the truly important properties of linear mappings. The words ``matrix" and ``linear map" are used somewhat synonymously, although one should always remember that the former are just special representations for the latter.

\subsection{Motivation}

How should one order matrices? What should we require from ordering anyway?

I could just give you the answer, but instead I try to explain why it is standard in the first place.

We would definitely like to have natural total order on the space of matrices, but it turns out that are no natural choices for that. Partial order is next best thing. Recall that a partial order on a set $X$ is a binary relation $\leq$ on such that
\begin{enumerate}
	\item $x \leq x$ for any $x \in X$.
	\item For any $x, y \in X$ for which $x \leq y$ and $y \leq x$, necessarily $x = y$.
	\item If for some $x, y, z \in X$ we have both $x \leq y$ and $y \leq z$, also $x \leq z$.
\end{enumerate}

The third point is the main point, the first two are just there preventing us from doing something crazy. But we can do better: this partial order on matrices should also respect addition.
\begin{enumerate}
\item[4.] For any $x, y, z \in X$ such that $x \leq y$, we should also have $x + z \leq y + z$.
\end{enumerate}

There's another way to think about this last point. Instead of specifying order among all the pairs, we just say which matrices are positive: matrix is positive if and only it's at least $0$.

If we know all the positive matrices, we know all the ``orderings". To figure out whether $x \leq y$, we just check whether $0 = x - x \leq y - x$, i.e. whether $y - x$ is positive. Also, positive matrices are just differences of the form $y - x$ where $x \leq y$. Now, conditions on the partial order are reflected to the set of positive matrices.
\begin{enumerate}
	\item[1'.] $0$ (zero matrix) is positive.
	\item[2'.] If both $x$ and $-x$ are positive, then $x = 0$.
	\item[3'.] If both $x$ and $y$ are positive, so is their sum $x + y$.
\end{enumerate}
Here $3'$ is kind of combination of $3$ and $4$.

The terminology here is rather unfortunate. Natural ordering of the reals satisfies all of the above with obvious interpretation of positive numbers, which however differs from the standard definition: $0$ is itself positive in our above definition. This is undoubtedly confusing, but what can you do? For real numbers we have total order, so every number is either zero, strictly positive or strictly negative, so when we say non-negative, it literally means ``not negative": we get all the positive numbers and zero. But with partial orders we might get more. So the main reasons why we are using this terminology are
\begin{enumerate}
	\item It's short.
\end{enumerate}
Also, now that we have decided to preserve the word ``positive" for ``at least zero" one might be tempted to preserve ``strictly positive" for ``at least zero, but not zero". We won't do that, we save that phrase for something more important.

Back to business. When we try to define ordering of the matrices, everything of course depends on the ground field. It hardly makes any sense to order matrices over $\F_{p}$: even $1 \times 1$ matrices, namely (canonically) the elements of $\F_{p}$ defy reasonable ordering. But real numbers, for instance, have ordering, so there's a serious change that all real matrices could be ordered.

We will first try to order all real square matrices. (Actually, we won't even try to order non-square matrices.) $1 \times 1$ matrices are easy to order, but as soon one moves to larger matrices, one faces difficult decisions:

\[
	\text{Is }
	\begin{bmatrix}
		1 & 0 \\
		0 & -1 \\
	\end{bmatrix}
	\leq
	\begin{bmatrix}
		0 & 0 \\
		0 & 0 \\
	\end{bmatrix}
	\text{ or }
	\begin{bmatrix}
		1 & 0 \\
		0 & -1 \\
	\end{bmatrix}
	\geq
	\begin{bmatrix}
		0 & 0 \\
		0 & 0 \\
	\end{bmatrix}
	\text{?}
\]

That's okay, we don't necessarily have to order all pairs of matrices. But there are other problems. We would like the ordering of the matrices to be independent of the choice matrix representation. If we flip basis vectors we change the rows and colums of the matrix, so change it's sign! So if we want
\begin{itemize}
	\item Positivity is readable from the matrix with respect to an arbitrary basis, alone.
	\item Positivity doesn't depend on the chosen basis.
\end{itemize}
there is no way to assign sign to the previous matrix.

Let's consider diagonalizable matrices: by change of basis these can be written as diagonal matrices, but these diagonal values, eigenvalues, need not be real anymore. One could try to ignore the complex bases, but how often really is ignoring the complex structure satisfactory. Another approach would be only to consider matrices with real eigenvalues. There's problem here though: sum of two matrices with real eigenvalues need not have real eigenvalues, even if the former are diagonalizable! Sad as it may sound, it shouldn't be too surprising since it's not very clear that eigenvalues should have any reasonable behaviour with respect to addition. Of course, one shouldn't just take my word for it: here are congrete examples:
\[
	MATRIX A and B
\]
First two matrices have eigenvalues TODO, TODO and TODO, TODO, respectively, but their sum has characteristic polynomial TODO, that most definitely has no real zeros.

There is however very special class of matrices, correponding to special class of linear mappings, which satisfy our requirements.
\begin{itemize}
	\item They have real eigenvalues.
	\item They are all diagonalizable.
	\item They are closed under sum, and (real) scalar multiplication.
\end{itemize}

\subsection{Hermitian matrices}

Hermitian matrices/mappings will be our non-commutative playing ground in which we can define rather natural partial order. Fix any finite-dimensional inner-product space $(V, \langle \cdot \rangle )$ over $\R$ or $\C$. For any $v \in V \setminus \{0\}$ we may define the corresponding projection, denoted by $P_{v}$ by setting $P_{v}(x) = \langle x , v \rangle /\langle v, v \rangle v$.

\begin{maar}
	Let $V$ be an finite-dimensional innerproduct space over $\R$ or $\C$. Now set of \textit{Hermitian maps} of $V$, denoted by $\H(V)$, is defined as
	\[
		\vspan \{P_{v} \mid v \in V \setminus \{0\}\},
	\]
	where the span is $\R$-linear, i.e. Hermitian maps are the maps of the form
	\[
		\sum_{i = 1}^{m} \lambda_{i} P_{v_{i}},
	\]
	for some positive integer $m$, $v_{i} \in V$ and $\lambda_{i} \in \R$.
\end{maar}

This is not the standard definition, and in a way it's horrible, but it very directly answers our needs. Also note that since for any non-zero $\alpha$ and $v \in V \setminus \{0\}$ we have $P_{v} = P_{\alpha v}$, we could just as well only allow vectors of norm one in our definition of Hermitian maps. This is of course just to simplify notation. Remember that we want to have diagonalizable maps with real eigenvalues. Projections are very prototypical examples of such maps and since we want real eigenvalues that is simply what we require in the definition. Lastly, one of course wants now to take the span to have linear structure.

It is however very surprising that such construction works: basis vectors (projections) satisfy our requirements, but there are no reasons to expect that these would be preserved in addition. This is guaranteed by the following theorem.

\begin{lause}[Spectral theorem]
	Let $V$ be an $n$-dimensional inner-product space over $\R$ or $\C$, and $A \in \H(V)$. Then there exists real numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$ and orthonormal vectors $v_{1}, v_{2}, \ldots, v_{n}$ such that
	\[
		A = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}}.
	\]
\end{lause}

It follows that vectors $v_{1}, v_{2}, \ldots, v_{n}$ are eigenvectors of $A$ with corresponding eigenvalues $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$. Indeed, for any $v_{j}$ we have
\[
	A v_{j} = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}} v_{j} = \sum_{i = 1}^{n} \lambda_{i} \langle v_{j}, v_{i}\rangle v_{i} = \sum_{i = 1}^{n} \lambda_{i} \delta_{ij} v_{i} = \lambda_{j} v_{j}.
\]

\begin{proof}[Proof (of the Spectral theorem)]
	We proof the statement by induction on the dimension of the space $V$. The case $n = 0$ is trivial: the span itself is trivial, and it's very easy to express $0$-mapping as a empty sum.

	Now fix a positive integer $n$. First note $A$ has at least one eigenvector over $\C$, as every other linear mapping. Let $v_{1}$ be that eigenvector (of lenght one) with corresponding eigenvalue $\lambda_{1}$. Write $A = \sum_{i = 1}^{m} c_{i} P_{u_{i}}$ for $c_{1}, c_{2}, \ldots, c_{m} \in \R$ and $u_{1}, u_{2}, \ldots, u_{m} \in V \setminus \{0\}$ are of norm $1$. Now the definition of the eigenvector and eigenvalue rewrites to
	\[
		A v_{1} = \sum_{i = 1}^{m} c_{i} \langle v_{1}, u_{i}\rangle u_{i} = \lambda_{i} v_{1}.
	\]
	Take inner product with $v_{1}$ from both sides to get
	\[
		\sum_{i = 1}^{m} c_{i} \langle v_{1}, u_{i}\rangle \langle u_{i}, v_{1} \rangle = \lambda_{1} \langle v_{1}, v_{1} \rangle.
	\]
	Since $\langle v_{1}, u_{i}\rangle \langle u_{i}, v_{1} \rangle = |\langle v_{1}, u_{i}\rangle|^{2}$, the left-hand side is real, so is right-hand side, and finally so is $\lambda_{1}$.

	The idea is then to factorize $A$ to two parts: projection corresponding to $v_{1}$ and an orthogonal part living in $v_{1}^{\perp}$.
\end{proof}















