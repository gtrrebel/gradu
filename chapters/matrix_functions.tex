\chapter{Matrix functions}

\section{Functional calculus}

\begin{maar}
	For any $-\infty \leq a < b \leq \infty$ $f : (a, b) \to \R$ the associated matrix function on $V$ is the map $f_{V} : \H_{(a, b)}(V) \to \H(V)$ given by
	\[
		f_{V}\left(A\right) = \sum_{\lambda \in \spec(A)} f(\lambda) P_{E_{\lambda}}
	\]
	if $A = \sum_{\lambda \in \spec(A)} \lambda P_{E_{\lambda}}$.
\end{maar}
Hence to calculate the matrix function we just apply the function to the eigenvalues of the map and leave the eigenspaces as they are. Note as the spectral representation is unique this definition makes sense.

We have already discussed four types of matrix functions: inverse, polynomials, square root and absolute value. All these notion coincide with the general notion of matrix function for real maps, as notion in (\ref{polynomial_matrix_function}) and TODO.

Matrix functions enjoy many natural and useful properties.

\begin{prop}\label{basic_matrix}
	Let $f : (a,b) \to \R$ and $A \in \H_{(a, b)}$
	\begin{enumerate}
		\item If $f[(a, b)] \subset (c, d)$ then $f_{V}(A) \in \H_{(c, d)}$.
		\item If also $g : (a, b) \to \R$ then $(f + g)_{V} = f_{V} + g_{V}$ and $(fg)_{V} = f_{V}g_{V}$.
		\item $f_{V_{1} \oplus V_{2}} = f_{V_{1}} \oplus f_{V_{2}}$.
		\item If $g : (a, b) \to \R$ and $f$ and $g$ agree on spectrum of $A$, then $f(A) = g(A)$.
		\item If $f[(a, b)] \subset (c, d)$ and $g : (c, d) \to \R$ then $(g \circ f)_{V} = g_{V} \circ f_{V}$.
		\item If $f_{n} : (a, b) \to \R$ converge pointwise to $f$, then the same holds true for $(f_{n})_{V}$'s.
	\end{enumerate}
\end{prop}

These properties make it clear that such definition is natural. We will drop the subscript $V$ and identify $f$ with its matrix function $f_{V}$ if $V$ is clear from context.

\section{Holomorphic functional calculus}

If $f$ is entire, there's another way to appoach matrix functions. As $f$ can be written as
\[
	f(z) = \sum_{n = 0}^{\infty} a_{n} z^{n},
\]
power series convergent whole any $z \in \C$, we should have
\[
	f_{V}(A) = \sum_{n = 0}^{\infty} a_{n} A^{n}.
\]
This matrix power series indeed converges as $\|A^{n}\| \leq \|A\|^{n}$. Also, this definition coincides with the spectral one. Indeed, if one writes $f_{n} : z \mapsto  \sum_{k = 0}^{n} a_{n} z^{n}$, then we have, by definition,
\[
	\sum_{n = 0}^{\infty}a_{n} A^{n} = \lim_{n \to \infty} \left[(f_{n})_{V}(A) \right] = f_{V}(A),
\]
by point (6) of proposition (\ref{basic_matrix}).

Note however that the power series definition makes perfect sense even if $a_{n} \notin \R$ and even better, $A$ need not be real.

If $f$ is not entire, the power series might not converge every $A \in \H_{(a, b)}(V)$. Instead, we can more generally use Cauchy's integral formula for matrix functions.
\[
	f_{V}(A) = \int_{\gamma} (z I - A)^{-1} f(z) dz,
\]
where $\gamma$ is simple closed curve enclosing the spectrum of $A$. This formula is immediate when viewed in a eigenbasis of $A$. Again, this formula makes perfect sense even for non-real $A$, given that spectrum of $A$ lies in the domain of $f$.

\section{Derivative of a matrix function}

If $f$ is analytic, for suitable $\gamma$ we have
\begin{align*}
	f(B) - f(A) &= \frac{1}{2 \pi i}\int_{\gamma} (z I - B)^{-1} f(z) dz - \int_{\gamma} (z I - A)^{-1} f(z) dz \\
	&=  \frac{1}{2 \pi i}\int_{\gamma} (z I - B)^{-1} (B - A) (z I - A)^{-1} f(z) dz.
\end{align*}
Writing $B = A + t H$, and letting $t \to 0$ we get
\begin{align*}
	\lim_{t \to 0} \frac{f(A + tH) - f(A)}{t} &= \lim_{t \to 0}\int_{\gamma} (z I - A - t H)^{-1} H (z I - A)^{-1} f(z) dz \\
	&= \int_{\gamma} (z I - A)^{-1} H (z I - A)^{-1} f(z) dz.
\end{align*}

Derivative of $f$ at $A$ is hence the linear map
\begin{align*}
	H \mapsto \int_{\gamma} (z I - A)^{-1} H (z I - A)^{-1} f(z) dz.
\end{align*}

If we write everything in the eigenbasis of $A$, $A = (\lambda_{i} \delta_{i, j})_{1 \leq i, j \leq n}$ and $H = (H_{i, j})_{1 \leq i,j \leq n}$, we have
\begin{align*}
	\int_{\gamma} (z I - A)^{-1} H (z I - A)^{-1} f(z) dz &= \left(H_{i, j} \int_{\gamma} (z - \lambda_{i})^{-1} (z - \lambda_{j})^{-1} f(z) dz \right)_{1 \leq i, j \leq n} \\
	&= \left(H_{i, j} [\lambda_{i}, \lambda_{j}]_{f} \right)_{1 \leq i, j \leq n} \\
	&= H \circ \left([\lambda_{i}, \lambda_{j}]_{f} \right)_{1 \leq i, j \leq n}.
\end{align*}
Here $\circ$ denotes the Hadamard product of matrices, given by $(A \circ B)_{i, j} = A_{i, j} \circ B_{i, j}$.

This formula holds even if $f$ is not analytic, namely as long as $f \in \C^{1}(a, b)$. Indeed, by polynomial interpolation it is sufficient to prove the following lemma.
\begin{lem}
	If $f \in C^{1}(a, b)$, $A \in \H_{(a, b)}$ such that $f(\lambda_{i}) = 0 = f'(\lambda_{i})$ for $1 \leq i \leq n$, then
	\[
		\|f(A + H)\| = o(\|H\|).
	\]
\end{lem}
\begin{proof}
	By lemma \ref{spectrum_stability} we see that $|\lambda_{i}(A + H) - \lambda_{i}(A)| \leq n \|H\|$. Now by Taylor expansion $|f(\lambda_{i}(A + H))| =  n \|H\| f'(\xi_{i})$ for some $\xi_{i} = \xi_{i, A, H}$ on $(\lambda_{i}(A) - n \|H\|, \lambda_{i}(A) + n \|H\|)$. Now
	\begin{align*}
		\|f(A + H)\| \leq \sum_{i = 1}^{n} |\lambda_{i}(A + H)| \|P_{v_{i}}\| \leq n \|H\| \sum_{i = 1}^{n} |f'(\xi_{i, A, H})|.
	\end{align*}
	But by continuity of $f'$ the sum tends to zero as $\|H\| \to 0$.
\end{proof}

Derivative of matrix function can be also approached via power series. Since
\begin{align*}
	(A + t H)^{k} = \sum_{j = 0}^{k}t^{j}\sum_{\atop{i_{0}, i_{1}, \ldots, i_{j} \geq 0}{i_{0} + i_{1} + \ldots + i_{j} = k - j}}^{k - 1} A^{i_{0}} H A^{i_{1}} H \cdots H A^{i_{j}},
\end{align*}
we have
\begin{align*}
	\lim_{t \to 0}\frac{(A + t H)^{k} - A^{k}}{t} = \sum_{j = 0}^{k - 1}A^{j} H A^{k - 1 - j}.
\end{align*}
With the same notation as before, we have for eigenbasis of $A$
\begin{align*}
	\sum_{j = 0}^{k - 1}A^{j} H A^{k - 1 - j}= \sum_{j = 0}^{k - 1}\left( \lambda_{k}^{j}H_{k, l} \lambda_{l}^{k - 1 - j}\right)_{1 \leq k, l \leq n} = \left(H_{k, l} [\lambda_{k}, \lambda_{l}]_{(x \mapsto x^{k})}\right)_{1 \leq k, l \leq n}.
\end{align*}
Summing this identity over the Taylor terms yields the derivative formula for entire functions.

TODO:
\begin{itemize}
	\item Basic definition
	\item Equivalent definitions
	\item Continuity properties
	\item Examples
	\item Calculating with matrix functions
	\item Smoothness properties, derivative formulas, Hadamard product
	\item Cauchy's integral formula
	\item Jordan block formula
	\item How to extend functions $f : (a, b)^2 \to \R^{2}$ to a matrix function taking two entries? What is $f(A, B)$? If $A$ and $B$ commute, there exists $h_{A}, h_{B} : \R \to (a, b)$, $C \in \H$ such that $h_{A}(C) = A$ and $h_{B}(C) = B$ and we should hence define $f(A, B) = f(h_{A}(C), h_{B}(C))$. What about the general case?
\end{itemize}
