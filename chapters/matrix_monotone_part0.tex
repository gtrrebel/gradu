\chapter{Matrix monotone functions -- part 0}

\section{Disclaimer}

Let's be honest: this master's thesis is not really about matrix monotone functions. What is it about, then? Well, unfortunately the only way I know how to answer that question is to explain what the matrix monotone functions are.\footnote{Worry not: one need not read beyond this chapter to get some kind of answer to the question.} Hence the title.

\section{What are matrix monotone functions?}

\begin{maar}\label{main_def}
	Let $(a,b) \subset \R$ be an open, possibly unbounded interval and $n$ positive integer. We say that $f : (a, b) \to \R$ is \textbf{$n$-monotone} or \textbf{matrix monotone} of order \textbf{$n$} on $(a, b)$, if for any two $n \times n$ Hermitian matrices $A$ and $B$ with spectra in $(a, b)$, such that $B - A$ is positive semidefinite, also $f(B) - f(A)$ positive semidefinite. Here $f(A)$ and $f(B)$ are defined via functional calculus.
\end{maar}

Now, it might not be too big of a surprise that, on the surface level at least, the main question of this thesis is the following.

\begin{quest}\label{question_1}
	Fix a positive integer $n$ and an open interval $(a, b)$. Which functions are $n$-monotone on $(a, b)$?
\end{quest}

If all this makes sense to you, great! Feel free to skip this section. If not, what follows is an attempt to give some kind of handwavy picture of the setup. Alternatively, if you don't like handwaving, you may feel free to visit chapter 2 for rigorous foundations.

Matrix monotonicity is a generalization of standard monotonicity of real functions: now we are just having functions mapping matrices to matrices. Formally, $f$ is \textbf{matrix monotone} if for any two matrices $A$ and $B$ such that
\begin{align*}
	A \leq B
\end{align*}
we also have
\begin{align*}
	f(A) \leq f(B).
\end{align*}

This kind of function might be more properly called \textbf{matrix increasing} but we will stick to monotonicity for couple of reasons:
\begin{itemize}
	\item For some reason, that is what people have been doing in the field.
	\item It doesn't make much difference whether we talk about increasing or decreasing functions, so we might just ignore the latter but try to symmetrize our thinking by the choice of words.
	\item Somehow I can't satisfactorily fill the following table:
	\begin{center}
	\begin{tabular}{| c | c |}
		\hline
		monotonic & monotonicity \\
		\hline
		increasing & ? \\
		\hline
	\end{tabular}
	\end{center}
	How very inconvenient.
\end{itemize}

Of course, it's not really obvious how one should make any sense of this ``definition''. There are essentially two things to understand.
\begin{itemize}
	\item How should matrices be ordered?
	\item How should functions act on matrices?
\end{itemize}
Both of these questions can be certainly answered in many ways, but for both of them there is very natural, in fact tensorial, answer. Instead of comparing matrices we can compare bilinear forms, $(0, 2)$-tensors (bilinear maps $V^2 \to \R)$. Similarly we can naturally apply function to linear mappings, $(1, 1)$-tensors (bilinear maps $V^{*} \times V \to \R$). Here $V$ is a $n$-dimensional vector space over $\R$.

For matrix (bilinear form) ordering we should first understand which matrices are \textit{positive}, which here, a bit confusingly maybe, means ``at least zero". We say that a form is positive if its diagonal is non-negative. This gives a partial order on the space of all bilinear forms.

For matrix functions, i.e. ``how to apply function to matrix" the idea is to take a real function, $f : \R \to \R$, and interpret it as function $f : \R^{n \times n} \to \R^{n \times n}$, \textit{matrix function}. Polynomials extend rather naturally, given the ring structure of linear maps themselves. If the argument (a linear map) is diagonalizable, this extension merely applies the function to the eigenvalues. This motivates us to define $f(A)$, for a linear map $A$, to be the linear map with the same eigenspace structure as $A$ but the eigenvalues changed from $\lambda \to f(\lambda)$, respectively. All this works for diagonalizable maps with real eigenvalues, so the domain isn't quite $\R^{n \times n}$, though. This extension idea is called \textbf{functional calculus}.

All this is kind of enough to make sense of matrix monotonicity, but to drastically simplify the setup it is customary to restrict the attention to a special set of diagonalizable matrices, which in this text are called \textbf{real maps}. They are exactly the symmetric matrices and they hold special place amidst the set of all matrices.

\begin{itemize}
	\item They exactly correspond to symmetric bilinear forms.
	\item They correspond to diagonalizable linear maps with real eigenvalues and orthogonal eigenbasis.
\end{itemize}

In the second point we are thinking about everything in terms of the standard inner product of $\R^n$. So the statement should be corrected to

\begin{itemize}
	\item If considered as a matrix of a linear map with respect to the standard orthonormal basis of $\R^n$ (with the standard inner product), then the linear map is diagonalizable with real eigenvalues, and has orthogonal eigenbasis.
\end{itemize}

Real maps are usually called \textbf{Hermitian} or \textbf{self-adjoint matrices}, whereas positive matrices are called \textbf{positive semidefinite matrices}. Now the definition of matrix monotinicity \ref{main_def} should make sense. We will call positive matrices \textbf{positive maps}.

Whether one should think about real maps as matrices, bilinear forms or linear maps depends on the context. If one does calculations, one might think about matrices. If one thinks about additive structure, bilinear forms are better suited. And of course functional calculus makes only sense with linear maps. We use the (linear) map terminology throughout mainly because it short. Also, it is a constant reminder that there is something tensorial going on.

\section{\ldots And why should we care?}

It's easy to come up with one family of matrix monotone functions: $x \mapsto \alpha x + \beta$ for $\alpha, \beta \in \R$ with $\alpha \geq 0$. Such affine functions are $n$-monotone for every $n \geq 1$ on $(-\infty, \infty)$. They are the only easy examples of matrix monotone functions.

But there are lot more.

Matrix monotone functions are truly horrible. All matrix monotone functions are increasing (in the usual sense) but not every increasing function is matrix monotone. They have some obscure regularity properties. Constructing non-trivial matrix monotone functions is a pain. Although usual increasing real functions and matrix monotone functions should be very much interlinked, hardly any of the properties of increasing functions pass on to matrix monotonicity. Generally, if one attacks matrix monotone functions, especially of order $n > 2$, and doesn't use sophisticated weaponry, one will perish. The reader is encouraged to try.

All this is exactly what makes them so interesting. One is driven to ask the question:

\begin{quest}
	How should one think about matrix monotone functions?
\end{quest}

If this sounds like the same question to you, think about increasing functions on $\R^n$. Function $f : \R^n \to \R$ is called $n$-increasing (this termonology lasts only for next couple of paragraphs) if $f(x_{1}, x_{2}, \ldots, x_{n}) \leq f(y_{1}, y_{2}, \ldots, y_{n})$ whenever $x_{i} \leq y_{i}$ for every $1 \leq i \leq n$. Which functions are $n$-increasing? I would argue that $n$-increasing functions are awful, much more so than the usual ($1$-)increasing functions. The reason is that they don't have good ``building blocks".

One might say that ``non-negative derivative" property (let's ignore regularity issues for a while) makes increasing functions easy to understand, and while there is certain truth to that, I would argue that what makes them so simple is really the dual property: ``increasing functions are sums of increasing step functions". This roughly implies that in order to understand increasing functions, it is enough to understand step functions, or just step functions with one jump upwards.

Note that we are heavily using the fact that increasing functions (of all types introduced before) form a convex cone:

\begin{maar}
	Subset $C$ of a vector space $V$ over $\R$ is a convex cone if whenever $v, w \in C$ and $\alpha, \beta \geq 0$, also $\alpha v + \beta w \in C$.
\end{maar}

Also, applicability of the ``only needing to understand step functions" is somewhat limited: it doesn't really explain smoothness phenomena all too well, for instance. But it is always nice to know that some objects are really sums of other much simpler objects.

There's no such nice dual property for $n$-increasing functions (for $n > 1$). One can understand them locally with derivatives, but there are no simple decompositions. Same thing could be said about convex functions on $\R^n$.

Much more importantly for us, these is no such nice additive structure for $n$-monotone functions. This is by no means trivial (as it is not even with $n$-increasing functions). It is also not even clear what one means by ``nice" and whether even increasing functions are that ``nice" in the end. These ideas shall however merely work as our guideline, so one should not feel too troubled.

All these issues can be, in a way, avoided by change of perspective: instead of trying to characterize matrix monotone functions by expressing them as sums of something simple, we express the definition itself as a sum of somethings simple. In particular we try to understand the ``dual" (or a ``predual" to be exact) of matrix monotone functions.

\section{Non-standardly defined dual cones}\label{dual_definitions}

Let in the following $V$ be a vector space over $\R$ and denote its dual by $V^{*}$.

\begin{maar}
	For a subset $C^{*}$ of $V^{*}$ we define its \textbf{dual cone} to be
	\begin{align*}
		C = \{v \in V | w^{*} v \geq 0 \text{ for every $w^{*} \in C^{*}$}\} \subset V.
	\end{align*}
\end{maar}

One immediately makes the following observation justifying the terminology.

\begin{lause}\label{dual_cone}
	Let $C^{*} \subset V^{*}$. Then the dual cone of $C^{*}$ is a convex cone.
\end{lause}

\begin{maar}
	Let $C \subset V$. Then $C^{*}$ is a \textbf{predual} of $C$ if $C$ is the dual cone of $C^{*}$.
\end{maar}

Of course only convex cones have preduals. Easy examples show that preduals are not unique in general (in fact never).

As an example, for an open interval $(a, b)$ consider the set
\begin{align*}
	P_{1}(a, b) := \{\text{Increasing functions $f : (a, b) \to \R$}\}.
\end{align*}
This set is a convex cone. If one denotes the evaluation functional or measure at $x$ by $\delta_{x}$, i.e. $\delta_{x}(f) = f(x_{0})$, then one possible predual of $P_{1}(a, b)$ if given by
\begin{align*}
	\{\delta_{y} - \delta_{x} | a < x < y < b \}.
\end{align*}

I hope the reader agrees that this predual is in many ways much simpler than the set of increasing functions (at least if one looks at objects themselves) and yet it carries the information thereof. As we will see, if chosen suitably, preduals can offer convenient and clean language for talking about the cone itself. And that is what this thesis is really about.

One can also bring the dual language to topological setting. If $V$ is further a topological vector space, denote its continuous dual by $V^{*}$. Now the notions of convex cone, dual cone and predual have natural counterparts in this setting. It also makes sense to talk about \textbf{closed convex cones}. One easily checks the following.

\begin{lause}\label{positive_machine}
	Let $V$ be a topological vector space and $C^{*}$ a subset of its continuous dual. Then
	\begin{align*}
		C := \{v \in V | w^{*}(v) \geq 0 \text{ for every $w^{*} \in C^{*}$}\}
	\end{align*}
	is a closed convex cone of $V$.
\end{lause} 

\textbf{Warning!} The dual and predual terminology above is non-standard. Often dual cones are subsets of the dual space (with definition as one would expect). Preduals would be defined similarly.

\section{Contents}

There are two goals to this thesis. The first is answering question \ref{question_1} (and really understanding the answer). The second one is answering the obvious follow up:

\begin{quest}\label{question_2}
	Fix an open interval $(a, b)$. Which functions are $n$-monotone on $(a, b)$ for any $n \geq 1$?
\end{quest}

This text is written for explorer like the author. This means couple of things.
\begin{itemize}
	\item The main results are not explained in the introduction.
	\item The story aims to be continuous: topics of adjacent chapters, sections and arguments should be close to each other.
\end{itemize}

Aim of this approach is to encourage readers own exploration. Most of the results of this thesis are nothing but straighforward and not very elegant calculations.

TODO

Arguments of this thesis are largely elementary, requiring only basic calculus, complex analysis, linear algebra and topology. Many of the underlying ideas and heuristics are however inspired by functional analysis and measure theory, so it pays to have a good understanding of them.

\section{Spoilers}

If you're curious, the answers for these questions are given by Theorem \ref{main_theorem} (question \ref{question_1}) and Theorems \ref{integral_loewner} and \ref{weak_loewner} (question \ref{question_2})

Aim of the chapters entitled ``Matrix monotone functions -- part $i$" for $0 \leq i \leq 3$ is to pursue these main goals. Other chapters, ones with even number, offer supplementary information.

The whole theory of matrix monotone functions originates from a 1934 paper by Charles Loewner (then known as Karl Löwner) entitled ``Über monotone Matrixfunktionen" \cite{Low}. In the paper Loewner asked the questions \ref{question_1} and \ref{question_2}, and gave them both answers. Since then the theory has been reworked and expanded by various authors, and many open questions remain. This text is an amalgamation of countless works on the subject. ``Notes and references" -sections at the end of each chapter (except this one) are an attempt to give attribution to these ancestors.

Also, there are no figures. Apologies.

\section{Notation and conventions}

\begin{tabular}{r l}
	w.l.o.g & without loss of generality \\
	t.f.i.f & this follows immediately from \\
	$\Lip(f)$ & (the best) Lipschitz constant of $f$ \\
	$V^{*}$ & (continuous) dual of (topological) vector space $V$ \\
	$\cone(S)$ & non-negative linear combinations of elements of $S$, \\
	& where $S$ is subset of some vector space\\
	$\L(V)$ & linear self-maps over vector space $V$ \\
	$\H(V)$ & real maps over inner product space $V$ \\
	$\H_{+}(V)$ & positive maps over inner product space $V$ \\
	$\H_{(a, b)}(V)$ & real maps over inner product space $V$ with spectra on $(a, b)$ \\
	$P_{n}(a, b)$ & $n$-monotone functions on $(a, b)$ \\
	$P_{\infty}(a, b)$ & intersection of $P_{n}(a, b)$ for $n \geq 1$, $\infty$-monotone functions on $(a, b)$ \\
	$P_{W} = P_{W, V}$ & orthogonal projection to $W \subset V$ \\
	$A^{*}$ & adjoint of a map $A \in \L(V)$ \\
	$f_{V}$ & matrix function lift of $f$ to $V$ \\
	$\R_{n}[x]$ & Real polynomials of degree at most $n$\\
	$\C_{n}[x]$ & Complex polynomial of degree at most $n$\\
	$r^{*}$ & for given rational function $r$, $r^{*}$ is the rational function with $r^{*}(z) = \overline{q(\overline{z})}$ \\
	$N(r)$ & $r r^{*}$ \\
	$C^{k}(a, b)$ & $k$ times continuously differentiable functions on $(a, b)$ \\
\end{tabular}
\\
We also preserve certain letters to have particular meaning.
\\
\begin{tabular}{r l}
	$a, b$ & extended reals with $-\infty \leq a < b \leq \infty$ \\ 
	$n, k$ & positive integers \\
	$V$ & inner product space over $\C$ (with inner product $\langle \cdot, \cdot \rangle$) of dimension $n$ \\
\end{tabular}

TODO:

\begin{itemize}
	\item Main theorem simplification: note that the rational function is non-negative outside $(a, b)$ just by monotonicity of inverse function. What kind of rational functions will there be?
	\item Use ``To wit".
\end{itemize}

TODO: check the TODO-lists (in comments).


















\begin{comment}

\section{How should one think about matrix monotone functions?}

Now, if one is seeking to characterize matrix monotone functions, this is the question one should answer first. Below three different ``answers" are given.

\subsection{Answer 1}

It should be no big surprise that the function $f : x \mapsto \alpha x + \beta$ is $n$-monotone for every $n \geq 1$ whenever $\alpha, \beta \in \R$ with $\alpha \geq 0$. As $f$ is polynomial, for every matrix we have $f(A) = \alpha A + \beta I$. Now we should prove that if $B - A$ is positive map then also $(\alpha B + \beta) - (\alpha A + \beta)$ is positive. But
\begin{align*}
	(\alpha B + \beta) - (\alpha A + \beta) = \alpha (B - A):
\end{align*}
as $B - A$ is positive, so is $\alpha (B - A)$, as non-negative multiple of positive map (bilinear form).

That's pretty much only simple example of a matrix monotone function.

See, there's a problem. In many ways positive maps work like positive real numbers, but some crucialities are missing. Properties of bilinear forms imply immediately that positive maps have the following properties:

\begin{enumerate}
	\item If $A$ and $B$ are positive, then so is $A + B$.
	\item If $A$ is positive and $\alpha \geq 0$, then also $\alpha A$ is positive.
\end{enumerate}

But we can't take product (composition) of positive maps, or to be precise, there's no reason for composition to be positive.

Why is this problem? The product property of reals implies that product of two positive increasing functions is increasing. As an example the function $f : x \mapsto x^2$ is increasing on $(0, \infty)$.

Same argument cannot however be applied to prove the matrix counterpart, i.e. it's not clear whether $0 \leq A \leq B$ implies $A^2 \leq B^2$. It turns out that $f$ is \textbf{not} matrix monotone (for any $n > 1$).

\subsection{Answer 2}



\subsection{Answer 3}

Firstly, how should one think about positive matrices? It turns out that positive matrices have non-negative eigenvalues and conversely symmetric matrices having non-negative eigenvalues are positive. So positive matrices are some kind of $n$-tuples with some kind of eigenspace information. Bilinear form can be thought of some kind of bilinear combination of the eigenvalues, where the eigenspaces tell us what kind of exactly. The diagonal of the bilinear form is some kind of positive linear combination of the eigenvalues.

If symmetric matrices are thought of as tuples of real numbers with some extra (eigenspace) information, matrix functions simply act on the elements of the tuple separately.

Now if $A \leq B$ for some symmetric matrices, the quadratic form of $B - A$ is non-negative, so the quadratic form of $A$ is smaller than that of $B$. This roughly means that the eigenvalues of $A$ are ``on average" smaller than the eigenvalues of $B$, where ``on averate" should be understood as combination of some kind of set of weighted averages determined by the respective eigenspaces of $A$ and $B$.

Similarly, $f(A) \leq f(B)$ means that some kind of weighted averages of $f(\lambda)$ where $\lambda$ runs over the eigenvalues of $A$ are less than the respective weighted averages with $B$. So, all in all matrix monotone functions of order $n$ that preserve some kind of order of $n$-tuples.

One way to write all this down is the following: a function $f$ is $n$-monotone if for every $i \in I$ we have
\begin{align*}
	\sum_{j = 1}^{n} a_{j, i} f(\lambda_{j, i}) \leq \sum_{j = 1}^{n} a_{j, i}' f(\lambda_{j, i}')
\end{align*}
where, as $i$ ranges over some indexing set $I$, the numbers
\begin{align*}
a_{1, i}, a_{2, i}, \ldots, a_{n, i}, \\
a_{1, i}', a_{2, i}', \ldots, a_{n, i}', \\
\lambda_{1, i}, \lambda_{2, i}, \ldots, \lambda_{n, i}, \\
\lambda_{1, i}', \lambda_{2, i}', \ldots, \lambda_{n, i}'
\end{align*}
range over some set of $4 n$ tuples of real numbers.

If all this sounds confusing and terribly vague$\ldots$ you're not alone. Unfortunately though, the relationship between eigenvalues and additive structure of real maps (both of which we need to understand matrix monotonicity) is really subtle and still rather poorly understood: it's not at all clear how to make all this explicit. But we will do it.

\section{Why matrix monotone functions?}

There is nothing really special about matrix monotone functions per se. They have (apparently) some ``real" applications in quantum physics (see e.g. TODO) but such are not discussed in this thesis. Instead, what makes them special for us is that they are prime examples of complicated positive objects.

\section{What are positive objects?}

Positive objects are objects which somehow work like positive numbers. Positive objects are modelled by \textit{closed salient cones}. 

\begin{maar}
	Let $V$ be a topological vector space over $\R$ or $\C$ and $C \subset V$. Then $C$ is \textit{closed salient cone} of $V$ if
	\begin{enumerate}[(i)]
		\item For every $v \in C$ and $\alpha > 0$ we have $\alpha v \in C$.
		\item For every $v, w, \in C$ we have $v + w \in C$.
		\item $0 \in C$.
		\item If both $v \in C$ and $-v \in C$, then $v = 0$.
		\item $C$ is closed.
	\end{enumerate}
\end{maar}

Subsets of vector spaces satisfying the first one, two, three and four properties are called \textit{cones}, \textit{convex cones}, \textit{pointed cones} and \textit{salient cones}, respectively. To shorten the terminology we call closed salient cones \textbf{\textit{proper cones}}, as is also somewhat customary.

(Pictures of every possibility)

\begin{esim}
	Let $(a, b)$ be an open interval and $V = \{ \text{functions $(a, b) \to \R$} \}$. Then $C = \{ \text{increasing functions $(a, b) \to \R$} \}$ is morally-but-not-quite proper cone of $V$.
\end{esim}

\section{Goals}

Main goal of this thesis is to understand two basic results (theorems TODO and TODO) on matrix monotone functions, which read roughly as follows.

\begin{lause}
	If a function is matrix monotone for $n \times n$ matrices, then it is $C^{2 n - 3}$.
\end{lause}

\begin{lause}
	If a function is matrix monotone for $n \times n$ matrices for every $n$, then it is analytic and extends analytically to upper half-plane.
\end{lause}

Both of these result can be turned to ``if and only" -characterizations, but let us ignore the technicalities for know. The giveaway is that matrix monotone functions are quite regular.

Why is that? Where does regularity come from? The only thing we know about matrix monotone functions is essentially some positivity condition (difference of matrices being positive). The moral goal of the thesis is to explain that it is exactly the positivity that leads to regularity.

\begin{phil}
	Positiveness leads to regularity.
\end{phil}

\section{More on positive objects}

As mentioned, proper cones are an attempt to generalize the set of non-negative real numbers to a vector space. The first four conditions are pretty much direct translations of the axioms of non-negative numbers to vector spaces. The closedness condition is more of a convenience. Every proper cone gives rise to partial order on the vector space. This makes the vector space (not surprisingly) \textit{ordered topological vector space}.

\begin{maar}
	Let $V$ be a topological vector space over $\R$ or $\C$ and $\leq \in V^{2}$ a relation on $V$. Then $(V, \leq)$ is \textit{ordered topological vector space} if
	\begin{enumerate}[(i)]
		\item $\leq$ is partial order in $V$, that is
		\begin{enumerate}
			\item $v \leq v$ for any $v \in V$.
			\item If $v, w, u$ such that $v \leq w$ and $w \leq u$, then also $v \leq u$.
			\item If for some $v, w \in V$ we have both $v \leq w$ and $w \leq u$, then $v = w$.
		\end{enumerate}
		\item If $v, w \in V$ are such that $v \leq w$, then also $v + u \leq w + u$ for any $u \in V$.
		\item If $v, w \in V$ are such that $v \leq w$, then for any $\alpha \in [0, \infty)$ also $\alpha v \leq \alpha w$.
		\item The set $\{v \in V | 0 \leq v\}$ is closed.
	\end{enumerate}
\end{maar}

Of course, one should always hope the order $\leq$ to be total, but this is usually wishful thinking, at least if one wants $\leq$ to be canonical in some way.

\begin{prop}
	For any proper cone $C$ of topological vector space $V$ the relation $\leq_{C}$ defined by
	\begin{align*}
		v \leq_{C} w \Leftrightarrow w - v \in C
	\end{align*}
	makes $(V, \leq_{C})$ ordered topological vector space.
\end{prop}
\begin{proof}
	Easy to check.
\end{proof}

Conversely, for every ordered topological vector space $(V, \leq)$ the set $\{v \in V | 0 \leq v\}$ is a proper cone (which induces $\leq$). Elements of the set $\{v \in V | 0 \leq v\}$ are called the \textit{positive elements} of $(V, \leq)$, or simply \textit{positive}. This terminology is rather unfortunate: if $V = \R$, then the positive elements in $(\R, \leq)$ coincide with the non-negative reals (and not the positive ones). The problem is that with non-total orders the term non-negative should mean something totally different. While potentially confusing, we stick with term positive because of the following reasons:
\begin{enumerate}
	\item It's short.
\end{enumerate}

\section{Structure}

The next four chapters are devoted to the four main proper cones of this thesis.

\begin{itemize}
	\item \textbf{Positive maps} (i.e. positive semidefinite maps). These are the main objects of interest and they are used to define the matrix monotone functions, as illustrated in the introduction. As a comprehensive account of these creatures is far beyond the scope of this thesis, only certain aspects, most relevant to the main topic, are discussed. Should the reader lack familiarity with positive maps, the author farmly recommends TODO as a welcome to the topic.

	Nevertheless, chapter should be self-contained given the background topics (in the next section).

	\item \textbf{k-tone functions}. $k$-tone functions are essentially functions with non-negative $k$'th derivative. Many of the regularity phenomena discussed in this thesis can be understood through the properties of these functions.

	\item \textbf{Pick functions}. Pick functions are analytic functions on upper half-plane with non-negative imaginary part. It turns out that these functions are intimately linked to matrix monotone functions.

	\item \textbf{Matrix monotone functions}. These are the main objects of the study and all the theory build so far is finally connected.
\end{itemize}

To be entirely honest, only the first of the four proper cones is really a proper cone (three others fail the fourth condition), but the shortcomings of the others are so minor and canonical that we still call them proper.

The last chapter discusses TODO.

\section{Plan of attack}

This master's thesis is a comprehensive review of the rich theory of matrix monotone functions.

Master's thesis is to be structured roughly as follows.

\begin{enumerate}
	\item Introduction
		\begin{itemize}
			\item Introduction to the problem, motivation
			\item Brief definition of the matrix monotonicity and convexity
			\item Past and present (Is this the right place)
				\begin{itemize}
					\item Loewner's original work, Loewner-Heinz -inequality
					\item Students: Dobsch' and Krauss'
					\item Subsequent simplifications and further results: Bendat-Sherman, Wigner-Neumann, Koranyi, etc.
					\item Donoghue's work
					\item Later proofs: Krein-Milman, general spectral theorem, interpolation spaces, short proofs etc.
					\item Development of the convex case
					\item Recent simplifications, integral representations
					\item Operator inequalities
					\item Multivariate case, other variants
					\item Further open problems?
				\end{itemize}
			\item Scope of the thesis
		\end{itemize}
	\item Positive matrices
		\begin{itemize}
			\item Motivation via restriction, basics
			\item Spectral theorem
			\item Congruence
			\item Characterizations
			\item Applications
			\item Spectrum
		\end{itemize}
	\item Divided differences
		\begin{itemize}
			\item Definition (what kind of?)
			\item Mean value theorem
			\item Smoothness
			\item k-tone functions on $\R$
			\item Cauchy's integral formula
			\item Regularizations
		\end{itemize}
	\item Matrix functions
		\begin{itemize}
			\item Several definitions: spectral and cauchy
			\item Smoothness of matrix functions
		\end{itemize}
	\item Pick functions
		\begin{itemize}
			\item Basic definitions and properties
			\item Pick matrices/ determinants
			\item Compactness
			\item Pick-Nevanlinna interpolation theorem
			\item Pick-Nevanlinna representations theorem
		\end{itemize}
	\item Monotonic and convex matrix functions
		\begin{itemize}
			\item Basics
				\begin{itemize}
					\item Basic definitions and properties (cone structure, pointwise limits, compositions etc.)
					\item Classes $P_{n}, K_{n}$ and their properties
					\item $-1/x$
					\item One directions of Loewner's theorem
					\item Examples and non-examples
				\end{itemize}
			\item Pick matrices/determinants vs matrix monotone and convex functions
				\begin{itemize}
					\item Proofs for (sufficiently) smooth functions
				\end{itemize}
			\item Smoothness properties
				\begin{itemize}
					\item Ideas, simple cases
					\item General case by induction and regularizations
				\end{itemize}
			\item Global characterizations
				\begin{itemize}
					\item Putting everything together: we get original characterization of Loewner and determinant characterization
				\end{itemize}
		\end{itemize}
	\item Local characterizations
		\begin{itemize}
			\item Dobsch (Hankel) matrix: basic properties, easy direction (original and new proof)
			\item Integral representations
				\begin{itemize}
					\item Introducing the general weight functions for monotonicity and convexity (and beyond?)
					\item Non-negativity of the weights
					\item Proof of integral representations
				\end{itemize}
			\item Proof of local characterizations
		\end{itemize}
	\item Structure of the classes $P_{n}$ and $K_{n}$, interpolating properties (?)
		\begin{itemize}
			\item Strict inclusions, strict smoothness conditions
			\item Strictly increasing functions
			\item Extreme values
			\item Interpolating properties
		\end{itemize}
	\item Loewner's theorem
		\begin{itemize}
			\item Preliminary discussion, relation to operator monotone functions
			\item Loewner's original proof
			\item Pick-Nevanlinna proof
			\item Bendat-Sherman proof
			\item Krein-Milman proof
			\item Koranyi proof
			\item Discussion of the proofs
			\item Convex case
		\end{itemize}
	\item Alternative characterizations (?)
		\begin{itemize}
			\item Some discussion, maybe proofs
		\end{itemize}
	\item Bounded variations (?)
		\begin{itemize}
			\item Dobsch' definition, basic properties
			\item Decomposition, Dobsch' theorems
		\end{itemize}
\end{enumerate}

\section{How to rewrite this thesis}

\begin{enumerate}
	\item Positive maps: lose all the fat.
	\item Divided differences: concentrate on important things, namely relationship between smoothness and $k$-tone functions.
	\item Keep it relatively short, as it is (?)
	\item Pick functions: is this the place for these. Start with Schwarz lemma as an rigidity example. Then express Schwarz lemma with contour integrals: generalize, proof by tricks. Notion of Pick points, and finally Pick-Nevanlinna interpolation theorem, some form of it.
\end{enumerate}

\section{Some random ideas}
\begin{enumerate}
	\item TODO: fix Boor in the references
	\item It's easy to see that [Something]. Actually, it's so so easy that we have no excuse for not doing it.
	\item When is matrix of the form $f(a_{i} + a_{j})$ positive: $f$ is completely monotone (?).
	\item Polynomial regression...
	\item TODO: Maximum of two matrices (at least as big), $(a + b)/2 + abs(a - b)/2$
	\item If $\langle A x, y \rangle = 0$ implies $\langle x, A y \rangle = 0$, then $A$ is constant times hermitian.
	\item Angularity preserving functions
	\item If subspace of linear maps are diagonalizable with real eigenvalues, is there a inner product such that subspace consists of only Hermitian maps
	\item One should be alarmed should one see a positive cone.
	\item Make DAG (hopefully) of logical structure of the thesis, colour-coded (with respect to the topic, maybe). Theorem numbers, maybe named theorems with names. To the introduction.
	\item Cut the bullshit
	\item What about general conditions on positive cones defined via finite linear combinations of function values? Let's say finite dimensional kernel and translation invariant.
\end{enumerate}

\section{Main TODO -list}
\subsection{Missing proofs}
\begin{enumerate}
	\item Eigenvalues of $AB$ when $A$ and $B$ are positive (?)
	\item Symmetric product fail (?)
	\item Hindmarsh theorem
	\item non-smooth Dobsch char.
	\item classes are different
	\item matrix $k$-tone (please expand)
\end{enumerate}

\subsection{Sections to write}
\begin{enumerate}
	\item \textcolor{red}{Integral representations}
	\item \textcolor{red}{All ``Notes and references" sections}
	\item More on convex and $k$-tone matrix functions
\end{enumerate}

\subsection{Figures to make}
\begin{enumerate}
	\item Proof of spectral theorem
	\item Compression
	\item Pictures of Peano kernels
	\item $k$-tone functions: visual definition
	\item $k$-tone functions are smooth
	\item Pick functions and measures
	\item Mean value theorem
	\item Pick extension lemma
	\item Eigenvalue inequalities: projections and compressions
	\item Concrete Pick function extension procedure
	\item Change of eigenvalues of $A + t B$
	\item Disc lemma
	\item Cones in the introduction
\end{enumerate}

\subsection{How to rewrite things}
Outline of the new structure:
\begin{enumerate}
	\item Introduction: What exactly is here?
	\item Positive matrices and matrix functions: What exactly is here?
	\item Monotone matrix functions, part 1
	\item $k$-tone functions
	\item Pick functions
	\item Monotone matrix functions, part 2
	\item Beyond
\end{enumerate}

\end{comment}


