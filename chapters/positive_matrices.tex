\chapter{Positive matrices}

This chapter is titled ``positive matrices", although ``positive maps" might be more appropriate title. We are mostly going to deal with finite-dimensional objects, but many of the ideas could be generalized infinite-dimensional settings, where matrices lose their edge. Also, one should always ask whether it really clarifies the situation to introduce concrete matrices: matrices are good at hiding the truly important properties of linear mappings. The words ``matrix" and ``linear map" are used somewhat synonymously, although one should always remember that the former are just special representations for the latter.

\begin{comment}
\section{Motivation}

How should one order matrices? What should we require from ordering anyway?

I could just give you the answer, but instead I try to explain why it is standard in the first place.

We would definitely like to have natural total order on the space of matrices, but it turns out that are no natural choices for that. Partial order is next best thing. Recall that a partial order on a set $X$ is a binary relation $\leq$ on such that
\begin{enumerate}
	\item $x \leq x$ for any $x \in X$.
	\item For any $x, y \in X$ for which $x \leq y$ and $y \leq x$, necessarily $x = y$.
	\item If for some $x, y, z \in X$ we have both $x \leq y$ and $y \leq z$, also $x \leq z$.
\end{enumerate}

The third point is the main point, the first two are just there preventing us from doing something crazy. But we can do better: this partial order on matrices should also respect addition.
\begin{enumerate}
\item[4.] For any $x, y, z \in X$ such that $x \leq y$, we should also have $x + z \leq y + z$.
\end{enumerate}

There's another way to think about this last point. Instead of specifying order among all the pairs, we just say which matrices are positive: matrix is positive if and only it's at least $0$.

If we know all the positive matrices, we know all the ``orderings". To figure out whether $x \leq y$, we just check whether $0 = x - x \leq y - x$, i.e. whether $y - x$ is positive. Also, positive matrices are just differences of the form $y - x$ where $x \leq y$. Now, conditions on the partial order are reflected to the set of positive matrices.
\begin{enumerate}
	\item[1'.] $0$ (zero matrix) is positive.
	\item[2'.] If both $x$ and $-x$ are positive, then $x = 0$.
	\item[3'.] If both $x$ and $y$ are positive, so is their sum $x + y$.
\end{enumerate}
Here $3'$ is kind of combination of $3$ and $4$.

The terminology here is rather unfortunate. Natural ordering of the reals satisfies all of the above with obvious interpretation of positive numbers, which however differs from the standard definition: $0$ is itself positive in our above definition. This is undoubtedly confusing, but what can you do? For real numbers we have total order, so every number is either zero, strictly positive or strictly negative, so when we say non-negative, it literally means ``not negative": we get all the positive numbers and zero. But with partial orders we might get more. So the main reasons why we are using this terminology are
\begin{enumerate}
	\item It's short.
\end{enumerate}
Also, now that we have decided to preserve the word ``positive" for ``at least zero" one might be tempted to preserve ``strictly positive" for ``at least zero, but not zero". We won't do that, we save that phrase for something more important.

Back to business. When we try to define ordering of the matrices, everything of course depends on the ground field. It hardly makes any sense to order matrices over $\F_{p}$: even $1 \times 1$ matrices, namely (canonically) the elements of $\F_{p}$ defy reasonable ordering. But real numbers, for instance, have ordering, so there's a serious change that all real matrices could be ordered.

We will first try to order all real square matrices. (Actually, we won't even try to order non-square matrices.) $1 \times 1$ matrices are easy to order, but as soon one moves to larger matrices, one faces difficult decisions:

\[
	\text{Is }
	\begin{bmatrix}
		1 & 0 \\
		0 & -1 \\
	\end{bmatrix}
	\leq
	\begin{bmatrix}
		0 & 0 \\
		0 & 0 \\
	\end{bmatrix}
	\text{ or }
	\begin{bmatrix}
		1 & 0 \\
		0 & -1 \\
	\end{bmatrix}
	\geq
	\begin{bmatrix}
		0 & 0 \\
		0 & 0 \\
	\end{bmatrix}
	\text{?}
\]

That's okay, we don't necessarily have to order all pairs of matrices. But there are other problems. We would like the ordering of the matrices to be independent of the choice matrix representation. If we flip basis vectors we change the rows and colums of the matrix, so change it's sign! So if we want
\begin{itemize}
	\item Positivity is readable from the matrix with respect to an arbitrary basis, alone.
	\item Positivity doesn't depend on the chosen basis.
\end{itemize}
there is no way to assign sign to the previous matrix.

Let's consider diagonalizable matrices: by change of basis these can be written as diagonal matrices, but these diagonal values, eigenvalues, need not be real anymore. One could try to ignore the complex bases, but how often really is ignoring the complex structure satisfactory. Another approach would be only to consider matrices with real eigenvalues. There's problem here though: sum of two matrices with real eigenvalues need not have real eigenvalues, even if the former are diagonalizable! Sad as it may sound, it shouldn't be too surprising since it's not very clear that eigenvalues should have any reasonable behaviour with respect to addition. Of course, one shouldn't just take my word for it: here are congrete examples:
\[
	\begin{bmatrix}
		0 & 4 \\
		1 & 0
	\end{bmatrix}
	\text{ and }
	\begin{bmatrix}
		0 & -1 \\
		-4 & 0
	\end{bmatrix}
\]
The two matrices have both distinct eigenvalues $- 2$ and $2$ and are hence diagonlizable, but their sum has characteristic polynomial $x^2 + 9$, which most definitely has no real zeros.

There is however very special class of matrices, correponding to special class of linear mappings, which satisfy our requirements.
\begin{itemize}
	\item They have real eigenvalues.
	\item They are all diagonalizable.
	\item They are closed under sum, and (real) scalar multiplication.
\end{itemize}

\section{Hermitian maps}

Hermitian mappings will be our non-commutative playing ground in which we can define rather natural partial order. Fix any finite-dimensional inner-product space $(V, \langle \cdot, \cdot \rangle )$ over $\R$ or $\C$. For any $v \in V \setminus \{0\}$ we may define the corresponding projection, denoted by $P_{v}$ by setting $P_{v}(x) = \langle x , v \rangle /\langle v, v \rangle v$.

\begin{maar}
	Let $V$ be an finite-dimensional innerproduct space over $\R$ or $\C$. Now set of \textit{Hermitian maps} of $V$, denoted by $\H(V)$, is defined as
	\[
		\vspan \{P_{v} \mid v \in V \setminus \{0\}\},
	\]
	where the span is $\R$-linear, i.e. Hermitian maps are the maps of the form
	\[
		\sum_{i = 1}^{m} \lambda_{i} P_{v_{i}},
	\]
	for some positive integer $m$, $v_{i} \in V \setminus \{0\}$ and $\lambda_{i} \in \R$.
\end{maar}

Also note that since for any non-zero $\alpha$ and $v \in V \setminus \{0\}$ we have $P_{v} = P_{\alpha v}$, we could just as well only allow vectors of norm one in our definition of Hermitian maps. This is of course just to simplify notation. This is not the standard definition, and in a way it's horrible, but it very directly answers our needs. Remember that we want to have diagonalizable maps with real eigenvalues. Projections are very prototypical examples of such maps and since we want real eigenvalues that is simply what we require in the definition (span over $\R$). Lastly, one of course wants to take the span to have any linear structure in the first place.

It is however very surprising that such construction works: basis vectors (projections) satisfy our requirements, but there are no reasons to expect that these would be preserved in addition. This is guaranteed by the following theorem.

\begin{lause}[Spectral theorem]
	Let $V$ be an $n$-dimensional inner-product space over $\R$ or $\C$, and $A \in \H(V)$. Then there exists real numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$ and orthonormal vectors $v_{1}, v_{2}, \ldots, v_{n}$ such that
	\begin{align}\label{spectralrepr}
		A = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}}.
	\end{align}
\end{lause}

It follows that $A$ is diagonalizable, vectors $v_{1}, v_{2}, \ldots, v_{n}$ are eigenvectors of $A$ with corresponding eigenvalues $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$. Indeed, for any $v_{j}$ we have
\begin{align}
	A v_{j} = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}} v_{j} = \sum_{i = 1}^{n} \lambda_{i} \langle v_{j}, v_{i}\rangle v_{i} = \sum_{i = 1}^{n} \lambda_{i} \delta_{ij} v_{i} = \lambda_{j} v_{j}.
\end{align}

Moreover, the rank of $A$ is simply the number of non-zero eigenvalues of $A$, and the all the eigenvalues appear in the representation exactly as many times the characteristic polynomial allows.

\begin{proof}[Proof (of the Spectral theorem)]
	We prove the statement by induction on $n$ the dimension of the space $V$. The case $n = 0$ is trivial: the span itself is trivial, and it's very easy to express $0$-mapping as a empty sum.

	Now fix a positive integer $n$. 

	\textit{Step 1.} First note $A$ has at least one eigenvector over $\C$, as every other linear mapping. Let $v_{1}$ be that eigenvector (of lenght one) with corresponding eigenvalue $\lambda_{1}$.

	\textit{Step 2.} Write $A = \sum_{i = 1}^{m} c_{i} P_{u_{i}}$ for $c_{1}, c_{2}, \ldots, c_{m} \in \R$ and $u_{1}, u_{2}, \ldots, u_{m} \in V \setminus \{0\}$ are of norm $1$. Now the definition of the eigenvector and eigenvalue rewrites to
	\[
		A v_{1} = \sum_{i = 1}^{m} c_{i} \langle v_{1}, u_{i}\rangle u_{i} = \lambda_{i} v_{1}.
	\]
	Take inner product with $v_{1}$ from both sides to get
	\[
		\sum_{i = 1}^{m} c_{i} \langle v_{1}, u_{i}\rangle \langle u_{i}, v_{1} \rangle = \lambda_{1} \langle v_{1}, v_{1} \rangle.
	\]
	Since $\langle v_{1}, u_{i}\rangle \langle u_{i}, v_{1} \rangle = |\langle v_{1}, u_{i}\rangle|^{2}$, the left-hand side is real, so is right-hand side, and finally so is $\lambda_{1}$.

	\textit{Step 3.} Now we have found $\lambda_{1}$ and $v_{1}$ as in the theorem statement. The idea is then to factorize $A$ to two parts: projection corresponding to $v_{1}$ and an orthogonal part living in $v_{1}^{\perp}$. If our choices for $\lambda_{1}$ and $v_{1}$ work, anything orthogonal to $v_{1}$ maps to something orthogonal to $v_{1}$. We verify this.

	Take any $v \perp v_{1}$. Now,
	\begin{eqnarray*}
		\langle A v, v_{1} \rangle &=& \langle \sum_{i = 1}^{m} c_{i} \langle v, u_{i}\rangle u_{i}, v_{1} \rangle \\
		= \sum_{i = 1}^{m} c_{i} \langle v, u_{i}\rangle \langle u_{i}, v_{1} \rangle &=& \overline{\sum_{i = 1}^{m} c_{i} \langle v_{1}, u_{i}\rangle \langle u_{i}, v \rangle} \\
		= \overline{\langle \sum_{i = 1}^{m} c_{i} \langle v_{1}, u_{i}\rangle u_{i}, v \rangle} &=& \overline{\langle A v_{1}, v \rangle} \\
		= \overline{\langle \lambda_{1} v_{1}, v \rangle} &=& 0,
	\end{eqnarray*}
	so also $A v \perp v_{1}$. It follows that we get a map $A' : v_{1}^{\perp} \to v_{1}^{\perp}$ which extends to $A$.

	\textit{Step 4.} We would naturally like to use our induction hypothesis for this map, but for that we need it to be of special form: although the map is well defined, there is no reason to expect that we could just pick some of the $u_{i}$:s for our representation. But we can do something else.

	Write $u_{i} = u_{i}' + \alpha_{i} v_{1} = P_{v_{1}^{\perp}} + P_{v_{1}}$. Now if $v \perp v_{1}$, we have
	\begin{eqnarray*}
		A v &=& \sum_{i = 1}^{m} c_{i} \langle v, u_{i}\rangle u_{i} \\
		&=& \sum_{i = 1}^{m} c_{i} \langle v, u_{i}' + \alpha_{i} v_{1}\rangle (u_{i}' + \alpha_{i} v_{1}) \\
		&=& \sum_{i = 1}^{m} c_{i} \langle v, u_{i}'\rangle u_{i}' +  v_{1} \sum_{i = 1}^{m} c_{i} \langle v, u_{i}'\rangle \alpha_{i}.
	\end{eqnarray*}
	Now since $u_{i}' \perp v_{1}$, the latter sum vanishes, and
	\[
		A' = \sum_{i = 1}^{m} c_{i} P_{u_{i}'}.
	\]
	This representation meets our requirements, so we can write $A' = \sum_{i = 2}^{n} \lambda_{i} P_{v_{i}}$, for some $\lambda_{2}, \ldots, \lambda_{n} \in \R$ and $v_{2}, \ldots, v_{n} \in v_{1}^{\perp}$, and adjoining $v_{1}$ and $\lambda_{1}$, we get the required representation.
\end{proof}

Representation \ref{spectralrepr}, which we will call \textit{spectral representation} is by no means unique. If $A = I$ for instance, we could choose the vectors $v_{i}$ rather arbitrarily. It also should be made clear why such representation is useful. First of all, calculating with spectral representation is easy. If $A = \sum_{i = 1} \lambda_{i} P_{v_{i}}$,
\[
	A^{2} = \left(\sum_{i = 1} \lambda_{i} P_{v_{i}}\right) \left(\sum_{j = 1} \lambda_{j} P_{v_{j}}\right) = \sum_{i = 1}^{n} \sum_{j = 1}^{n}\lambda_{i} \lambda_{j} P_{v_{i}} P_{v_{j}} = \sum_{i = 1}^{n} \lambda_{i}^2 P_{v_{i}},
\]
since $P_{v}P_{w}$ for any orthogonal $v, w \in V$. There is an obvious generalization for higher powers, and polynomials too. Similar identities work for more general diagonalizable maps, but now we have additionally projection representation. Spectral representation has however many other nice properties not shared with the more general diagonalizable maps: operator norm of map is determined by its eigenvalues only. More precisely $\|A\| = \max_{i = 1}^{n} |\lambda_{i}|$. We will come back to this in a minute.

It should be noted that we only used one special property of ($\R$ linear combination of) projections $A$ in the proof:
\[
	\langle A v, w \rangle = \overline{\langle A w, v \rangle} = \langle v, A w \rangle,
\]
for any $v, w \in V$. In the first step we didn't use any properties of $A$. In the step 3 this is exactly what we do in the manipulation. Also in the step 2 the idea is to show that
\[
	\lambda_{1} \langle v_{1}, v_{1} \rangle = \langle A v_{1}, v_{1} \rangle \in \R,
\]
but $\langle A v_{1}, v_{1} \rangle = \overline{\langle A v_{1}, v_{1} \rangle}$ so $\langle A v_{1}, v_{1} \rangle \in \R$. Of course here we didn't need the fact that $v_{1}$ is an eigenvector in any way. Step 4 is just simpler: once we have constructed $A'$, it will obviously have the same property as $A$.

In all of the above it doesn't make much difference whether we have $\R$ or $\C$ as the ground field, since the eigenvalues will be anyway real. Especially when one is working over $\R$, one might whether there is a more straightforward way to get through step $1$: get rid of the complex numbers. There is. Another way to find the eigenvector and eigenvalue is to look at the so called \textit{Rayleigh quotient}
\[
	R(A, v) = \frac{\langle A v, v \rangle}{\langle v, v \rangle},
\]
when $v \in V \setminus \{0\}$. This is scale-invariant bounded real quantity so it attains maximum somewhere outside zero, say at $v$. We claim that $v$ is an eigenvector of $A$. Looking at $v_{t} = v + t w$ and differentiating at zero yields
\begin{eqnarray*}
	0 = \frac{d}{dt} R(A, v_{t})\Big|_{t = 0} &=& \frac{(\langle A v, w \rangle + \langle A w, v \rangle) \langle v, v \rangle - (\langle v, w \rangle + \langle w, v \rangle) \langle Av, v \rangle}{\langle v, v \rangle^2} \\
	&=& \frac{\Re\left(\langle A v, w \rangle \langle v, v \rangle -  \langle v, w \rangle \langle Av, v \rangle \right)}{\langle v, v \rangle^2}.
\end{eqnarray*}
We write $A v = \lambda v + v'$ where $v \perp v'$ and set $w = v'$. Now
\begin{eqnarray*}
	0 &=& \frac{\Re\left(\langle \lambda v + v', v' \rangle \langle v, v \rangle -  \langle v, v' \rangle \langle \lambda v + v', v \rangle \right)}{\langle v, v \rangle^2} \\
	&=& \frac{\Re\langle v', v' \rangle \langle v, v \rangle}{\langle v, v \rangle^2},
\end{eqnarray*}
so $v' = 0$ and $v$ is an eigenvector.

\section{Self-adjoint maps}

\begin{maar}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an finite-dimensional inner-product space. \textit{Adjoint} of a linear map $A : V \to V$ is a linear map $A^{*} : V \to V$ such that
	\[
		\langle A v, w \rangle = \langle v, A^{*} w \rangle
	\]
	for any $v, w \in V$.
\end{maar}

\begin{maar}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an finite-dimensional inner-product space. A linear map $A : V \to V$ is \textit{self-adjoint} if it is its own adjoint, i.e.
	\[
		\langle A v, w \rangle = \langle v, A w \rangle
	\]
	for any $v, w \in V$.
\end{maar}

The spectral theorem shows that Hermitian maps correspond exactly to the self-adjoint maps. It is the good property of projections that made the Spectral theorem work: we got to move mappings from one side of inner-product to another. Generally the mapping changes, and adjoint is what comes out. We gather here many of the important properties of adjoint.

\begin{lause}\label{basic_adjoint}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an $n$-dimensional inner-product space, $A, B \in \L(V)$ linear, and $\lambda \in \C$. Then
	\begin{enumerate}[i)]
		\item $A$ has an unique adjoint
		\item Matrix of $A^{*}$ with respect to any orthonormal basis is conjugate transpose of matrix of $A$, i.e. $A^{*}_{i, j} = \overline{A_{j, i}}$.
		\item $(A^{*})^{*} = A$
		\item $(A + B)^{*} = A^{*} + B^{*}$
		\item $(\lambda I)^{*} = \overline{\lambda} I$
		\item $(AB)^{*} = B^{*}A^{*}$.
		\item $\kernel(A) = \image(A)^{\perp}$
	\end{enumerate}
\end{lause}
\begin{proof}
	\begin{enumerate}[i)]
		\item Fix any orthonormal basis of $V$, $(e_{i})_{i = 1}^{n}$. Now for any $v, w \in V$
		\begin{eqnarray*}
			\langle A v, w \rangle &=& \langle A \sum_{i = 1}^{n} \langle v, e_{i} \rangle e_{i}, w\rangle\\
			&=& \sum_{i = 1}^{n} \langle A e_{i}, w \rangle \langle v, e_{i} \rangle \\
			&=& \langle v, \sum_{i = 1}^{n} \langle w, A e_{i} \rangle e_{i} \rangle,
		\end{eqnarray*}
		so we should set $A^{*} w = \sum_{i = 1}^{n} \langle w, A e_{i} \rangle e_{i}$. This clearly defines an linear mapping $A^{*}$. Also, if $A$ has two adjoints, their difference $C$ is linear map such that $\langle v, C w \rangle = 0$ for any $v, w \in V$. Setting $v = C w$ we get that $C w = 0$ for any $w \in V$ so the adjoints are equal.
		\item Observe that $A^{*}_{i, j} = \langle A^{*}e_{j}, e_{i} \rangle = \overline{\langle e_{i}, A^{*} e_{j} \rangle} = \overline{ \langle A e_{i}, e_{j} \rangle} = \overline{A_{j, i}}$.
		\item We have for any $v, w \in V$ that $\langle A^{*} v, w \rangle = \overline{\langle w, A^{*} v \rangle} = \overline{\langle A w, v \rangle} = \langle v, A w \rangle$, so $A$ is adjoint of $A^{*}$.
		\item Since $\langle (A + B) v, w \rangle = \langle A v, w \rangle + \langle B v, w \rangle = \langle v, A^{*} w \rangle + \langle v, B^{*} w \rangle = \langle v, (A^{*} + B^{*})w \rangle$, $A^{*} + B^{*}$ is adjoint of $A + B$.
		\item We simply calculate that $\langle \lambda v, w \rangle = \lambda \langle v, w \rangle = \langle v, \overline{\lambda} v \rangle$.
		\item Note that $\langle A B v, w \rangle = \langle B v, A^{*}w \rangle = \langle v, B^{*} A^{*} w \rangle$.
		\item We have $v \in \kernel(A)$ i.e. $A v = 0$ if and only $\langle A v, w \rangle = \langle v, A w \rangle$ for any $w \in V$, which is to say that $v \perp \image(A)$.
	\end{enumerate}
\end{proof}

\section{Operator norm}

As we noticed, if $A \in \L(V)$ is self-adjoint $\langle A v, v \rangle \in \R$ for any $v \in V$. This motivates us to define the quadratic form of $A$, a map $Q_{A} : V \to \R$, by setting $Q_{A}(v) = \langle A v, v \rangle$. Similarly, if $Q_{A}$ is real, $A$ is self-adjoint. Indeed, looking at $Q_{A}(v + t w) = Q_{A}(v) + |t|^2 Q_{A}(w) + \overline{t} \langle A v, w \rangle + t \langle A w, v \rangle$ we see that $\overline{t} \langle A v, w \rangle + t \langle A w, v \rangle \in \R$ for any $v, w \in V$ and $t \in \C$. Still $\overline{t} \langle A v, w \rangle + t \langle A w, v \rangle = \overline{t} \langle A v, w \rangle + t \overline{\langle A v, w \rangle} - t \langle w, A v \rangle   + t \langle A w, v \rangle = \Re(\overline{t} \langle A v, w \rangle) + t (\langle A w, v \rangle  - \langle w, A v \rangle) $ so we must have $\langle A w, v \rangle  = \langle w, A v \rangle$.

Rayleigh quotient is sort of a scale-invariant version of the quadratic form, and that's why it captures many of the properties of self-adjoint maps. It is also a good way to think about the operator norm of a linear map. Take any $\H(V) \ni A = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}}$ and also any $V \setminus \{0\} \ni x = \sum_{i = 1}^{n} x_{i} v_{i}$. Now
\[
	R(A, x) = \frac{\langle A x, x \rangle}{\langle x, x \rangle} = \frac{\sum_{i = 1}^{n} \lambda_{i} |x_{i}|^2}{\sum_{i = 1}^{n} |x_{i}|^2},
\]
the Rayleigh quotient is a weighted average of the eigenvalues of $A$. Now if $A$ is any linear map, and $x \in V \setminus \{0\}$
\[
	\|A x\|^{2} = \langle A x, A x \rangle = \langle A^{*} A x, x \rangle = R(A^{*} A, x) \|x\|^{2}
\]
Now $A^{*} A$ is self-adjoint. Also its eigenvalues must be non-negative, since the Rayleigh quotient is: square roots of these values are called the \textit{singular values} of $A$. These considerations make it also clear that the largest singular value of $A$ is the operator norm of $A$. If $A$ is self-adjoint itself, singular values are simply the absolute values of the eigenvalues, and the operator norm is the maximum of these absolute values, as claimed before. More generally, if $A$ is normal, i.e. we can write $A = \sum_{i = 1}^{n}\lambda_{i} P_{v_{i}}$ with $\lambda_{i} \in \C$ and $v_{i}$'s orthonormal, we have
\[
	A^{*} A = \left(\sum_{i = 1}^{n} \overline{\lambda_{i}} P_{v_{i}}\right) \left( \sum_{j = 1}^{n} \lambda_{j} P_{v_{j}} \right) = \sum_{i = 1}^{n} |\lambda_{i}|^2 P_{v_{i}},
\]
so again singular values are the absulute values of the eigenvalues and $\|A\| = \max_{i = 1}^{n} |\lambda_{i}|$. Rayleigh quotient also has interpratation for normal maps, it is again weighted average of the eigenvalues. When $x$ ranges over non-zero vectors, or equivalently over some sphere, $R(A, x)$ ranges over the convex full of the eigenvalues.

Many of the previous properties don't hold for general linear maps. Also, singular values are absolute values of eigenvalues exactly for the normal maps.


\section{Intuition}

One way to understand adjoints is to look at a bit more general case. Fix any two finite-dimensional inner-product spaces, $(V, \langle \cdot, \cdot \rangle_{V})$ and $(W, \langle \cdot, \cdot \rangle_{W})$, not necessarily of the same dimension, over, say, $\C$.
Now given any linear $A \in \L(V, W)$, adjoint is a linear map $A^{*} \in \L(W, V)$ such that for any $v \in V$ and $w \in W$ we have
\[
	\langle A v, w \rangle_{W} = \langle v, A^{*} w \rangle_{V}.
\]
Again, adjoint exists and it's unique. Fix any $w \in W$. Vector $w$ induces a linear mapping $W \to \C$ by $w' \mapsto \langle w', w \rangle_{W}$. Hence we get a map $\Phi_{W} : W \to W^{*}$ where $V^{*}$ is the dual of $V$. It turns out that $\Phi_{W}$ is anti-linear bijection. Anti-linearity and injectivity are easy to check, and since both spaces have same dimension, map has to be also surjection.

Composing with $A$, $\Phi_{W}$ induces a map $W \to V^{*}$ given by $w \mapsto \Phi_{W}(w) \circ A$. Now we finally compose this with $\Phi^{-1}_{V}$ to get a map $W \to V$, the adjoint. So $A^{*}(w) = \Phi^{-1}_{V}(\Phi_{W}(w) \circ A)$.

We can still clean the definition a bit. Any linear map $A \in \L(V, W)$ induces a mapping between the correponding duals (with the order reversed) ${}^{t}A : W^{*} \to V^{*}$, given by $({}^{t}A)(\phi) = \phi \circ A$ for any $\phi \in W^{*}$. With this interpretation in mind we rewrite $w \mapsto \Phi_{W}(w) \circ A = {}^{t}A \circ \Phi_{W}$, and finally
\[
	A^{*} = \Phi^{-1}_{V} \circ {}^{t} A \circ \Phi_{W}.
\]
So the point is: from map $A \in \L(V, W)$ we (canonically) get a map ${}^{t}A : W^{*} \to V^{*}$ and since we can, thanks to the inner product, identify inner-product spaces with their duals, this gives us a map $A^{*} \in \L(W,V)$.

It should be noted that all parts of the theorem \ref{basic_adjoint} carry directly to this more general setup with obvious modifications.

\subsection{Adjoint vectors}
Map $\Phi_{V}$ itself can be interpreted as a kind of an adjoint. For any $v \in V$ we have canonical map, which we also denote by $v : \C /to V$, given by $v(t) = t v$. Now $\Phi_{V}(v)$ is exactly the adjoint of this map. This allows us to write $v^{*} = \Phi_{V}(v)$, that is $v^{*}w = \langle w, v \rangle$ for any $w, v \in V$, and call $v^{*}$ the \textit{adjoint vector} of $v$, or adjoint of $v$.

One could also start with this notion of adjoint vector and use it to define the whole adjoint. Take any $A \in \L(V, W)$. Now the adjoint of $A$ is the map $A^{*} \in \L(W, V)$ such that for any $v \in V$ we have
\[
	(A v)^{*} = v^{*} \circ A^{*}.
\]

TODO?

\subsection{Linear maps vs sesquilinear forms}

Inner products give also other canonical isomorphism: one between linear maps and sesquilinear forms. Sesquilinear forms are complex generalizations of bilinear forms. Sesquilinear form $B : V \times W \to \C$ is a mapping linear in its first entry and anti-linear in the second. The vector space of all sesquilinear forms on $V \times W$ is denoted by $\sesqui(V, W)$. Any linear mapping $A \in \L(V, W)$ gives rise to a sesquilinear form given by $\langle A \cdot, \cdot \rangle_{W}$. This gives us a map $\Psi_{V, W} : \L(V, W) \to \sesqui(V, W)$, and we also denote $\Psi_{V, W} A = B_{A}$. This map is an isomorphism: it can be easily seen that is linear, and injection, and since both spaces are of dimension $\dim(V) \cdot \dim(W)$, the map is also an surjection.


We also have a natural map $(\cdot)^{H} : \sesqui(V, W) \to \sesqui(W, V)$, \textit{conjugate transpose} given by $B^{H}(w, v) = \overline{B(v, w)}$: we simply exchange the arguments, and conjugate to preserve the sesquilinearity. Now it's not hard to check that $(\cdot)^{*} = \Psi^{-1}_{W, V} \circ (\cdot)^{H} \circ \Psi_{V, W}$, so we get an another route to adjoint.

Self-adjoint maps of course only make sense when $V = W$. When $A \in \L(V)$ is a self-adjoint, $B_{A}^{H} = B_{A}$: such sesquilinear forms are called Hermitian. The diagonal of $B$ is the of course just the quadratic form, i.e. $B(v, v) = Q_{A}(v)$ for any $v \in V$.

Adjoint also behaves in many ways like (complex) conjugate.
\begin{itemize}
	\item Notions agree in $1$-dimensional spaces.
	\item Taking adjoint is an anti-linear involution.
	\item Eigenvalues of the adjoint are conjugates of the eigenvalues of the original map.
	\item If $A \in \L(V)$ has an orthonormal eigenbasis, $A^{*}$ has the same eigenbasis with conjugated eigenvalues.
	\item Self-adjoint maps correspond to real numbers: they have real eigenvalues.
\end{itemize}

Maps for which the fourth point holds are called \textit{normal}. The class of normal maps on $V$ is denoted by $\normal(V)$

One of the very good properties of conjugation is that it commutes with many operations, taking inverse, for instance. The same carries to adjoints:
\[
	I = (A^{-1}A)^{*} = A^{*} (A^{-1})^{*},
\]
so $(A^{-1})^{*} = (A^{*})^{-1}$. There's a natural generalization we'll come back to later.

TODO: projections

\section{Commuting self-adjoint maps}

\textbf{Warning!} Composition of self-adjoint maps need not be self-adjoint!

It's rather surprising that they are closed under sums in the first place. Of course both Hermitian and self-adjoint maps are by definition, but it's rather surprising that such class has such nice properties (most of which are left to discuss).

The problem is that even if $A, B \in \H(V)$, $(AB)^{*} = B^{*} A^{*} = B A$, which is in general different from $A B$: two self-adjoint maps need not commute. Again, one should have a counterexample and almost anything works for that. We need to have $n > 1$. Also we should be able to find counterexamples that are projetions, since if any two projections commute, so do maps in their span, Hermitian maps. Finally we should not take parallel or orthogonal vectors: projections are idemponent and if one applies two orthogonal projections, result if the zero map. Actually anything else works as a counterexample. Indeed, when $v$ and $w$ are not parallel or orthogonal. Now $P_{w} P_{v} v = P_{w} v =$ something non-zero parallel to $w$, while $P_{v} P_{w} v$ is something non-zero parallel to $v$ or at least not parallel to $w$. Hence $P_{v}$ and $P_{w}$ do not commute.

From the previous we see that projections commute exactly when the corresponding vectors are parallel or orthogonal. More generally one would like to determine exactly which products of self-adjoint maps are self-adjoint, i.e. which products commute. Large family of examples is given by pairs of self-adjoint maps with the same eigenvectors. Namely if $A = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}}$ and $B = \sum_{i = 1}^{n} \lambda'_{i} P_{v_{i}}$,
\[
	A B = \left(\sum_{i = 1}^{n} \lambda_{i} P_{v_{i}} \right) \left(\sum_{j = 1}^{n} \lambda'_{j} P_{v_{j}}\right) = \sum_{i = 1}^{n} \sum_{j = 1}^{n}\left(\lambda_{i} P_{v_{i}}\right)\left(\lambda'_{j} P_{v_{j}}\right) = \sum_{i = 1}^{n}\lambda_{i}\lambda'_{i} P_{v_{i}} = BA.
\]

In this case eigenvalues of the product are also products of the eigenvalus of $A$ and $B$, but not necessarily all of them. It turns out that that's all. Before going through the proof, let's get back to the spectral theorem. One of the unfortunate aspects of it is that the spectral representation is not unique. We can however make it unique. Essentially only choices we get to make in the representation are the choices for the orthonormal bases of the eigenspaces of the map, so if we replace the projections to vectors with projections to the eigenspaces, we have the uniqueness.

Recall that for arbitrary subspace of $V$, say $T$, (orthogonal) projection to $T$, denoted by $P_{T}$ is the unique linear map such that $P_{T}^2 = P_{T}$, $\image(P_{T}) = T$, $\kernel(P_{T}) = \image(P_{T})^{\perp}$. The first condition is the projection part, the second projetion being to $T$ and the third is the orthogonality. Orthogonality can be also replaced by requiring $P_{T}$ to be self-adjoint. Equivalently for any $v \in V$ $P_{T}v$ is the unique vector in $T$, such that $v - P_{T} v \perp T$. Or still equivalently, $P_{T} v$ is the unique vector in $T$ closest to $v$. Equivalence of these definitions is easy to check, and also the fact that projections to vectors used before are just projections to corresponding subspaces. Finally, if $e_{1}, e_{2}, \ldots, e_{m}$ is an orthonormal basis for an subspace $T$, $P_{T} = \sum_{i = 1}^{m} P_{e_{i}}$.

\begin{lause}[Spectral theorem]
	Let $V$ be an $n$-dimensional inner-product space and $A \in \H(V)$. Then there exists unique non-negative integer $m$, real numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}$ and non-trivial orthonormal subspace of $V$, $E_{\lambda_{1}}, E_{\lambda_{2}}, \ldots E_{\lambda_{m}}$ with $E_{\lambda_{1}} + E_{\lambda_{2}} + \ldots + E_{\lambda_{m}} = V$, such that
	\[
		A = \sum_{i = 1}^{m} \lambda_{i} P_{E_{\lambda_{i}}}.
	\]
	Moreover, this representation is unique.
\end{lause}

Here $m$ is the number of distinct eigenvalues of $A$, $\lambda_{i}$'s are the eigenvalues, and $E_{\lambda_{i}}$'s are the corresponding eigenspaces: representation is necessarily unique. Existence of such representation is immediate from the previous version of the spectral theorem.

We can also reinterpret the original proof a bit further. One of the key facts we used was that if $v$ is an eigenvector, $v^{\perp}$ is mapped to itself. This fact has rather natural generalization: if $A \in \H(V)$ and $T$ is a subspace of $V$ which maps to itself, so does its orthocomplement. Now we can decompose $A$ to $A_{T}$ and $A_{T^{\perp}}$. Put a bit differently: every self-adjoint map is canonically a product of real multiplication maps.

\begin{lause}
	Let $V$ be an $n$-dimensional inner-product space and $A_{j} \in \H(V)$ for $j \in J$ be an arbitrary family of (pairwise) commuting self-adjoint maps on $V$. Then there exists on-negative integer $m$, non-trivial orthogonal subspaces $T_{1}, T_{2}, \ldots, T_{m}$ of $V$ with $T_{1} + T_{2} + \ldots + T_{m} = V$, and sequences of real numbers $(\lambda_{i, j})_{i = 1}^{m}$ for $j \in J$ such that for any $j \in J$ we have
	\[
		A_{j} = \sum_{i = 1}^{m} \lambda_{i, j} P_{T_{i}}
	\]
	and if $i \neq i'$, for some $j \in J$ we have $\lambda_{i, j} \neq \lambda_{i', j}$. Moreover, this representation is unique.
\end{lause}

When such a representation exists, we call the family $\{ A_{j} | j \in J \}$ \textit{simultaneously diagonalizable}. We again see that the $\lambda_{i, j}$'s are eigenvalues of the corresponding $A_{j}$'s. The last requirement on the subspaces and $\lambda_{i, j}$'s is just technical minimality condition to ensure uniqueness. If we have any representation ignoring this constraint, we can just combine the subspaces if all the eigenvalues are equal.

\begin{proof}

	Let us first check the uniqueness. It is clear that if two representations share subspaces, also the corresponding $\lambda_{i, j}$'s are equal. So we may assume to the contrary that there are two such representations with subspaces $T_{1}, T_{2}, \ldots, T_{m}$ and $T'_{1}, T'_{2}, \ldots, T'_{m'}$ and $T_{1} \not\subset T_{1}'$, but $T_{1}$ and $T_{1}'$ are not orthogonal. Take $v \in T_{1}' \setminus (T_{1} \cup T_{1}^{\perp})$ and write it in the form $v_{1} + v_{1}'$ where $v_{1} \in T_{1}$ and $v_{1}' \in T_{1}^{\perp}$. For any $A_{j}$ both $v$ and $v_{1}'$ are eigenvalues not orthogonal, so they correspond to the same eigenvalue. Hence $v_{1}'$ is also a eigenvector for any $A_{j}$ corresponding to the same eigenvalue. Now $v_{1}'$ can't be orthogonal to everything in $T_{1}'$ so it is not orthogonal to some vector $v_{2}$ in $T_{2}$, say. As before we see that for any $A_{j}$ the eigenvalue corresponding to $T_{1}$ is same as the eigenvalue corresponding to $v_{2}$, and so $T_{2}$. This means that the first representation is not minimal, we could have combined the $T_{1}$ and $T_{2}$, a contradiction.
 
	We proceed by induction on $n$, the dimension of the space. The case $n = 0$ is trivial. When $n > 0$ if we look at the spectral representations of the maps $A_{j}$. If all of them simple representation ($m = 1$), i.e. they are multiples of identity map, we can also choose $m = 1$, let $\lambda_{1, j}$'s be the corresponding unique eigenvalue of $A_{j}$ and let $T_{1} = V$.

	Assume then that some $A_{j}$ has non-trivial decomposition and let $T$ be an subspace in the decomposition, eigenspace of $A_{j}$, with eigenvalue $\lambda$. We know that $A_{j}$ itself can be decomposed into maps living in $T$ and $T^{\perp}$: we shall prove that same is true about all the other maps $A_{j'}$.

	Take any $j' \in J$. We want to prove that $A_{j'} T \subset T$ Since $A_{j'}$ and $A_{j}$, for any $v \in T$ we have
	\[
		\lambda A_{j'} v = A_{j'} A_{j} v = A_{j} A_{j'} v.
	\]
	If $A_{j'} v = 0$, everything's fine. If not, $A_{j'}v$ is an eigenvector of $A_{j}$ with eigenvalue $\lambda$: it lives by definition in $T$. It follows that also $A_{j'} T^{\perp} \subset T^{\perp}$, and $A_{j'}$ factorizes to maps in $T$ and $T^{\perp}$.

	Once we perform this factorization for all the maps, we have reduced our problem to smaller case: $T$ was a proper non-trivial subspace of $V$. It follows by induction that we can find factorizations as in the statement of the theorem for both sides, and we simply put them together to get final representation. This representation is also minimal, as is rather is easy to check, put even if it wasn't, we could cook up such as explained before the proof.
\end{proof}

From the proof we also see that any representation ignoring the minimality condition is just a subdivision of a minimal one: the subspaces are further divided to subspaces.

Previous theorem also sheds some light to the normal maps. Take any normal map $A = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}}$. When the eigenvalues are split to real and imaginary parts $\lambda_{i} = a_{i} + i b_{i}$, we can write $A = \sum_{i = 1}^{n}a_{i} P_{v_{i}} + i \sum_{i = 1}^{n}b_{i} P_{v_{i}}$, where now $\sum_{i = 1}^{n}a_{i} P_{v_{i}}$ and $\sum_{i = 1}^{n}b_{i} P_{v_{i}}$ are self-adjoint: we have decomposed normal map itself to its real and imaginary part. This construction can be done in general too.  We will denote $\Re(A) = \frac{1}{2}(A + A^{*})$ and $\Im(A) = \frac{1}{2 i}(A - A^{*})$. For general linear map $A$, we can write $A = \Re(A) + i \Im(A)$. While again both $\Re(A)$ and $\Im(A) = \Re(A/i)$ are self-adjoint, $i \Im(A)$ is ``purely imaginary": such maps are called \textit{skew-Hermitian}: they are maps satisfying $A^{*} = -A$. We see that if $A$ is normal, the eigenvalues of $\Re(A)$ and $\Im(A)$ are exactly the real and imaginary parts of the eigenvalues of $A$, respectively.

For any linear map $A$ we can definitely find spectral representation for its real and imaginary parts. If they are simultaneously diagonalizable, they give rise to the spectral representation of the original map. By the previous theorem this is equivalent to real and imaginary parts commuting, which by short computation is equivalent to $A^{*}A = A A^{*}$: $A$ commutes with its adjoint. It follows that if $A$ commutes with its adjoint, $A$ is normal. Also if $A$ is normal, $A$ definitely commutes with its adjoint, so these conditions are equivalent. The previous is actually a very common definition of normal map.

\section{Unitary maps}

Besides self-adjoint maps, there is other very nice subfamily of normal maps: unitary maps. Unitary maps are normal maps with eigenvalues on unit circle. Such maps satisfy
\[
	A^{*} A = \sum_{i = 1}^{n} \overline{\lambda_{i}} P_{v_{i}} \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}} = \sum_{i = 1}^{n} |\lambda_{i}|^{2} P_{v_{i}} = I,
\]
On the other hand, if $A \in \L(V)$ with $A^{*}A = I$, then $A^{*}$ is inverse of $A^{-1}$, and $A^{*}$ commutes with $A$: $A$ is normal. By previous computation we see that all the eigenvalues of $A$ are on unit circle, hence map is unitary, if and only if $A^{*} A = I$. Finally if $A$ is unitary, for any $v, w \in V$ we have
\[
	\langle A v, A w \rangle = \langle v, A^{*}A w \rangle = \langle v, w \rangle,
\]
so $A$ preserves innerproduct. Again, if $A$ preserves innerproducts, for any $v, w \in V$ we have
\[
	\langle v, w \rangle = \langle A v, A w \rangle = \langle v, A^{*} A w \rangle,
\]
so $A^{*}A = I$: we have third equivalent definition for unitary maps. This definition might be also most enlightening: unitary maps are exactly the automorphisms of the inner-product space $V$.

There is also fourth equivalent definition for unitary maps: they are the isometries of $V$. It's clear that any unitary map is isometry, but the other direction might not be entirely obvious. The innerproduct can however be recovered from the norm. If $V$ is over $\R$, we have
\[
\langle v, w \rangle = \frac{1}{4} \left(\|v + w\|^{2} - \|v - w\|^{2} \right).
\]
It follows that if $A$ is an isometry, for any $v, w \in V$ we have
\[
\langle Av, Aw \rangle = \frac{1}{4} \left(\|Av + Aw\|^{2} - \|Av - Aw\|^{2} \right) = \frac{1}{2} \left(\|v + w\|^{2} - \|v - w\|^{2} \right) = \langle v, w \rangle.
\]
If $V$ is over $\C$ instead, we have to modify our polarization identity a little:
\[
\langle v, w \rangle = \frac{1}{4} \left(\|v + w\|^{2} - \|v - w\|^{2} + i \|v + i w \|^{2} - i \|v - i w\|^{2}\right).
\]

Both the third and the fourth definitions make it clear that the unitary maps on $V$ form a group. This group is often denoted by $\text{Hilb}(V)$, but we will use $\unitary(V)$, for brevity, and because $\H$ is preserved.

TODO: sesquilinear forms vs. quadratic form correspondence.

TODO: Unitary maps and matrices

TODO: Cayley transform

\section{Positive maps}

\begin{maar}
	Hermitian map is said to be \textit{positive} if all of its eigenvalues are non-negative.
\end{maar}

Such maps are usually called \textit{positive semi-definite}: the term \textit{positive definite} is usually preserved for the following class.

\begin{maar}
	Hermitian map is said to be \textit{strictly positive} if all of its eigenvalues are positive.
\end{maar}

``$A$ is positive" is denoted by $A \geq 0$, and ``$A$ is stricly positive" by $A > 0$. One could adopt analogous notation for (strictly) negative maps, definition of which should be clear. The set of positive maps is denoted by $\H_{+}(V)$, shortened to $\H_{+}$. Here is a big list of important facts and equivalent definitions for the previous.

\begin{itemize}
	\item A map is positive if and only if its quadratic form, or equivalently the Rayleigh quotient is non-negative. This is clear since we noticed the Rayleigh is just a weighted average of the eigenvalues, and quadratic form is just a positively scaled version of Rayleigh quotient. Similarly, a map is strictly positive if and only if the Rayleigh quotient is positive, of the quadratic form is positive, except at $0$. In both instances one could replace the non-negativity/positivity on everywhere/outside zero by requiring it only on unit sphere, for instance.
	\item Previous point makes it clear that sum of (strictly) positive maps is (stricly) positive. Analogous claim is true for positive scalar multiple.
	\item A map is strictly positive if and only if the respective sesquilinear form is inner-product. Inner-products can hence also be naturally called stricly positive.

	Positive map don't necessarily lead to inner-products, but to a so called semi-definite sesquilinear forms, which are here also called positive (sesquilinear) forms.
	\item Composition of positive maps need not be positive, but this is just because it need not be even Hermitian. If it is, however, by our previous results, the maps commute, are simultaneosly diagonalizable and they're composition is positive: its eigenvalues are (some of the) products of the eigenvalues of the original maps. Similar statement holds for striclty positive maps.
	\item The class of positive maps is topological closure (with operator norm topology) of the class of stricly positive maps. This is clear, and the closedness is very useful. Also, the class of stricly positive maps is the interior the class of positive maps. The reason is the following: if $A \geq 0$, $Q_{A} \geq \delta_{K} > 0$ for any compact set $K \subset V$ not containing $0$, so in particular on unit sphere. Now if $B \in \L(V)$ has operator norm $< \delta$, $Q_{A + B}(v) \geq Q(A)(v) - |Q_{B}(v)| > \delta - \delta$, so $Q$ is positive on unit circle, and thus outside zero. 
	\item Squares are positive, i.e. $A^{2} \geq 0$ for any Hermitian $A$. Also, any positive map is a square, and has an unique positive square root. These claims follows easily from the better version of the spectral theorem. Positive square root of $A \geq 0$ is denoted by $A^{\frac{1}{2}}$, as one would hope.
	%\item One can rewrite $Q_{A}(v) = \langle A v, v \rangle = \langle A^{\frac{1}{2}} A^{\frac{1}{2}} v, v \rangle = \langle A^{\frac{1}{2}} v, A^{\frac{1}{2}} v \rangle= \|A^{\frac{1}{2}} v\|^{2}$.
	\item Projections are positive: their eigenvalues belong to $\{0, 1\}$.
	\item We already noticed that maps of the form $A^{*}A$ are positive: their Rayleigh quotient is non-negative. Every positive map is also of this form (square root fits the bill), but not uniquely. 
	\item Strictly positive maps are central at the study of the local maxima and mimima of multivariate functions. Let $V$ be real inner-product space and $f : V \to \R$ be a twice continuously differentiable. Now me write $f(x + y) = f(x) + G_{x}(y)  + B_{x}(y, y) + o(\|y\|^{2})$, where $G_{x}$ is the derivative of $f$ at $x$, and $H_{x}$ is the hessian, the bilinear form correponding to the second derivative. Now if $G_{x} = 0$ and the Hessian $H_{x}$ is strictly positive, the function $f$ has a local minimum at $x$. If $H_{x}$ is merely positive, such conclusions can't be drawn. If, however, $H_{\cdot}$ is positive in a neighbourhood of $x$, $f$ has again a local minimum at $x$.

	\item One way to think about positive maps is that they don't turn vectors too much. If we are working in real inner product space, the angle between two non-zero vectors $v$ and $w$ is the unique real number $\alpha \in [0, \pi]$ such that
	\[
		\cos(\alpha) = \frac{\langle v, w\rangle}{\|v\| \|w\|}.
	\]
	The angle is less than $\frac{\pi}{2}$ if and only if $\cos(\alpha) > 0$, or equivalently if $\langle v, w \rangle > 0$. This means that a Hermitian map is positive if and only if the angle between $v$ and $A v$ is less than $\frac{\pi}{2}$: positive map doesn't turn a vector too much. This intuition can be somewhat pushed to complex spaces, but then the concept of an angle is harder to grasp.

	FIGURE: this intuition visualized in $\R^{2}$ or $\R^{3}$
\end{itemize}

\section{Congruence and Inertia}

\subsection{$*$-conjugation}

There is one very important way to produce positive maps from others, called congruence. Given any two positive maps $A$ and $B$, their composition need not be positive, but the map $BAB$ is. First of all, it is self-adjoint, as $(BAB)^{*} = B^{*} A^{*} B^{*} = BAB$. Also $Q_{BAB}(v) = \langle BAB v, v \rangle = \langle A (B v), (B v) \rangle \geq 0$ for any $v \in V$. We didn't really need the assumption on the positivity of $B$, but self-adjointness was not that important either. Namely for arbitrary linear $B$ we could consider the product $B^{*}AB$ instead: this is positive whenever $A$ is. If $C = B^{*}AB$ for some $B \in \L(V)$, we say that $C$ is $*$-conjugate of $A$.

We also see that $Q_{B^{*}AB} = Q_{A} \circ B$: conjugation is a change of basis in the quadratic form. Similar statement is evidently true for the respective sesquilinear form. This is the main motivation for the definition of the $*$-conjugation. We have already seen that the quadratic form of a map is a good way to characterize many of its good properties, so to some extent to undestand maps, we just to need to understand structure of their quadratic forms. By change of basis of the quadratic form we have a good control of what happens. We might however lose some information: if $B = 0$, for instance, the quadratic form after $*$-conjugation by $B$ doesn't tell much about $A$. But if $B$ is invertible, or equivalently if $C$ and $B$ are $*$-conjugates of each other, we shouldn't lose any information. If this is the case, we say that $A$ and $C$ are congruent. It is easily verified that congruence is a equivalence relation.

The construction of $*$-conjugation also sense for general linear map $A$, i.e. we could just as well $*$-conjugate non-positive, or even non-self-adjoint maps. The result then need not be positive or self-adjoint, and in general, $*$-conjugation loses its usefulness. Also, even if $A$ is normal, $*$-conjugate of $A$ need not be normal. TODO

The previous construction can be also performed between two spaces $V$ and $W$: given any map $B \in \L(V, W)$ and $A \in \H_{+}(W)/\H(W)/\L(W)$, we note that $B^{*}AB \in \H_{+}(V)/\H(W)/\L(W)$. For self-adjoint maps we can say a lot more: while congruence doesn't in general preserve eigenvalues, it preserves their signs.

\begin{lause}[Sylvester's Law of Inertia]
	$A, B \in \H(V)$ are congruent, if and only if $A$ and $B$ have equally many positive, negative and zero eigenvalues, counted with multiplicity.
\end{lause}
\begin{proof}
	Let's start with the ``if" part. Let's denote the eigenvalues of $A$ and $B$ by $\lambda_{1} \leq \lambda _{2} \leq \ldots \leq \lambda_{n}$ and $\lambda_{1}' \leq \lambda _{2}' \leq \ldots \leq \lambda_{n}'$, respectively, and the corresponding eigenvectors with $v_{1}, v_{2}, \ldots, v_{n}$ and $v_{1}', v_{2}', \ldots, v_{n}'$. By assumption $\lambda_{i}$ and $\lambda_{i}'$ have the same sign (or are both zero) for any $1 \leq i \leq n$, so we may find non-zero real numbers $t_{1}, t_{2}, \ldots, t_{n}$ such that $\lambda_{i} = \lambda_{i}' t_{i}^{2}$. Now consider a linear map $C$ with $C v_{i} = t_{i} v_{i}'$. $C$ is clearly a surjection and hence a bijection. Also if $v = \sum_{i = 1}^{n} x_{i} v_{i}$ $(Q_{B} \circ C)(v) = Q_{B}(\sum_{i = 1}^{n} x_{i} t_{i} v_{i}') = \sum_{i = 1}^{n} |x_{i}|^{2} t_{i}^2 \lambda_{i}' = \sum_{i = 1}^{n} |x_{i}|^{2} \lambda_{i} = Q_{A}(v)$ so $Q_{C^{*}BC} = Q_{B} \circ C = Q_{A}$. It follows that $C^{*}BC = A$ and hence $A$ and $B$ are congruent.

	The ``only if" - part is a bit trickier. The idea is to find a good description for the number of positive non-negative eigenvalues. We noticed before that we can write quadratic forms in the form $Q_{A}(v) = \sum_{i = 1}^{n} \lambda_{i} |x_{i}|^{2}$ if $v = \sum_{i = 1}^{n} x_{i}v_{i}$, and $v_{i}$ are the eigenvectos of $A$ with $\lambda_{i}'s$ as the corresponding eigenvectors. In particular if say first $k$ eigenvalues are negative, $Q_{A}$ will be negative on $\vspan\{v_{i} | 1 \leq i \leq k\}$, a $k$-dimensional subspace, minus zero. Similarly, now $n - k$ of the eigenvalues are non-negative, so the quadratic form is non-negative on a subspace of dimension of at least $n - k$. But the dimensions can't be any bigger: if $Q_{A}$ were for instance negative on some $k + 1$ dimesional subspace, this subspace would necessarily intersect a subspace where $Q_{A}$ is non-negative, which is non-sense.

	Congruence preserves the previous notion: if $Q_{B}$ is negative on a subspace of dimension $k$, so is $Q_{B} \circ C$ for any invertible $C$; namely in the inverse image. Same reasoning holds for the the subspace on which $Q_{B}$ is non-negative, so again, $Q_{B} \circ C$ has to have similar structure. We are done.
\end{proof}

If $n_{0}, n_{-}$ and $n_{+}$ denote the number of zero, negative and positive eigenvalues of $A$, \textit{inertia} of $A$ is the triplet $\{n_{0}, n_{-}, n_{+} \}$. The previous theorem can be hence restated, that inertia is invariant under congruence.

The proof also gives a useful characterization for the number of non-negative eigenvalues.

\begin{kor}
	If $A \in \H(V)$, number of non-negative eigenvalues of $A$ equals largest non-negative integer $k$ such that $Q_{A}$ is non-negative on subspace of $V$ of dimension $k$.
\end{kor}

TODO: explicit expression if $A$ and $B$ are positive definite.

It gets better: if two normal maps are congruent, we can vastly generalize the previous result: for normal maps, congruence preserves the number of eigenvalues on any open ray.

For any $A \in \normal(V)$ define the mapping $\theta[A] : S^{1} \to \N$ by setting $\theta[A](z)$ be the number of eigenvalues (counted with multiplicity) on ray $\{rz | r > 0\}$. This map is called the \textit{angularity} of $A$.

TODO: figure of change of eigenvalues on congruence.

\begin{lause}[Generalized Sylvester's Law of Inertia]
	$A, B \in \normal(V)$ are congruent, if and only if $\theta[A] = \theta[B]$.
\end{lause}
\begin{proof}
	The ``if"-part can be proven almost identically to proof of the self-adjoint case. For the ``only if" -part, note that if $A = C^{*}BC$, the also $\Re(A) = C^{*}\Re(B)C$. Now both sides are self-adjoint, and eigenvalues of $\Re(A)$ and $\Re(B)$ are just the real parts of the eigenvalues of $A$ and $B$. Applying the previous version Law of Inertia for these maps, we see that $A$ and $B$ have equally many eigenvalues on closed right half-plane. 

	Carrying out the previous reasoning for the maps $e^{i t}A$ and $e^{i t} B$ for real $t$ we see that these maps have equally many eigenvalues on any closed half plane containing origin. If one considers function sending $t$ to the number of eigenvalues of $e^{i t} A$ on closed right half-plane, we see that this function has jump discontinuitys, and the value, and the left and the right limits at these point reveals number of eigenvalues at any open ray.

	TODO: figure of functions $t \mapsto$ number of eigenvalues of $e^{it}A$ on closed right half-plane, or even better, map from $\S \to \N$.
\end{proof}

Sylvester's Law of inertia gives another proof of the fact that strictly positive maps are exactly the maps congruent to the identity, and positive maps are the maps congruent to some projection. More precisely, the positive maps are partitioned to $n + 1$ congruene classes depending on their rank, $k$:th congruence class containing the projections to $k$-dimensional subspaces. $0$:th class contains only the zero map, the only rank $0$ (positive) map, and the $n$:th class is the class of strictly positive maps.

If one $*$-conjugates with non-invertible, the angularity may change, but in quite obvious way only: some eigenvalues may move to $0$. In particular, we have the following supplement even a bit more general version of the law.

\begin{lause}[General Sylvester's Law of Inertia]
	For $A, B \in \normal(V)$ and $A$ is $*$-conjugate of $B$, if and only if $\theta[A] \leq \theta[B]$.
\end{lause}
\begin{proof}
	TODO
\end{proof}

This extension draws a picture about the relation of previously mentioned congcruence classes. We can move to the congruence classes of lower indeces by $*$-conjugation, but cannot move up the ladder: the complexity of quadratic forms cannot increase.

The important property of $*$-conjugates we have used repeatedly is that it commutes with taking adjoint (and hence with real and imaginary parts). This is not of course immediate from the definition but is also clear since taking adjoint corresponds to taking (complex) conjugate of quadratic form, and complex conjugation commutes with pretty much anything anyway.

\subsection{Unitary congruence}

There is a very important class of congruence called unitary congruence: congruence by unitary map. Since adjoint of an unitary map is just its inverse, $*$-conjugation by unitary map, or unitary conjugation is also conjugation in the usual sense. Unitary congruence hence also preserves eigenvalues.

TODO

\subsection{Sylvester's Criterion}
Congruence also leads to Sylvester's criterion.

\section{Fake products}

In the previous section we noticed that although the usual composition doesn't work nicely with respect to self-adjointness, $*$-conjugation does. It might be however tempting to try to come up with something a bit more similar to the standard products. One natural idea could be to consider a symmetrized product
\[
	\frac{A B + B A}{2}.
\]
If $A$ and $B$ commute, this coincides with usual product and if $A$ and $B$ are both positive, so is this symmetriced version. Without the assumption on the commutation everything breaks yet again.

TODO

\section{Absolute value and Polar decompositions}

We defined singular values of a linear map to be the eigenvalues of map $\left(A^{*}A\right)^{\frac{1}{2}}$. As we noticed, this map is Hermitian and even positive, and if $A$ is normal to begin with, its eigenvalues are simply absolute values of the eigenvalues of $A$. This motivates us to define
\[
	|A| := \left(A^{*}A\right)^{\frac{1}{2}},
\]
absolute value of a map.

The following list of properties of the absolute value make it clear that this is indeed good definition.

\begin{itemize}
	\item $|A| \geq 0$ for any $A \in \L(V)$ and $|A| = A$, if and only if $A \geq 0$.
	\item (Polar decomposition) If $A \in \L(V)$, we can write $A = U |A|$ and $A = |A| V$ for some $U, V \in \unitary(V)$. $U$ and $V$ coincide if and only $A$ is normal.
	\item For any $A \in \H(V)$ we have $-|A| \leq A \leq |A|$.
\end{itemize}

\section{Matrices and computation}

We have made whole bunch of definitions and observations concerning positive matrices, but just because we know how to define something doesn't mean we can calculate it. In some cases, calculations are just as easy the proofs, but in many cases one needs further ideas. What follows is a hopefully useful collection of information on how to really calculate things.

\begin{itemize}
	\item How to calculate adjoint of a map?
	\item How to calculate spectral composition of a map?
	\item How to calculate square root of a map?
	\item How to check if a map is positive?
	\item How to check if a map is strictly positive?
	\item If map $A$ is not positive, how to find a vector $v \in V$ such that $Q_{A}(v) < 0$.
	\item How to calculate a polar decomposition of a map?
	\item If $A$ and $B$ are $*$-adjoint, how to find $C$ such that $A = C^{*} B C$?
	\item How to calculate projection matrices?
\end{itemize}

\section{Positivity without inner-product}

\end{comment}

Ideas how to rewrite this section:
\begin{itemize}
	\item Map is positive, if all of its restrictions are. One dimensional maps are positive if and only the scalar is positive.
	\item Map is real, if all of its restrictions are. One dimensional maps are real if and only the scalar is real.
	\item Adjoint commutes with restriction. Adjoint of one dimensional map is its conjugate.
	\item How should one restrict linear mappings? (Use inclusions and finally project)
	\item Central notion is adjoint
	\item $*$-conjugation is natural notion with adjoint
	\item Increasing sequence of subspaces and restriction with positive determinant $\Rightarrow$ the whole map is positive. Equivalently, map is positive if it has positive determinant and has a subspace where it is positive.
\end{itemize}

\section{Motivation}

How should one order matrices? What should we require from ordering anyway?

We would definitely like to have natural total order on the space of matrices, but it turns out there are no natural choices for that. Partial order is the next best thing. Recall that a partial order on a set $X$ is a binary relation $\leq$ on such that
\begin{enumerate}
	\item $x \leq x$ for any $x \in X$.
	\item For any $x, y \in X$ for which $x \leq y$ and $y \leq x$, necessarily $x = y$.
	\item If for some $x, y, z \in X$ we have both $x \leq y$ and $y \leq z$, also $x \leq z$.
\end{enumerate}

The third point is the main point, the first two are just there preventing us from doing something crazy. But we can do better: this partial order on matrices should also respect addition.
\begin{enumerate}
\item[4.] For any $x, y, z \in X$ such that $x \leq y$, we should also have $x + z \leq y + z$.
\end{enumerate}

There's another way to think about this last point. Instead of specifying order among all the pairs, we just say which matrices are positive: matrix is positive if and only it's at least $0$.

If we know all the positive matrices, we know all the ``orderings". To figure out whether $x \leq y$, we just check whether $0 = x - x \leq y - x$, i.e. whether $y - x$ is positive. Also, positive matrices are just differences of the form $y - x$ where $x \leq y$. Now, conditions on the partial order are reflected to the set of positive matrices.
\begin{enumerate}
	\item[1'.] $0$ (zero matrix) is positive.
	\item[2'.] If both $x$ and $-x$ are positive, then $x = 0$.
	\item[3'.] If both $x$ and $y$ are positive, so is their sum $x + y$.
\end{enumerate}
Here $3'$ is kind of combination of $3$ and $4$.

The terminology here is rather unfortunate. Natural ordering of the reals satisfies all of the above with obvious interpretation of positive numbers, which however differs from the standard definition: $0$ is itself positive in our above definition. This is undoubtedly confusing, but what can you do? For real numbers we have total order, so every number is either zero, strictly positive or strictly negative, so when we say non-negative, it literally means ``not negative": we get all the positive numbers and zero. But with partial orders we might get more. So the main reasons why we are using this terminology are
\begin{enumerate}
	\item It's short.
\end{enumerate}
Also, now that we have decided to preserve the word ``positive" for ``at least zero" one might be tempted to preserve ``strictly positive" for ``at least zero, but not zero". We won't do that, we save that phrase for something more important.

To figure out a correct notion for positive maps, let's start simple. If we are in a $1$-dimensional vector space $V$ over $\R$ there's rather canonical choice for positivity. Any linear map is of the form $v \mapsto a v$ for some $a \in \R$ and we should obviously say that a map is positive if $a \geq 0$ (note our non-standard terminology concerning positivity). More generally, if a map if scalar multiple of identity, map should be positive if and only if the corresponding scalar is non-negative.

Natural extension of this idea could be try the following: map is positive if all of its eigenvalues are non-negative. Of course, this doesn't quite work: not every map has real eigenvalues. But even if we restrict to maps with real eigenvalues, this property is not preserved in addition. Consider for example the pair
\[
	\begin{bmatrix}
		0 & 4 \\
		1 & 0
	\end{bmatrix}
	\text{ and }
	\begin{bmatrix}
		0 & -1 \\
		-4 & 0
	\end{bmatrix}
\]
The two matrices have both distinct eigenvalues $- 2$ and $2$ and are hence diagonlizable, but their sum has characteristic polynomial $x^2 + 9$, which most definitely has no real zeros. In general one should not except summation and eigenvalues go very well together.

\section{Real maps}

\subsection{Restricting linear maps}

There's however quite clever way to go around this. Instead of requiring non-negativity of eigenvalues, we require that map ``restricts" to positive map. The idea is: we already know which maps should be positive in one-dimensional spaces, or more generally, which scalar multiples of identity should be positive. Now we should require that when we restrict our look to one-dimensional subspaces, we should get a positive map.

Of course, one should first understand what restricting linear maps means. Usually if we have a linear map $A : V \to V$, we could take subspace $W \subset V$ and consider the usual restriction map $\restr{A}{W} : W \to V$ given by $\restr{A}{W}(w) = Aw$ for any $w \in W \subset V$. In other words $\restr{A}{W} = A \circ \incl{V}{W}$, where $\incl{V}{W}$ denotes the natural inclusion from $W$ to $V$. But this map is going to wrong space. Instead we would like to define something satisfying
\begin{itemize}
	\item Restriction is a linear map $\arestr{(\cdot)}{V, W} = \arestr{(\cdot)}{W} : \L(V) \to \L(W)$.
	\item If $A \in \L(V)$ and $A(W) \subset W$, restriction should coincide with the original map, in the sense that $A = \incl{V}{W} \circ \arestr{A}{W}$.
	\item If $W' \subset W \subset V$, we should have $\arestr{(\cdot)}{W'} = \arestr{(\arestr{(\cdot)}{W})}{W'}$.
\end{itemize}

These properties don't uniquely define a linear map but they say that $\arestr{A}{W}$ should be of the form $P_{V, W} \circ A \circ \incl{V}{W}$ where $P_{V, W}$ is a projection, i.e. a map for which $P_{V, W} \circ \incl{V}{W} = I_{W}$. Moreover, these projections should satisfy $P_{V, W'} = P_{W, W'} \circ P_{V, W}$.

If we are working in a inner-product space, there's rather natural choice for the map $P_{V, W}$: orthogonal projections. Orthogonal projections are projections with $\ker(P) = \image(P)^{\perp}$. Such maps are easily seen to satisfy all the requirements. Finally, we will call our new concept \textit{compression} instead of restriction, to distinguish between the two.

\begin{maar}
	Let $(V, \langle \cdot, \cdot \rangle )$ be an inner product space and $W \subset V$ a subset. We define the map $\arestr{A}{W}$, \textit{compression} of $A$ to $W$ to be the linear map given by $P_{W} \circ A \circ \incl{V}{W}$.
\end{maar}

\begin{lause}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an inner product space and $W \subset V$ subspace of $V$. Then the compression to $W$ is unique linear contraction from $\L(V)$ to $\L(W)$, such that for any $A \in L(V, W)$ we have $\arestr{(\incl{V}{W} \circ A)}{W} = A \circ \incl{V}{W}$. Moreover, if $W' \subset W$, we have $\arestr{(\cdot)}{W'} = \arestr{(\arestr{(\cdot)}{W})}{W'}$.
\end{lause}
\begin{proof}
	TODO
\end{proof}

For one-dimensional compressions we have convenient representation. As one easily checks, one dimensional projection onto subspace spanned by vector $v$ is given by
\[
	P_{(v)} = \frac{\langle \cdot, v \rangle}{\langle v, v \rangle} v,
\]
as long as $v \neq 0$, and thus
\[
	\arestr{A}{(v)} = \frac{\langle A \cdot, v \rangle}{\langle v, v \rangle} v.
\]

If $w \in (v) \setminus \{0\}$, we could rewrite the previous in the form
\[
	\arestr{A}{(v)}(w) = \frac{\langle A v, v\rangle}{\langle v, v\rangle} w = \frac{\langle A w, w\rangle}{\langle w, w \rangle} w.
\]
This gives rise to so called Rayleigh quotient $R(A, \cdot) : V \setminus \{0\} \to \C$, given by
\[
	R(A, v) = \frac{\langle A v, v \rangle}{\langle v, v \rangle}.
\]
Compression in the direction of $v$ is given by scaling by the corresponding Rayleigh quotient.

We will call $\langle A v, v \rangle$ the quadratic form of $A$, and denote it by $Q_{A}(v)$.

There's one more important property of compression we need. When map is compressed to a subspace, we naturally lose some information about the map. Knowing about all of the compressions, however, we can get our map back.

\begin{lem}[Injectivity of compression]
	If $A, B \in \L(V)$, $A = B$ if and only if $\arestr{A}{W} = \arestr{B}{W}$ for any one-dimensional subspace $W \subset V$.
\end{lem} 
\begin{proof}
	By linearity, it is sufficient to prove that if $Q_{A}(v) = 0$ for any $v \in V$, then $A = 0$. TODO polarization identity.
\end{proof}

\subsection{Positive maps}

Now that we have defined the compression we are ready define positive maps.

\begin{maar}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an inner product space. We say that a map $A \in \L(V)$ is positive, and write $A \geq 0$, if for any one-dimensional subspace $W \subset V$ the map $\arestr{A}{W}$ is positive, i.e. is induced by a non-negative real.
\end{maar}

We denote the space of positive maps by $\H_{+}(V)$. Positive maps have the following useful properties.

\begin{prop}\label{basic_positive}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an inner product space over $\C$. Then
	\begin{enumerate}[(i)]
		\item $A \in \L(V)$ is positive if and only if $\arestr{A}{W}$ is positive for every subspace $W \subset V$.
		\item If $A, B \in \L(V)$ are positive and $\alpha, \beta \geq 0$, also $\alpha A + \beta B$ is positive.
		\item If $(A_{i})_{i = 1}^{\infty}$ are positive and $\lim_{i \to \infty} A_{i} = A$, also $A$ is positive.
		\item $A \in \L(V)$, $A$ is positive if and only for any $v \in V$, or for any $v \in V$ with $|v| = 1$ we have $\langle A v, v \rangle \geq 0$, or still equivalently, for any $v \in V \setminus \{0\}$ the Rayleigh quotient $R(A, v)$ is non-negative.
		\item If both $A$ and $-A$ are positive, then $A = 0$.
		\item If $A$ is positive, all of its eigenvalues are non-negative.
	\end{enumerate}
\end{prop}
\begin{proof}
	\begin{enumerate}[(i)]
		\item Other direction is immediate. Also if for any subspace $W \subset V$ take any one-dimensional $W' \subset W$. Now $\arestr{(\arestr{A}{W})}{W'} = \arestr{A}{W'}$, is positive by assumption, and so is $\arestr{A}{W}$.
		\item The claim evidently holds for one-dimensional spaces. Now for any one-dimesional $W \subset V$ we have $\arestr{(\alpha A + \beta B)}{W} = \alpha \arestr{A}{W} + \beta \arestr{B}{W} \geq 0$, by the one-dimensional case, so $\alpha A + \beta B \geq 0$.
		\item Again, the claim evidently holds for one-dimensional spaces. Now for any one-dimesional $W \subset V$ we have $\arestr{(\lim_{i \to \infty} A_{i})}{W} = \lim_{i \to \infty}\arestr{(A_{i})}{W}\geq 0$, by the one-dimensional case, so $A \geq 0$.
		\item These claims are immediate from our representation for one-dimensional compressions.
		\item If $A, -A \geq 0$, all the one-dimensional compressions of $A$ are both non-negative and non-positive, so zero. But by the injectivity of restriction, it follows that $A = 0$.
		\item Note that if $v$ is any eigenvector of $v$, $\arestr{A}{(v)} w = \frac{\langle A v, v\rangle}{\langle v, v\rangle} w = \lambda w$, so by assumption $\lambda \geq 0$.
	\end{enumerate}
\end{proof}

The map $v \mapsto \langle A v, v \rangle$ is called the quadratic form of $A$, and is denoted by $Q_{A}$.

We can also lift other important notions.

\begin{maar}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an inner product space. We say that a map $A \in \L(V)$ is real, if for any one dimensional subspace $W \subset V$ the map $\arestr{A}{W}$ is real, i.e. is induced by real number.
\end{maar}

\begin{maar}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an inner product space. We say that a map $A \in \L(V)$ is imaginary, if for any one dimensional subspace $W \subset V$ the map $\arestr{A}{W}$ is imaginary, i.e. is induced by imaginary number.
\end{maar}

The previous two families of maps are usually called Hermitian and Skew-Hermitian and as with positive maps, many of their properties are lifted form usual complex numbers. Hermitian maps will have a special role in our discussion. They form a vector space over $\R$, which is denoted by $\H(V)$. Of course, every imaginary map is just $i$ times real map, and we won't preserve any special notation for such maps.

\subsection{Adjoint}

We can also lift the notion of complex conjugate. If $V$ is one-dimensional, $\overline{A}$, conjugate of $A$ should be a linear map which is induced by the complex conjugate of the scalar inducing $A$.

\begin{lause}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an inner product space. Then for any $A \in \L(V)$ there exists unique map $A^{*} \in \L(V)$, which we will call adjoint of $A$, for which for any one-dimensional subspace $W$ we have $\arestr{(A^{*})}{W} = \overline{\arestr{A}{W}}$.
\end{lause}
\begin{proof}
	The uniqueness of adjoint is immediate from the injectivity of compression. The map $(\cdot)^{*} : \L(V) \to \L(V)$ should evidently be conjugate linear, so for existence it suffices to find adjoint for suitable basis elements of $\L(V)$: the maps of the form $A = (x \mapsto \langle x, v \rangle w)$ for $v, w \in V$ will do.

	Note that the Rayleigh quotient for such map is given by
	\[
		R(A, x) = \frac{\langle x, v \rangle \langle w, x \rangle}{\langle x, x \rangle}.
	\]
	But if we define $A^{*} = (x \mapsto \langle x, w \rangle v)$, we definitely have
	\[
		R(A^{*}, x) = \frac{\langle x, w \rangle \langle v, x \rangle}{\langle x, x \rangle} = \overline{\frac{\langle w, x \rangle \langle x, w \rangle}{\langle x, x \rangle}} = \overline{R(A, x)}.
	\]
\end{proof}

Real maps are their own adjoints, and that is why they are often called \textit{self-adjoint}.

There is also a meaningful way to extend the notion of adjoint for general (non-endomorphism) linear maps. In general setting, we don't have a notion of compression of linear map: there's no canonical way to restrict the codomain. We can however interpret a map in a bigger space. Indeed, any map $A \in \L(V, W)$ can be canonically interpreted as a map $\tilde{A} \in \L(V \oplus W)$: define $\tilde{A}(v, w) = (0, A v)$. We call this map the \textit{symmetrization} of $A$. Now it makes sense to consider $(\tilde{A})^{*}$, the symmetrization has a unique adjoint. This adjoint does not in general live in $\L(V, W)$ anymore: but it turns out that it does live in $\L(W, V)$!

\begin{lause}
	For any $A \in \L(V, W)$ there exists a unique map $A^{*} \in \L(W, V)$ which we call the adjoint of $A$, such that $(\tilde{A})^{*} = \tilde{(A^{*})}$. Moreover, if $V = W$, the new notion of adjoint coincides with the old one.
\end{lause}

\begin{proof}
	Of course, strictly speaking $\tilde{A^{*}}$ would be map in $\L(W \oplus V)$, not in $\L(V \oplus W)$, but the two spaces are canonically isomorphic.

	The uniqueness follows from the already known uniqueness for the old notion. The map $(\cdot)^{*} : \L(V, W) \to \L(W, V)$ should again evidently be conjugate linear. Also, the same construction for basis elements of the form $A = (x \mapsto \langle x, v \rangle_{V} w)$ (where $v \in V$ and $w \in W$) works again. Indeed, for any $(x, y) \in V \oplus W$ the corresponding Rayleigh quotient is given by
	\[
		R(\tilde{A}, (x, y)) = \frac{\langle (0, A x), (x, y)\rangle_{V \oplus W} }{\langle (x, y), (x, y) \rangle_{V \oplus W}} = \frac{\langle A x, y\rangle_{W} }{\langle x, x \rangle_{V} + \langle y, y \rangle_{W}} = \frac{\langle x, v\rangle_{V} \langle w, y\rangle_{W} }{\langle x, x \rangle_{V} + \langle y, y \rangle_{W}},
	\]
	and it's clear that we may set $A^{*} = (x \mapsto \langle x, w \rangle_{W} v)$. Our construction also makes it clear that this notion coincides with the old one.
\end{proof}

The previous proof also gives a convenient corollary, which is the most common definition for adjoint.

\begin{kor}
	For any $A \in \L(V, W)$, the adjoint $A^{*} \in \L(W, V)$ is unique linear map such that for any $v \in V$ and $w \in W$ we have
	\[
		\langle A v, w \rangle_{W} = \langle v, A^{*} w \rangle_{V}.
	\]
	In particular, map $A \in \L(V)$ is real for any $v, w \in V$ we have
	\[
		\langle A v, w \rangle = \langle v, A w \rangle.
	\]
	and imaginary if for any $v, w \in V$
	\[
		\langle A v, w \rangle = -\langle v, A w \rangle.
	\]
\end{kor}

The previous corollary makes many of the basic properties of adjoint, which we collect below, evident.

\begin{lause}\label{basic_adjoint}
	For any linear maps $A$ and $B$, with appropriate domains and codomains, and $\lambda \in \C$ we have
	\begin{enumerate}[i)]
		\item Matrix of $A^{*}$ with respect to any orthonormal basis is conjugate transpose of matrix of $A$, i.e. $A^{*}_{i, j} = \overline{A_{j, i}}$.
		\item $(A^{*})^{*} = A$
		\item $(A + B)^{*} = A^{*} + B^{*}$
		\item $(\lambda I)^{*} = \overline{\lambda} I$
		\item $(AB)^{*} = B^{*}A^{*}$.
		\item $\kernel(A) = \image(A)^{\perp}$
	\end{enumerate}
\end{lause}

\subsection{Examples}

It's high time to have some examples.

Most obvious, although not very interesting, representatives of real/imaginary/positive maps are real/imaginary/positive scalar multiples of the identity. Projections are stereotypical examples of positive and hence real maps. \textcolor{red}{Projections, proper definition}. Indeed, one-dimensional projections are given by $A = (x \mapsto \langle x, v \rangle v)$ for some $v \in V$ with $|v| = 1$. For such maps $\langle A x, x \rangle = \langle x, v \rangle \langle v, x \rangle = |\langle x, v \rangle|^{2} \geq 0$.
Higher dimensional projections are simply sums of one-dimensional ones, so they are also positive and real. More generally one could take any positive linear combination of projections to get much more positive maps, and real linear combination of projections to get real maps.

As we earlier noticed, however, not every map with real eigenvalues is real, and not every map non-negative eigenvalues is positive. It turns out that the basis elements of the form $A = (x \mapsto \langle x, v \rangle w)$ are real if and only if $v$ and $w$ are real multiples of each other, or to be precise, if there exists $\alpha, \beta \in \R$, not both $0$, such that $\alpha v + \beta w = 0$. Indeed, by the corollary, the map is real if for any $x, y \in V$ we have
\[
	\langle x, v \rangle \langle w, x \rangle = \langle A x, y \rangle = \langle x, A y \rangle = \langle x, w \rangle \langle v, x \rangle.
\]
Now if $v$ and $w$ are not parallel, we can find $x$ such that $\langle x, v \rangle = 0 \neq \langle x, w \rangle$, which contradicts the previous. The case of parallel $v$ and $w$ is easy to check.

While hunting for examples, it's worthwhile to note that in some sense $\H(V)$ is not essentially bigger than $\H_{+}(V)$: if $A \in \H(V)$ we can always find a positive real number $\lambda$ such that $A + \lambda I \in \H_{+}(V)$.
To this end, note that the quadratic form of $A + \lambda I$ at $v \in V$ is $\langle (A + \lambda I) v, v \rangle = \langle A v, v \rangle + \lambda \langle v, v \rangle$. But if $\lambda \geq \|A\|$, the operator norm of $A$, the previous quantity is non-negative for any $v \in V$.

TODO: $2 \times 2$ case.

\section{Spectral theorem}

One might wonder if there are other examples of positive maps than positive linear combination of projections. Rather surprisingly, there are none.

\begin{lause}\label{cheapSpectral}
	$A \in \L(V)$ is positive if and only for some $m \geq 0$, $\lambda_{i} > 0$ and $v_{i} \in V$ for $1 \leq i \leq m$ we have
	\[
		A = \sum_{1 \leq i \leq m} \lambda_{i} P_{v_{i}}.
	\]
\end{lause}
\begin{proof}
	We already proved one direction: every map of the previous form is positive.

	The other direction is tricky. The idea is to somehow find the vectors $v_{i}$. The problem is that such representation is by no means unique. If $A$ is any projection for instance, we could let $v_{i}$'s by any orthonormal basis of the corresponding subspace and $\lambda_{i}$'s all equal to one. There's no vector one has to choose.

	But we can think in reverse: there could be many vectors we cannot choose, depending on the map $A$. If $A$ is any non-identity projection to subspace $W$, say, we can only choose $v_{i}$'s in $W$ itself. Indeed, if $x \in W^{\perp}$, we have $A x = 0$, and hence $\langle A x, x \rangle = 0$. By comparing the quadratic form it follows $\langle P_{v_{i}} x, x \rangle = |\langle v_{i}, x \rangle|^{2}$ for any $1 \leq i \leq m$. But this means that $v_{i} \perp W^{\perp}$ and hence $v_{i} \in W$.

	More generally, if it so happens that for some $v \in V$ we have $\langle A v, v \rangle = 0$, we must have $v_{i} \perp v$ for any $1 \leq i \leq m$. But this means that were there such representation, we should have the following.

	\begin{lem}\label{spectralZeroLemma}
		If $A \in \H_{+}(V)$ and $\langle A v, v \rangle = 0$ for some $v \in V$, then $A v = 0$ and $A w \perp v$ for any $w \in v$.
	\end{lem}

	Before proving the Lemma, we complete the proof given the Lemma.

	Proof is by induction on $n$, the dimension of the space. If $n = 0$, the claim is evident. For induction step assume first that there exists $v \in V$ such that $\langle A v, v \rangle = 0$. Then by the Lemma for any $w \in v^{\perp}$ we have $A w \in v^{\perp} =: W$. But this means that $A = \incl{V}{W} \circ \arestr{A}{W} \circ P_{W} = A$. Now $\arestr{A}{W}$ is also positive, and $\dim(W) < n$. By induction assumption we have numbers $\lambda_{i}$ and vectors $v_{i} \in V$ for the map $\arestr{A}{W}$, but such representation for $\arestr{A}{W}$ immediately gives representation for $A$ also.

	We just have to get rid of the extra assumption on the existence of such $v$. But for this, note that if $\lambda = \inf_{|v| = 1} \langle A v, v \rangle$, consider $B = A - \lambda I$. Now $\inf_{|v| = 1} \langle B v, v \rangle = 0$, and $B$ is hence positive. Also, by compactness, the infimum is attained at some point $v$, so $B$ satisfies our assumptions. Now cook up a representation for $B$ and add orthonormal basis of $V$ with $\lambda_{i}$'s equal to $\lambda$: this is required representation for $A$. 
\end{proof}

TODO: image of the proof process in $\R^{3}$.

\begin{proof}[Proof of lemma \ref{spectralZeroLemma}]
	Take any $w \in V$. Now by assumption for any $c \in \C$ we have
	\[
		Q_{A}(c v + w) = \langle A (c v + w), c v + w \rangle = |c| \langle A v, v \rangle + c \langle A v, w \rangle + \overline{c} \langle A w, v \rangle + \langle A w, w \rangle \geq 0
	\]
	But this easily implies that $\langle A v, w \rangle = 0 = \langle A w, v \rangle$ for any $w \in V$. The first equality implies that $A v = 0$ and the second that $A w \perp v$ for any $w \in V$.
\end{proof}

As discussed, there should be obvious generalization for real maps: they real linear combinations for projections. We can however still improve the statement: we can take $v_{i}$'s to be orthogonal, and hence $m \leq n$. We have thus arrived at Spectral theorem.

\begin{lause}[Spectral theorem]
	Let $A \in \H(V)$. Then there exists real numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$ and orthonormal vectors $v_{1}, v_{2}, \ldots, v_{n}$ such that
	\begin{align}\label{spectralrepr}
		A = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}}.
	\end{align}
\end{lause}
\begin{proof}
	Let's first check case of positive $A$. There's not very many things to change from the proof of theorem \ref{cheapSpectral}. Indeed, we again argue by induction. The case $n = 0$ is again clear. In the induction step we found that induction assumption applies to $A - \lambda I$ compressed to suitable $(n - 1)$-subspace. There we can cook up required representation, and bring back the representation for $A - \lambda I$ itself. That is we have
	\[
		A - \lambda I = \sum_{i = 1}^{n - 1} \lambda_{i} P_{v_{i}}
	\]
	for orthonormal $v_{i}$'s and non-negative $\lambda_{i}$'s. But then if $v_{n}$ is a missing orthonormal vector, we find that
	\[
		A = \sum_{i = 1}^{n} (\lambda_{i} + \lambda) P_{v_{i}},
	\]
	where $\lambda_{n} = 0$. But this is what we wanted.

	For non-positive $A$, simply add suitable multiple of identity to get $B := A + \lambda I \geq 0$ and apply what we have proved to $B$. If we have representation for $B$, we can easily cook up one for $A$: just subtract $\lambda$ for $\lambda_{i}$'s in the representation of $B$.
\end{proof}

In the representation \ref{spectralrepr} the numbers $\lambda_{i}$ are evidently the eigenvalues of $A$ and vectors $v_{i}$ the corresponding eigenvectors; this is why we call it the \textit{Spectral representation}. Such representation is of course not unique: if $A = I$, we could again choose $v_{i}$'s to be any orthonormal basis of $V$.

There is way to make the Spectral representation unique, however. For this we have to change $v_{i}$ to corresponding eigenspaces.

\begin{lause}[Spectral theorem]
	Let $A \in \H(V)$. Then there exists unique non-negative integer $m$, distinct real numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}$ and non-trivial orthogonal subspaces of $V$, $E_{\lambda_{1}}, E_{\lambda_{2}}, \ldots E_{\lambda_{m}}$ with $E_{\lambda_{1}} + E_{\lambda_{2}} + \ldots + E_{\lambda_{m}} = V$, such that
	\begin{align*}\label{spectralrepr2}
		A = \sum_{i = 1}^{m} \lambda_{i} P_{E_{\lambda_{i}}}.
	\end{align*}
	Moreover, this representation is unique.
\end{lause}
\begin{proof}
	Existence of such representation immediately follows from the previous form of Spectral theorem. For uniqueness, note that $\lambda_{i}$'s are necessarily the eigenvalues of $A$ and $E_{\lambda_{i}}$'s the corresponding eigenspaces.
\end{proof}

Although the latter version is definitely of theoretical importance, we will mostly stick the former: it only contains one-dimensional projections.

Spectral representation makes many of the properties of real maps obvious. For instance any power of real map is real: indeed, if $A = \sum_{1 \leq i \leq n} \lambda_{i} P_{v_{i}}$, then
\[
	A^{2} = \left(\sum_{i = 1} \lambda_{i} P_{v_{i}}\right) \left(\sum_{j = 1} \lambda_{j} P_{v_{j}}\right) = \sum_{i = 1}^{n} \sum_{j = 1}^{n}\lambda_{i} \lambda_{j} P_{v_{i}} P_{v_{j}} = \sum_{i = 1}^{n} \lambda_{i}^2 P_{v_{i}},
\]
since $P_{v} P_{w} = 0$ for $v \perp w$. By induction one can extend the previous for higher powers. In other words: eigenspaces are preserved under compositional powers, and eigenvalues are ones to get powered up. From the original definition this is not all that clear. One could even extend to polynomials. If $p(x) = c_{n} x^{n} + c_{n- 1} x^{n - 1} + \ldots c_{1} x + c_{0}$, with $c_{i} \in \R$, we should write
\[
	p(A) = c_{n} A^{n} + c_{n - 1} A^{n - 1} + \ldots c_{1} A + c_{0} = \sum_{1 \leq i \leq n} p(\lambda_{i}) P_{v_{i}}.
\]
This implies that if $p$ is the characteristic polynomial of $A$, then $p(A) = 0$: the special case of Cayley Hamilton theorem. Moreover, the minimal polynomial of $A$ is the polynomial with the eigenvalues of $A$ as single roots.

But even better, if $p$ is polynomial with all except one, say $\lambda_{i}$, of the eigenvalues of $A$ as roots, then $p(A) = p(\lambda_{i}) P_{E_{\lambda_{i}}}$. In particular, the projections to eigenspaces of $A$ are actually polynomials of $A$.

Also, given $A \in \H(V)$, we may write any $x \in V$ in the form $v = \sum_{1 \leq i \leq n} x_{i} v_{i}$, where $(v_{i})_{i = 1}^{n}$ is a eigenbasis for $A$ and $x_{i} = \langle x, v_{i} \rangle$. Now $A x = \sum_{1 \leq i \leq n} \lambda_{i} x_{i} v_{i}$, so for instance

\begin{itemize}
\item $Q_{A}(x) = \langle A x, x \rangle = \sum_{1 \leq i \leq n} \lambda_{i} x_{i}^{2}$. Thus $Q_{A}$ is just a positive linear combination of eigenvalues, and $R(A, \cdot)$ convex combination.
\item $\|A x\|^{2} = \langle A x, A x \rangle = \sum_{1 \leq i \leq n} \lambda_{i}^{2} x_{i}^{2} \leq \left(\max_{1 \leq i \leq n} \lambda_{i}^2 \right) \sum_{1 \leq i \leq n} x_{i}^2 = \left(\max_{1 \leq i \leq n} \lambda_{i}^2 \right) \|x\|^{2}$. It follows that $\|A\| = \max_{1 \leq i \leq n} |\lambda_{i}|$.
\end{itemize}

Similarly, if $A \geq 0$, $A$ has a unique positive square root, which we denote by $A^{\frac{1}{2}}$: map such that $A^{\frac{1}{2}} A^{\frac{1}{2}} = A$. Given the spectral reprsentation $A = \sum_{1 \leq i \leq n} \lambda_{i} P_{v_{i}}$, we can simply set $A^{\frac{1}{2}} = \sum_{1 \leq i \leq n} \lambda_{i}^{\frac{1}{2}} P_{v_{i}}$. As for the uniqueness, note that if $B$ is a positive square root for $A$ and $B = \sum_{1 \leq i \leq n} \lambda_{i}' P_{v_{i}'}$, then $B^2 = \sum_{1 \leq i \leq n} \lambda_{i}'^{2} P_{v_{i}'}$. It follows that eigenvalues of $B$ are simply square roots of eigenvalues of $A$ and the corresponding eigenspaces are equal. Of course, the whole uniqueness argument floats more naturally with unique spectral representation.

\subsection{Commuting real maps}

\textbf{Warning!} Composition of positive maps need not be positive!

If $A, B \in \H_{+}(V)$, then, as we noticed, $(A B)^{*} = B^{*} A^{*} = B A$, so for $A B$ to be even real, $A$ and $B$ would at least need to commute. Natural question follows: when do two positive maps commute? Since $(c_{1} I + A)$ and $(c_{2} I + B)$ commute if and only if $A$ and $B$ do, this is same as asking when do two real maps commute.

It turns out that real maps commute only if they ``trivially" commute, in the following sense. If there exists vectors $v_{1}, v_{2}, \ldots, v_{n}$ and numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$ and $\lambda'_{1}, \lambda'_{2}, \ldots, \lambda'_{n}$ such that
\[
	A = \sum_{1 \leq i \leq n} \lambda_{i} P_{v_{i}} \; \text{ and } \; B = \sum_{1 \leq i \leq n} \lambda'_{i} P_{v_{i}},
\]
then $A$ and $B$ are said to be \textit{simultaneously diagonalizable}. Simultaneosly diagonalizable maps trivially commute, and it turns out that if two real maps commute, they are indeed simultenously diagonalizable.

To prove this statement, we start with a lemma, simplest non-trivial case of the statement.

\begin{lem}\label{projectionLemma}
	Let $W_{1}, W_{2} \subset V$ be two subspaces. Then $P_{W_{1}}$ and $P_{W_{2}}$ commute if and only if there exists orthogonal subspaces $U_{1}, U_{2}$ and $U_{0}$ such that
	\[
		W_{1} = U_{1} + U_{0}  \; \text{ and } \; W_{2} = U_{2} + U_{0}.
	\]
	We then have $P_{W_{1}} = P_{U_{1}} + P_{U_{0}}$ and $P_{W_{2}} = P_{U_{2}} + P_{U_{0}}$, and $U_{0} = W_{1} \cap W_{2}$.
\end{lem}
\begin{proof}
	Write $U_{0} := W_{1} \cap W_{2}$ and $W_{i} = U_{0} + U_{i}$ for some $U_{i} \perp U_{0}$ for $i \in \{1, 2\}$. Now $P_{W_{i}} = P_{U_{i}} + P_{U_{0}}$ for $i \in \{1, 2\}$ so it suffices to check that $U_{1} \perp U_{2}$. Equivalently, it suffices to prove that if $W_{1} \cap W_{2} = \{0\}$, and $P_{W_{1}}$ and $P_{W_{2}}$ commute, then $W_{1} \perp W_{2}$ or equivalently $P_{W_{1}}P_{W_{2}} = 0 = P_{W_{2}}P_{W_{1}}$. But for any $v \in V$ we have $W_{1} \ni P_{W_{1}}P_{W_{2}}v = P_{W_{2}}P_{W_{1}}v \in W_{2}$, so indeed $P_{W_{1}}P_{W_{2}} = 0 = P_{W_{2}}P_{W_{1}}$.
\end{proof}

\begin{maar}
	We say that two $W_{1}, W_{2} \subset V$ subspaces commute if the respective projections commute.
\end{maar}

\begin{lause}
	Let $\mathcal{A} = (A_{j})_{j \in J}$ by an arbitrary family of commuting real maps. Then there exists non-trivial orthogonal subspaces of $V$, $E_{1}, E_{2}, \ldots E_{m}$ with $E_{1} + E_{2} + \ldots + E_{m} = V$ and numbers $\lambda_{i, j}$ for $j \in J$ and $1 \leq i \leq n$ such that
	\[
		A_{j} = \sum_{1 \leq i \leq m} \lambda_{i, j} P_{E_{i}}
	\]
	for any $j \in J$.
\end{lause}
\begin{proof}
	The main idea is the following: like in the spectral theorem, we would like to somehow find the subspaces $E_{1}, E_{2}, \ldots E_{m}$. Also, at least for finite families, we could probably use induction, so we should get far just by proving the theorem for a family of only two maps. For two projections we have already proved the statement as lemma \ref{projectionLemma}.

	Now here's the trick: if two maps commute, so do all their polynomials. Hence if we have two commuting $A$ and $B$, we know that all the respective eigenspaces commute. Now if we could prove the statement at least for finite families of projections, we could conclude the case of two general maps. Indeed we could write any eigenprojection of $A$ or $B$ as a linear combination of sum finite family of orthogonal (orthogonal) projections, but those projections would then also span $A$ and $B$.

	More generally, if we could prove the statement for arbitrary families of projections, the same argument would yield it for any family of more general linear maps, so we can safely assume that all the maps $A_{j}$ are projections.

	Let's first deal with the finite case by induction. As mentioned, we already dealt with the case $|J| = 2$, but we can draw better conclusions. If we have two commuting projections $P_{W_{1}}$ and $P_{W_{2}}$ in $\mathcal{A}$. Now by the lemma we may write $P_{W_{1}} = P_{U_{1}} + P_{U_{0}}$ and $P_{W_{2}} = P_{U_{2}} + P_{U_{0}}$. The nice things is that any map in $\mathcal{A}$ also commutes with $P_{W_{1}} + P_{W_{2}} = P_{U_{1}} + P_{U_{2}} + 2 P_{U_{0}}$, so also with it's eigenprojections, $P_{U_{0}}$ and $P_{U_{1} + U_{2}}$. It follows that any map in $\mathcal{A}$ commutes with $U_{0}, U_{1}$ and $U_{2}$.

	We have split the subspaces $W_{1}$ and $W_{2}$ in pieces, and we could actually forget $W_{1}$ and $W_{2}$ altogether and replace them by $U_{0}, U_{1}$ and $U_{2}$: note that all the same assumption hold for this new family, and $U_{0}, U_{1}$ and $U_{2}$ span $W_{1}$ and $W_{2}$.

	Problem here is of course: it's not clear that the new family, say $\mathcal{A}'$ is any simpler than $\mathcal{A}$! It could well have more elements than $\mathcal{A}$ so we can't just do straightforward induction. What could happen also is that some of the subspaces $U_{0}, U_{1}, U_{2}$ coincide with the subspaces already present in the family, so the size of the family doesn't increase, and it could even decrease. This will indeed happen. One way to see this is to look at the sum of dimensions of all the projections of the family: if we change the family this sum cannot increase. Moreover, if we pick two subspaces $W_{1}$ and $W_{2}$ which are not orthogonal, this sum will decrease!

	The conclusion is: pick pairs projections with non-orthogonal subspaces and do the replacing procedure as explained before; this process will eventually stop since the sum of dimesions can't drop below zero. But the only reason this process could stop is that all subspaces are pairwise orthogonal in which case we are done. The proof of finite case is complete.

	There are many ways to bootstrap the previous argument for arbitrary families. For any finite subfamily we can form the set of generating projections. If add one more map, the set projections get refined: some of the subspaces get split to pieces. Now sizes of all these generating families are bounded by $n$ so we may pick one with most number of elements. Now if $A$ is any projection in $\mathcal{A}$, by maximality, adding it to the family does not refine the generating set. But this means that the generating set generates any element of $\mathcal{A}$ and we are done.

	We also see that there exists unique minimal family of generating projections TODO.

	Alternative approach to the theorem could be to look at the commutative $\R$-algebra of real maps generated by $\mathcal{A}$: generating projections will be in some sense minimal projections in this algebra.
\end{proof}

The previous theorem sends a very important message.

\begin{phil}
	Commutativity kills the exciting phenomena.
\end{phil}

One would naturally hope that product of positive maps is still positive, but as soon as we try to make such restriction, everything degenerates to $\R^{m}$, or to diagonal maps. Dealing with diagonal maps is then again just dealing with many real numbers at the same time: of course this makes sense and all, but doesn't lead to very interesting concept.

Conversely, if one wants exciting things to happen, one should make things very non-commutative.

TODO: independence of random variables.

\section{Congruence}

\subsection{$*$-conjugation}

There is one very important way to produce positive maps from others, called congruence. Given any two positive maps $A$ and $B$, their composition need not be positive, but the map $BAB$ is. First of all, it is real as $(BAB)^{*} = B^{*} A^{*} B^{*} = BAB$. Also $Q_{BAB}(v) = \langle BAB v, v \rangle = \langle A (B v), (B v) \rangle \geq 0$ for any $v \in V$. We didn't really need the assumption on the positivity of $B$, but realness was not that important either. Namely for arbitrary linear $B$ we could consider the product $B^{*}AB$ instead: this is positive whenever $A$ is. If $C = B^{*}AB$ for some $B \in \L(V)$, we say that $C$ is $*$-conjugate of $A$.

We also see that $Q_{B^{*}AB} = Q_{A} \circ B$: conjugation is a change of basis in the quadratic form. This is the main motivation for the definition of the $*$-conjugation. We have already seen that the quadratic form of a map is a good way to characterize many of its good properties, so to some extent to understand maps, we just to need to understand structure of their quadratic forms. By change of basis of the quadratic form we have a good control of what happens. We might however lose some information: if $B = 0$, for instance, the quadratic form after $*$-conjugation by $B$ doesn't tell much about $A$. But if $B$ is invertible, or equivalently if $C$ and $B$ are $*$-conjugates of each other, we shouldn't lose any information. If this is the case, we say that $A$ and $C$ are congruent. It is easily verified that congruence is a equivalence relation.

The construction of $*$-conjugation makes also sense for general linear map $A$, i.e. we could just as well $*$-conjugate non-positive, or even non-real maps. The result then need not be positive or real, and in general, $*$-conjugation loses its usefulness.

The previous construction can be also performed between two spaces $V$ and $W$: given any map $B \in \L(V, W)$ and $A \in \H_{+}(W)/\H(W)/\L(W)$, we note that $B^{*}AB \in \H_{+}(V)/\H(W)/\L(W)$. For self-adjoint maps we can say a lot more: while congruence doesn't in general preserve eigenvalues, it preserves their signs.

\begin{lause}[Sylvester's Law of Inertia]
	$A, B \in \H(V)$ are congruent, if and only if $A$ and $B$ have equally many positive, negative and zero eigenvalues, counted with multiplicity.
\end{lause}
\begin{proof}
	Let's start with the ``if" part. Let's denote the eigenvalues of $A$ and $B$ by $\lambda_{1} \leq \lambda _{2} \leq \ldots \leq \lambda_{n}$ and $\lambda_{1}' \leq \lambda _{2}' \leq \ldots \leq \lambda_{n}'$, respectively, and the corresponding eigenvectors with $v_{1}, v_{2}, \ldots, v_{n}$ and $v_{1}', v_{2}', \ldots, v_{n}'$. By assumption $\lambda_{i}$ and $\lambda_{i}'$ have the same sign (or are both zero) for any $1 \leq i \leq n$, so we may find non-zero real numbers $t_{1}, t_{2}, \ldots, t_{n}$ such that $\lambda_{i} = \lambda_{i}' t_{i}^{2}$. Now consider a linear map $C$ with $C v_{i} = t_{i} v_{i}'$. $C$ is clearly a surjection and hence a bijection. Also if $v = \sum_{i = 1}^{n} x_{i} v_{i}$ $(Q_{B} \circ C)(v) = Q_{B}(\sum_{i = 1}^{n} x_{i} t_{i} v_{i}') = \sum_{i = 1}^{n} |x_{i}|^{2} t_{i}^2 \lambda_{i}' = \sum_{i = 1}^{n} |x_{i}|^{2} \lambda_{i} = Q_{A}(v)$ so $Q_{C^{*}BC} = Q_{B} \circ C = Q_{A}$. It follows that $C^{*}BC = A$ and hence $A$ and $B$ are congruent.

	The ``only if" - part is a bit trickier. The idea is to find a good description for the number of positive non-negative eigenvalues. We noticed before that we can write quadratic forms in the form $Q_{A}(v) = \sum_{i = 1}^{n} \lambda_{i} |x_{i}|^{2}$ if $v = \sum_{i = 1}^{n} x_{i}v_{i}$, and $v_{i}$ are the eigenvectos of $A$ with $\lambda_{i}'s$ as the corresponding eigenvectors. In particular if say first $k$ eigenvalues are negative, $Q_{A}$ will be negative on $\vspan\{v_{i} | 1 \leq i \leq k\}$, a $k$-dimensional subspace, minus zero. Similarly, now $n - k$ of the eigenvalues are non-negative, so the quadratic form is non-negative on a subspace of dimension of at least $n - k$. But the dimensions can't be any bigger: if $Q_{A}$ were for instance negative on some $k + 1$ dimesional subspace, this subspace would necessarily intersect a subspace where $Q_{A}$ is non-negative, which is non-sense.

	Congruence preserves the previous notion: if $Q_{B}$ is negative on a subspace of dimension $k$, so is $Q_{B} \circ C$ for any invertible $C$; namely in the inverse image. Same reasoning holds for the the subspace on which $Q_{B}$ is non-negative, so again, $Q_{B} \circ C$ has to have similar structure. We are done.
\end{proof}

If $n_{0}, n_{-}$ and $n_{+}$ denote the number of zero, negative and positive eigenvalues of $A$, \textit{inertia} of $A$ is the triplet $\{n_{0}, n_{-}, n_{+} \} := \{n_{0}(A), n_{-}(A), n_{+}(A) \}$. The previous theorem can be hence restated, that inertia is invariant under congruence.

The proof also gives a useful characterization for the number of non-negative eigenvalues.

\begin{kor}
	If $A \in \H(V)$, number of non-negative eigenvalues of $A$ equals largest non-negative integer $k$ such that for some subspace $W \subset V$ of dimension $k$ the quadratic form $Q_{A}$ is non-negative on $W$, or equivalently, $\arestr{A}{W} \geq 0$.
\end{kor}

Sylvester's Law of inertia gives another proof of the fact that strictly positive maps are exactly the maps congruent to the identity, and positive maps are the maps congruent to some projection. More precisely, the positive maps are partitioned to $n + 1$ congruence classes depending on their rank, $k$:th congruence class containing the projections to $k$-dimensional subspaces. $0$:th class contains only the zero map, the only rank $0$ positive map, and the $n$:th class is the class of strictly positive maps.

If one $*$-conjugates with non-invertible, the inertia may change, but in quite obvious way only: some eigenvalues may move to $0$. In particular, we have the following even a bit more general version of the law.

\begin{lause}[General Sylvester's Law of Inertia]
	For $A, B \in \normal(V)$ and $A$ is $*$-conjugate of $B$, if and only if $n_{\pm}(A) \leq n_{\pm}(B)$.
\end{lause}
\begin{proof}
	TODO
\end{proof}

This extension draws a picture about the relation of previously mentioned congcruence classes. We can move to the congruence classes of lower indeces by $*$-conjugation, but cannot move up the ladder: the complexity of quadratic forms cannot increase. One could also think that $*$-congruence for linear maps corresponds to multiplication by non-negative real for real numbers.

\subsection{Block decomposition}

Congruence is a convenient to tool to investigate positivity. The idea is that with conguence we can perform sort of a Gaussian elimination. If $n = 2$ for instance, we can write any real map in the matrix form
\[
	M =
	\begin{bmatrix}
		a & b \\
		\overline{b} & c
	\end{bmatrix}
\]
for some $a, c \in \R$ and $b \in \C$. Now if $a \neq 0$, we could eliminate with
\[
	D =
	\begin{bmatrix}
		1 & -\frac{b}{a} \\
		0 & 1
	\end{bmatrix}
\]
to get
\[
	M D =
	\begin{bmatrix}
		a & b \\
		\overline{b} & c
	\end{bmatrix}
	\begin{bmatrix}
		1 & -\frac{b}{a} \\
		0 & 1
	\end{bmatrix}
	=
	\begin{bmatrix}
		a & 0 \\
		\overline{b} & \frac{a c - |b|^2}{a}.
	\end{bmatrix}
\]
If we however eliminate from the other side by $D^{*}$, we get
\[
	D^{*} M D =
	\begin{bmatrix}
		a & 0 \\
		0 & \frac{a c - |b|^2}{a}.
	\end{bmatrix}
	=: M'
\]
Now $D$ is evidently invertible, it's determinant being $1$, so $M$ and $M'$ are congruent. Sylvester's law of inertia tell's us hence that that if $a > 0$ and $\det(M) \geq 0$, then $M \geq 0$.

We can generalize this thinking. For general $n$ if we have decomposition $V = W_{1} \oplus W_{2}$, then we can decompose any mapping $M$ as
\[
	M =
	\begin{bmatrix}
		A & B \\
		B^{*} & C
	\end{bmatrix},
\]
where $A, B$ and $C$ are the \textit{blocks} of $M$ given by $A = P_{W_{1}} \circ M \circ \incl{V}{W_{1}} = \arestr{M}{W_{1}}$, $B = P_{W_{1}} \circ M \circ \incl{V}{W_{2}}$ and $C = P_{W_{2}} \circ M \circ \incl{V}{W_{2}} = \arestr{M}{W_{2}}$. Now we can generalize the previous elimination: if $A$ happens to be invertible and we let
\[
	D =
	\begin{bmatrix}
		I & -A^{-1} B \\
		0 & I
	\end{bmatrix}
\]
then
\[
	D^{*} =
	\begin{bmatrix}
		I & 0 \\
		-B^{*} A^{-1}  & I
	\end{bmatrix}
\]
and
\begin{align}\label{schur_complement}
	D^{*} M D =
	\begin{bmatrix}
		I & 0 \\
		-B^{*} A^{-1}  & I
	\end{bmatrix}
	\begin{bmatrix}
		A & B \\
		B^{*} & C
	\end{bmatrix}
	\begin{bmatrix}
		I & -A^{-1} B \\
		0 & I
	\end{bmatrix}
	=
	\begin{bmatrix}
		A & 0 \\
		0 & C - B^{*} A^{-1} B
	\end{bmatrix}.
\end{align}
The map $(C - B^{*} A^{-1} B) : W_{2} \to W_{2}$ is called the \textit{Schur complement} of block $A$ of $M$, or maybe one should say Schur complement of $M$ with respect to $W_{1}$. We denote the Schur complement by $M/A$.

Now again if $A$ is invertible, $M \geq 0$ if and only if $A > 0$ and $M/A \geq 0$.

This observations leads to convenient characterization for strictly positivity, called Sylvester's criterion. If $W_{2}$ is $1$-dimensional, $M/A$ is just a real number and $M$ is stricly positive if and only if $A > 0$ and this real number is positive. On the other hand, by computing determinants we see that
\[
	\det(M) = \det \left(
	\begin{bmatrix}
		A & 0 \\
		0 & M/A
	\end{bmatrix}
	\right)
	=
	\det(A) \det(M/A),
\]
as $\det(D) = 1$. Hence $M$ is positive if and only if $\det(M)$ is positive and $A > 0$. Applying the same idea inductively we arrive at
\begin{lause}[Sylvester's criterion]
	$A \in \H(V)$ is stricly positive if and only for some (and then for any) sequence of subspaces $W_{1} \subset W_{2} \subset \ldots \subset W_{n - 1} \subset W_{n} = V$ with $\dim(W_{m}) = m$ we have $\det(A_{W_{m}}) > 0$ for any $1 \leq m \leq n$.
\end{lause}

One can solve $M$ from \ref{schur_complement} to arrive at so-called \textit{LDL-decomposition} of $M$:
\begin{align}\label{ldl_decomposition}
	M =
	\begin{bmatrix}
		A & B \\
		B^{*} & C
	\end{bmatrix}
	=
	\begin{bmatrix}
		I & 0 \\
		B^{*} A^{-1}  & I
	\end{bmatrix}
	\begin{bmatrix}
		A & 0 \\
		0 & C - B^{*} A^{-1} B
	\end{bmatrix}
	\begin{bmatrix}
		I & A^{-1} B \\
		0 & I
	\end{bmatrix}.
\end{align}

\section{Loewner Order}

\begin{maar}
	If $A, B \in \H(V)$, we write that $A \leq B$ ($A$ is smaller than $B$) if $B - A \geq 0$, $B - A$ is positive. If $B - A$ is strictly positive, we write $A < B$.
\end{maar}

We could of course have made such definition immediately after defining positive maps, but now we have proper tools to investigate such order. Proposition \ref{basic_positive} tells us that such order is indeed partial order on the $\R$-vector space of real maps. More explicitly, we have the following properties:


\begin{prop}
\begin{enumerate}[(i)]
		\item If $A \leq B$ then $\alpha A \leq \alpha B$ for any $\alpha \geq 0$.
		\item If $A \leq B$ and $B \leq C$ then $A \leq C$.
		\item If $A \leq B$ and $B \leq A$ then $A = B$.
		\item If $\lambda I \leq A$, all the eigenvalues of $A$ are at least $\lambda$.
\end{enumerate}
\end{prop}

Key thing here is to note what is missing from the standard real ordering: multiplication by positive map doesn't preserve usual ordering. This is the reason many standard arguments don't work for general real maps.

For example if $0 < a \leq b$, with real numbers one could multiply the inequalities by the positive number $(a b)^{-1}$ to get $0 < b^{-1} \leq a^{-1}$. This doesn't quite work with linear maps anymore.

Congruence is way to at least partially fix this deficit: it's almost like multiplying by positive number. We have
\begin{prop}
	If $A \leq B$, then for any $C$ we have $C^{*} A C \leq C^{*} B C$.
\end{prop}

Using the previous we can mimic the previous proof to make it work.

\begin{lause}
	If $0 < A \leq B$, then $B^{-1} \leq A^{-1}$.
\end{lause}
\begin{proof}
	As mentioned, we can't really multiply by $(A B)^{-1}$, as it does not preserve the order and doesn't even need to be positive. If $A$ and $B$ commute, this would work though. We can almost multiply by $A^{-1}$: $*$-conjugate by $A^{-\frac{1}{2}}$. This preserves the order, and we get
	\[
		I \leq A^{-\frac{1}{2}} B A^{-\frac{1}{2}}.
	\]
	Now one would sort of want to multiply $B^{-1}$; so $*$-conjugate by $B^{-\frac{1}{2}}$, but $B$ is in the middle, so this doesn't quite work. But now we can follow the original strategy: since $I \leq X := A^{-\frac{1}{2}} B A^{-\frac{1}{2}}$ we have $X^{-1} \leq I$, that is
	\[
		A^{\frac{1}{2}} B^{-1} A^{\frac{1}{2}} \leq I.
	\]
	This is already almost what we wanted: simply $*$-conjugate by $A^{-\frac{1}{2}}$.
\end{proof}

There's one wee bit non-trivial part in the proof: if $I \leq X$ then $X^{-1} \leq I$. But if $I \leq X$, all the eigenvalues of $X$ are at least $1$, so all the eigenvalues of its inverse are at most $1$, so $X \leq 1$.

\begin{huom}

Alternatively, we could conjugate both sides by $X^{-\frac{1}{2}}$ to arrive at the conclusion. Note that by doing this we have only used $*$-conjugation in the proof: actually we have $*$-conjugated altogether with 
\[
	A^{-\frac{1}{2}} (A^{-\frac{1}{2}}B A^{-\frac{1}{2}})^{-\frac{1}{2}} A^{-\frac{1}{2}} = (A^{\frac{1}{2}} (A^{-\frac{1}{2}}B A^{-\frac{1}{2}})^{\frac{1}{2}} A^{\frac{1}{2}})^{-1}.
\]
The map $A^{\frac{1}{2}} (A^{-\frac{1}{2}}B A^{-\frac{1}{2}})^{\frac{1}{2}} A^{\frac{1}{2}}$, which is real, is usually called the geometric mean of $A$ and $B$. It turns out that this mean, denoted by $G(A, B)$ satifies
\[
	G(A, B) = G(B, A) \;\;\; \text{ and } \;\;\; G(A, B)^{-1} = G(A^{-1}, B^{-1}),
\]
and if $A$ and $B$ commute we have $G(A, B) = (A B)^{\frac{1}{2}}$. The defining property of it we used it was that $G(A, B)$ is unique real map with
\[
	B = G(A, B) A^{-1} G(A, B).
\]

The point is: somewhat curiously we can almost do the original proof: just replace multiplication by congruence by squareroot, and replace squareroot of product by geometric mean.

\end{huom}

To further highlight the importance of Congruence, we can use it to change map inequalities to usual real inequalities. For instance, one can generalize so called (two variable) arithmetic-harmonic mean inequality, which states that for any two positive real numbers $a$ and $b$ we have
\[
	\frac{a + b}{2} \geq \frac{2}{\frac{1}{a} + \frac{1}{b}}.
\]
This classic inequality, which can be seen as a restatement of the convexity of the map $x \mapsto \frac{1}{x}$, can be verified for instance by multiplying out the denominator and rewriting it as $\frac{(a - b)^{2}}{ab} \geq 0$.

To prove the matrix version, namely
\[
	\frac{A + B}{2} \geq (A^{-1} + B^{-1})^{-1}
\]
for any $A, B > 0$, we can $*$-conjugate both sides by $A^{-\frac{1}{2}}$ to arrive at
\[
	\frac{I + A^{-\frac{1}{2}}B A^{-\frac{1}{2}}}{2} \geq 2 (I + A^{\frac{1}{2}}B^{-1} A^{\frac{1}{2}})^{-1}.
\]
If one writes $X = A^{-\frac{1}{2}}B A^{-\frac{1}{2}}$, this rewrites to
\[
	\frac{I + X}{2} \geq 2 (I + X^{-1})^{-1}.
\]
But now since $I$ and $X$ commute, the claim is evident form the scalar inequality.

\section{Absolute Value}

As adjoint behaves as conjugate, it would be natural to guess that
\[
	|A| := \left(A^{*}A\right)^{\frac{1}{2}},
\]
absolute value of a map, would have many similar properties as the standard absolute value. Note that in general we have $|A| \neq |A^{*}|$, and maps need not even go between the same spaces.

The following list of properties of the absolute value make it clear that this is indeed good definition.

\begin{itemize}
	\item $|A| \geq 0$ for any $A \in \L(V)$ and $|A| = A$, if and only if $A \geq 0$.
	\item For any $A \in \H(V)$ we have $-|A| \leq A \leq |A|$, or equivalently $|Q_{A}(v)| \leq Q_{|A|}(v)$ for any $v \in V$
	\item For any $v \in V$ we have $\|A v\| = \||A|v\|$.
\end{itemize}

One might be tempted to think that the previous absolute value leads to an triangle inequality, i.e.
\[
	|A + B| \leq |A| + |B|,
\]
for any $A, B \in \L(V)$, or at least $A, B \in \H^{n}$. Such inequality doesn't hold, but it's not that far from being true. Like in the real case, one would like to add
\[
	-|A| \leq A \leq |A| \; \text{ and } \; -|B| \leq B \leq |B|,
\]
to get
\[
	-(|A| + |B|) \leq A + B \leq |A| + |B|.
\]
The problem is that we can't make any further conclusions: just because $-Y \leq X \leq Y$, it is not necessarily the case that $|X| \leq Y$. Thinking in quadratic forms we get the inequality
\begin{align}
	|Q_{A + B}(v)| \leq Q_{|A| + |B|}(v),
\end{align}
for any $v \in V$, but this does not imply that $Q_{|A + B|}(v) \leq Q_{|A| + |B|}(v)$. Indeed $|Q_{A + B}(v)| \leq Q_{|A + B|}(v)$, as we noticed, so the inequality is going to the wrong direction. If however $v$ is an eigenvector of $A + B$, we have $|Q_{A + B}(v)| = Q_{|A + B|}(v)$, and it follows that
\[
	Q_{|A + B|}(v) \leq Q_{|A| + |B|}(v)
\]
holds for eigenvectors $v$ of $|A + B|$. Summing over the eigenvector we see that
\[
	\tr|A + B| \leq \tr |A| + \tr |B|,
\]
so instead of the full inequality, we get inequality for traces. There is a nice generalization for the previous we'll get back to.

\section{Notes and references}

\section{Ideas}

\begin{itemize}
	\item Normal maps
	\item Square root of a matrix
	\item Ellipses map to ellipses
	\item Sylvester's criterion
	\item adjoints of vectors
	\item projections with inclusions
	\item order of projections
	\item Loewner order
	\item Schur complements
	\item Moore-Penrose pseudoinverse
	\item (canonical, lwdin) orthogonalization, polar decomposition and orthogonal Procrustes problem
	\item projection matrices
	\item Hilbert-Schmidt norm ($\to$ matrix functions?) and inner product
	\item Hilbert spaces
	\item Real vs. complex
	\item Positive definite kernels
	\item Weakly positive matrices
	\item Hlawka inequality for determinant %http://mathoverflow.net/questions/182181/hlawka-inequality-for-determinants-of-positive-definite-matrices?rq=1
	\item Trace-characterization of positive maps.
	\item Splitting positive maps to pseudo square roots
	\item Product of maps
\end{itemize}

Ideas how to rewrite this section:
\begin{itemize}
	\item Spectral theorem for positive maps
	\item Map is positive, if all of its restrictions are. One dimensional maps are positive if and only the scalar is positive.
	\item Map is real, if all of its restrictions are. One dimensional maps are real if and only the scalar is real.
	\item Adjoint commutes with restriction. Adjoint of one dimensional map is its conjugate.
	\item How should one restrict linear mappings? (Use inclusions and finally project)
	\item Central notion is adjoint
	\item $*$-conjugation is natural notion with adjoint
	\item More generally, adjoint of a map $A \in L(V, W)$ is a unique map $A^{*} \in \L(V, W)$ such that $A \oplus A^{*}$ is real.
	\item Increasing sequence of subspaces and restriction with positive determinant $\Rightarrow$ the whole map is positive. Equivalently, map is positive if it has positive determinant and has a subspace where it is positive: $A \geq 0$ if and only if for any $B \geq 0$ we have $\tr(A B) \geq 0$.
\end{itemize}





