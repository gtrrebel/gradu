\chapter{Positive maps}

\section{Motivation}

\subsection{The right definition}

Throughout this thesis, if not stated otherwise, $V = (V, \langle \cdot, \cdot \rangle)$ denotes an inner product space over $\C$ of dimension $n$ (where $n$ is an arbitrary but fixed positive integer).

\begin{maar}
	We say that $A$ is \textit{positive map}, or simply \textit{positive}, and write $A \geq 0$, if for any $v \in V$ we have
	\begin{align*}
		\langle A v, v \rangle \geq 0.
	\end{align*}
\end{maar}

Why is this the right definition for positivity? Do we really need an inner product to define positivity?

While these are both excellent questions (and one should definitely think about them), there is no way to satisfactorily answer them in the scope of this thesis. Instead, this section is an attempt to explain why the definition is pretty damn good.

Note that, contrary to the previous chapter, we snuck in the complex numbers and ``general" vector space to the definition (see ``Notation and conventions" in the previous chapter). It doesn't make much difference whether we talk about real or complex numbers but the author thinks that some of the arguments are more natural in the complex world. Also, working with a general vector space $V$ is mostly just a reminder of the fact that there is something tensorial going on.

Theorem \ref{positive_machine} immediately implies

\begin{lause}
The set
\begin{align*}
	\{A \in \L(V) | \text{ $A$ is positive} \}
\end{align*}
is a closed convex cone.
\end{lause}

We denote this cone of positive maps (of $V$) by $\H_{+}(V)$.

In general one should think that the convex cones are models of positive real numbers. Such model need not be very good however: the whole vector space is always a convex cone. To fix this problem one introduces the concept of salient cone.

\begin{maar}
	A convex cone $C \subset V$ is \textbf{salient cone}, or simply \textbf{salient}, if whenever both $v \in C$ and $-v \in C$, then necessarily $v = 0$.
\end{maar}

Conveniently enough $\H_{+}(V)$ is a salient cone, but this is by no means trivial property.

\begin{lem}\label{inj_compr}
	If $A \in \L(V)$ and $\langle A v, v \rangle = 0$ for any $v \subset V$, then $A = 0$.
\end{lem} 
\begin{proof}
	The idea is that we can recover the inner product from norm. Indeed, if $v, w \in V$, then $\|v + w\|^2 = \|v\|^2 + \|w\|^2 + 2 \Re(\langle v, w \rangle)$, so knowing the norm, we at least know the real part of the inner product. Doing the same trick with $\|v + i w\|^2$ we can figure out the imaginary part.

	How does this help us? By a similar argument $\langle A(v + w), v + w \rangle = \langle A v, v \rangle + \langle A w, w \rangle + \langle A v, w\rangle + \langle A w, v \rangle$, so given that the quadratic form is always zero, we have $\langle A v, w \rangle + \langle A w, v \rangle = 0$ for any $v, w \in V$. Expanding $\langle A (v + i w), v + i w \rangle$ we see that $-i \langle A v, w \rangle + i \langle A v, w \rangle = 0$, which together with the previous observation implies that $\langle A v , w \rangle = 0$ for any $v, w \in V$. By setting $w = A v$, this implies that $\|A v\|^{2} = 0$ for every $v \in V$, so $A = 0$.
\end{proof}

The idea of recovering the inner product from the norm is called \textbf{polarization}.

\begin{kor}\label{basic_positive}
	$\H_{+}(V)$ is a salient closed convex cone.
\end{kor}

Previous arguments carry directly to a much more general setting:

\begin{lause}\label{positive_proper}
	Let $V$ be a topological vector space (over $\R$ or $\C$) and $C^{*}$ a subset of its continuous dual. Then
	the dual cone of $C^{*}$ is salient, if and only if
	\begin{align*}
		\{v \in V | w^{*}(v) = 0 \text{ for every $w^{*} \in C^{*}$} \} = \{0\}.
	\end{align*}
\end{lause}

In our case the subset of the linear functionals are the mappings of the form $A \mapsto \langle A v, v \rangle$: they are called \textit{quadratic functionals}. For fixed $A \in \L(V)$ the map $v \mapsto \langle A v, v \rangle$ is the \textit{quadratic form} of $A$.

As one would hope, map $v \mapsto \alpha v$, i.e. $\alpha I$ is positive, if and only if $\alpha \geq 0$. In particular in one-dimensional spaces the notion works as expected. Fortunately there are other examples of positive maps: any orthogonal projection is positive.

\begin{prop}
	If $A \in \L(V)$ is a orthogonal projection, then $A \geq 0$.
\end{prop}
\begin{proof}
	As any orthogonal projetion is sum of one-dimensional orthogonal projections, we can assume that the $A$ is one-dimensional in the first place. It follows that $A = \langle \cdot, v \rangle v/\|v\|^2$ for some $v \in V \setminus \{0\}$. Now for every $w \in V$ we have
	\begin{align*}
		\langle A w, w \rangle = \langle \langle w, v \rangle v, w \rangle/\|v, v\|^{2} = |\langle w, v \rangle|^{2}/\|v\|^{2} \geq 0,
	\end{align*}
	so $A$ is positive.
\end{proof}

We denote the one-dimensional orthogonal projection to the span of $v \in V \setminus \{0\}$, i.e. the map $ \langle \cdot, v \rangle v/\|v\|^2$, by $P_{v}$. More generally, orthogonal projection to a subspace $W \subset V$ is denoted by $P_{W}$.

Taking positive linear combinations of orhogonal projections leads to large number of examples of positive maps.

\subsection{Real maps and adjoint}

Dual cone thinking lets us also lift other important notions.

\begin{maar}
	We say that a map $A \in \L(V)$ is \textit{real}, if
	\begin{align*}
		\langle A v, v \rangle \in \R
	\end{align*}
	for any $v \in V$.
\end{maar}

\begin{maar}
	We say that a map $A \in \L(V)$ is \textit{imaginary}, if
	\begin{align*}
		\langle A v, v \rangle \in i \R
	\end{align*}
	for any $v \in V$.
\end{maar}

\begin{maar}
	We say that a map $A \in \L(V)$ is \textit{strictly positive}, if
	\begin{align*}
		\langle A v, v \rangle > 0
	\end{align*}
	for any $v \in V \setminus \{0\}$.
\end{maar}

Map is strictly positive, if and only if it is positive and invertible, or equivalently, if it is real and has positive eigenvalues.

Families of real, imaginary and strictly positive maps are usually called Hermitian and Skew-Hermitian and positive definite. Reals maps will have a special role in our discussion. We write $A > 0$ if $A$ is strictly positive. They form a vector space over $\R$, which is denoted by $\H(V)$. Of course, every imaginary map is just $i$ times real map, and we won't preserve any special notation for such maps. Also, they don't really play any role in this thesis anyway.

We can also lift the concept of complex conjugate.

\begin{lause}
	For any $A \in \L(V)$ there exists unique map $A^{*} \in \L(V)$, called the \textit{adjoint} of $A$, for which for any $v \in V$ we have
	\begin{align*}
	\langle A^{*} v, v\rangle = \overline{\langle A v, v \rangle}
	\end{align*}
\end{lause}
\begin{proof}
	The uniqueness of adjoint is immediate from the Lemma \ref{inj_compr}. The map $(\cdot)^{*} : \L(V) \to \L(V)$ should evidently be conjugate linear, so for existence it suffices to find adjoint for suitable basis elements of $\L(V)$: the maps of the form $A = (x \mapsto \langle x, v \rangle w)$ for $v, w \in V$ will do.

	The quadratic form for such map is given by
	\begin{align*}
		\langle A x, x \rangle = \langle x, v \rangle \langle w, x \rangle.
	\end{align*}
	But if we define $A^{*} = (x \mapsto \langle x, w \rangle v)$, we have
	\begin{align*}
		\langle A^{*} x, x \rangle = \langle x, w \rangle \langle v, x \rangle = \overline{\langle w, x \rangle \langle x, v \rangle} = \overline{\langle A x, x \rangle}
	\end{align*}
	for any $x \in V$, so $A^{*}$ is indeed the adjoint of $A$.
\end{proof}

In more common terms: a adjoint of linear map $A \in \L(V)$ is the unique map $A^{*}$ such that

\begin{align}\label{adjoint_common}
	\langle A v, w \rangle = \langle v, A^{*} w \rangle
\end{align}
for any $v, w \in V$. This fact is easily verified by a polarization argument.

As real maps are their own adjoints, they are often appropriately called \textbf{self-adjoint}.

The previous observation makes many of the basic properties of adjoint, which we collect below, evident.

\begin{lause}\label{basic_adjoint}
	For any $A, B \in \L(V)$ and $\lambda \in \C$ we have
	\begin{enumerate}[i)]
		\item Matrix of $A^{*}$ with respect to any orthonormal basis is conjugate transpose of matrix of $A$, i.e. $A^{*}_{i, j} = \overline{A_{j, i}}$.
		\item $(A^{*})^{*} = A$
		\item $(A + B)^{*} = A^{*} + B^{*}$
		\item $(\lambda I)^{*} = \overline{\lambda} I$
		\item $(AB)^{*} = B^{*}A^{*}$.
	\end{enumerate}
\end{lause}

Using \ref{adjoint_common}, adjoint could also be defined between arbitrary two inner product spaces. With this more general definition the maps
\begin{align*}
	v : \C \to V & \hspace{1cm} & v(x) &= x v \\
	v^{*} : V \to \C & \hspace{1cm} & v^{*}(w) &= \langle w, v \rangle
\end{align*}
will be adjoints of each other. This lets us rewrite one-dimensional projections conveniently in the form
\begin{align*}
	P_{v} = \frac{1}{\|v\|^2} v v^{*}.
\end{align*}
More generally, for subspace $W \subset V$ the orthogonal projection $V \to W$ is the adjoint of the inclusion $\incl{V}{W} : W \to V$.

\subsection{More convincing}

Positive maps have many other desirable properties. First of all, eigenvalues of a positive map are non-negative. This fact is a corollary of a more general property.

\begin{maar}
	Let $W \subset V$ be a subspace and $A \in \L(V)$. Then the \textbf{compression} of $A$ to $W$, denoted by $\arestr{A}{W}$ is the linear map
	\begin{align*}
		\incl{V}{W}^{*} A \incl{V}{W} : W \to W.
	\end{align*}
\end{maar}

\begin{lem}
	Let $W \subset V$ and $A \geq 0$. Then also $\arestr{A}{W} \geq 0$. In particular all the eigenvalues of $A$ are non-negative.
\end{lem}
\begin{proof}
	Note that quadratic form gives essentially the one-dimensional compressions. Indeed, if $W = \vspan(v)$, then
	\begin{align*}
		\arestr{A}{W} x = \frac{\langle A x, v \rangle}{\langle v, v \rangle} v = \frac{\langle A v, v \rangle}{\langle v, v \rangle} x
	\end{align*}
	for any $x \in \vspan(v)$. This means that a map is positive, if and only if its compressions to one-dimensional subspaces are.

	Now the trick is that nested compressions work nicely: if $W' \subset W \subset V$ and $A \in \L(V)$, then $\arestr{(\arestr{A}{W})}{W'} = \arestr{A}{W'}$. Consequently, if every one-dimensional compression $A$ is positive, same is true for all of its compressions.

	By compressing to eigenspaces, we see that if $A$ is positive, all its eigenvalues are non-negative.
\end{proof}

In addition, (direct) sum of two positive maps is positive.
\begin{lem}\label{direct_product}
	Let $A_{1} \in \L(V_{1})$ and $A_{2} \in \L(V_{2})$. Then $A_{1} \oplus A_{2} \in \H_{+}(V_{1} \oplus V_{2})$, if and only if $A_{1} \in \H_{+}(V_{1})$ and $A_{2} \in \H_{+}(V_{2})$.
\end{lem}
\begin{proof}
	Recall that one defines $\langle(v_{1}, v_{2}), (w_{1}, w_{2}) \rangle_{V_{1} \oplus V_{2}} = \langle v_{1}, w_{1} \rangle_{V_{1}} + \langle w_{2}, w_{2} \rangle_{V_{2}}$. Now clearly
	\begin{align*}
	\langle (A_{1} \oplus A_{2})(v_{1}, v_{2}), (v_{1}, v_{2}) \rangle_{V_{1} \oplus V_{2}} =  \langle A_{1} v_{1}, v_{1} \rangle_{V_{1}} + \langle A_{2} v_{2}, v_{2} \rangle_{V_{2}} \geq 0
	\end{align*}
	for every $(v_{1}, v_{2}) \in V_{1} \oplus V_{2}$, if and only if both $\langle A_{1} v_{1}, v_{1} \rangle_{V_{1}} \geq 0$ for every $v_{1} \in V_{1}$ and $\langle A_{2} v_{2}, v_{2} \rangle_{V_{2}} \geq 0$ for every $v_{2} \in V_{2}$.
\end{proof}

\section{Spectral theorem}

The most important result in the theory of positive and real maps is the spectral theorem.

\begin{lause}[Spectral theorem, version $1$]\label{cheapSpectral}
	$A \in \L(V)$ is real, if and only there exists real numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$ and pairwise orthogonal vectors $v_{1}, v_{2}, \ldots, v_{n} \in V$ such that
	\begin{align}\label{spectralrepr}
		A = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}}.
	\end{align}
\end{lause}
\begin{proof}
	We first prove the theorem for positive maps.

	We already proved one direction: every map of the previous form is positive.

	The other direction is tricky. The idea is to somehow find the vectors $v_{i}$. The problem is that such representation is by no means unique. If $A$ is any projection for instance, we could let $v_{i}$'s be any orthonormal basis of the corresponding subspace (and $\lambda_{i}$'s be all equal to one). There's no vector one has to choose.

	But we can think in reverse: there could be many vectors we cannot choose, depending on the map $A$. If $A$ is any non-identity projection to subspace $W$, say, we can only choose $v_{i}$'s in $W$ itself. Indeed, if $x \in W^{\perp}$, we have $A x = 0$, and hence $\langle A x, x \rangle = 0$. By comparing the quadratic form it follows $\langle P_{v_{i}} x, x \rangle = |\langle v_{i}, x \rangle|^{2}$ for any $1 \leq i \leq m$. But this means that $v_{i} \perp W^{\perp}$ and hence $v_{i} \in W$.

	More generally, if it so happens that for some $v \in V$ we have $\langle A v, v \rangle = 0$, we must have $v_{i} \perp v$ for any $1 \leq i \leq m$. But this means that were there such representation, we should have the following.

	\begin{lem}\label{spectral_zero_lemma}
		If $A \in \H_{+}(V)$ and $\langle A v, v \rangle = 0$ for some $v \in V$, then $A v = 0$ and $A w \perp v$ for any $w \perp v$.
	\end{lem}

	Before proving this lemma, we use it to complete the proof of Theorem \ref{cheapSpectral}.

	Proof is by induction on $n$, the dimension of the space. If $n = 0$, the claim is evident. For induction step assume first that there exists $v \in V$ such that $\langle A v, v \rangle = 0$. Then by the lemma for any $w \in \vspan(v)^{\perp}$ we have $A w \in \vspan(v)^{\perp} =: W$. But this means that $A = \arestr{A}{W} \oplus 0$. Now $\arestr{A}{W}$ is also positive, and $\dim(W) < n$, so by the induction assumption we have numbers $\lambda_{i}$ and vectors $v_{i} \in V$ for the map $\arestr{A}{W}$. Such representation for $\arestr{A}{W}$ immediately gives one also for $A$.

	We just have to get rid of the extra assumption on the existence of such $v$. But for this, note that if $\lambda = \inf_{|v| = 1} \langle A v, v \rangle$, one may consider $B = A - \lambda I$. Now $\inf_{|v| = 1} \langle B v, v \rangle = 0$, and $B$ is hence positive. Also, by compactness, the infimum is attained at some point $v$, so $B$ satisfies our assumptions. Representation for $B$ easily gives one for $A$.

	Note that the previous trick also covers the case of a general real map.
\end{proof}

\begin{proof}[Proof of Lemma \ref{spectral_zero_lemma}]
	Take any $w \in V$. By assumption for any $c \in \C$ we have
	\begin{align*}
		\langle A (c v + w), c v + w \rangle = c \langle A v, w \rangle + \overline{c} \langle A w, v \rangle + \langle A w, w \rangle \geq 0
	\end{align*}
	But this easily implies that $\langle A v, w \rangle = 0 = \langle A w, v \rangle$ for any $w \in V$. The first equality implies that $A v = 0$ and the second that $A w \perp v$ for any $w \in V$.
\end{proof}

One may interpret that the spectral theorem is saying that \ref{direct_product} is essentially the only way to build real maps from identity. In the representation \ref{spectralrepr} the numbers $\lambda_{i}$ are evidently the eigenvalues of $A$ and vectors $v_{i}$ the corresponding eigenvectors; this is why we call it the \textbf{Spectral representation}. While the representation is not unique, there is a way to make it unique. For this we have to change $v_{i}$ to corresponding eigenspaces.

\begin{lause}[Spectral theorem, version $2$]
	Let $A \in \H(V)$. Then there exists unique non-negative integer $m$, distinct real numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}$ and (non-trivial) subspaces of $V$, $E_{\lambda_{1}}, E_{\lambda_{2}}, \ldots E_{\lambda_{m}}$ with $V = \bigoplus_{i = 1}^{m} E_{i}$, such that
	\begin{align}\label{spectralrepr2}
		A = \sum_{i = 1}^{m} \lambda_{i} P_{E_{\lambda_{i}}}.
	\end{align}
	Moreover, this representation is unique.
\end{lause}
\begin{proof}
	Existence of such representation immediately follows from the previous form of spectral theorem. For uniqueness, note that $\lambda_{i}$'s are necessarily the eigenvalues of $A$ and $E_{\lambda_{i}}$'s the corresponding eigenspaces.
\end{proof}

Although the latter version is definitely of theoretical importance, we will mostly stick to the former, as it only contains one-dimensional projections and is thus easier to work with.

Spectral representation makes many of the properties of real maps obvious. For instance, any power of a real map is real: indeed, if $A = \sum_{1 \leq i \leq n} \lambda_{i} P_{v_{i}}$, then
\begin{align*}
	A^{2} = \left(\sum_{i = 1} \lambda_{i} P_{v_{i}}\right) \left(\sum_{j = 1} \lambda_{j} P_{v_{j}}\right) = \sum_{i = 1}^{n} \sum_{j = 1}^{n}\lambda_{i} \lambda_{j} P_{v_{i}} P_{v_{j}} = \sum_{i = 1}^{n} \lambda_{i}^2 P_{v_{i}},
\end{align*}
since $P_{v} P_{w} = 0$ for $v \perp w$. By induction one can extend the previous to higher powers. In other words: eigenspaces are preserved under powers, and eigenvalues are the ones to get powered up. From the original definition of a real map this is not all that clear. One could even extend to polynomials. If $p(x) = c_{k} x^{k} + c_{k - 1} x^{k - 1} + \ldots c_{1} x + c_{0}$, with $c_{i} \in \R$, we should write
\begin{align*}
	p(A) = c_{k} A^{k} + c_{k - 1} A^{k - 1} + \ldots c_{1} A + c_{0} = \sum_{1 \leq i \leq k} p(\lambda_{i}) P_{v_{i}}.
\end{align*}
This implies that if $p$ is the characteristic polynomial of $A$, then $p(A) = 0$: the special case of the Cayley--Hamilton theorem. Moreover, the minimal polynomial of $A$ is the polynomial with the eigenvalues of $A$ as single roots.

But even better, if $p$ is polynomial with all except one, say $\lambda_{i}$, of the eigenvalues of $A$ as roots, then $p(A) = p(\lambda_{i}) P_{E_{\lambda_{i}}}$. In particular, the projections to eigenspaces of $A$ are actually polynomials of $A$.

Fix $A \in \H(V)$ with eigenbasis $(v_{i})_{i = 1}^{n}$. For any $v \in V$ we have $A v = \sum_{1 \leq i \leq n} \lambda_{i} \langle v, v_{i} \rangle v_{i}$, so for instance

\begin{itemize}
\item $\langle A v, v \rangle = \sum_{1 \leq i \leq n} \lambda_{i} |\langle v, v_{i} \rangle|^{2}$: the quadratic form is just a positive linear combination of eigenvalues.
\item $\|A v\|^{2} = \sum_{1 \leq i \leq n} \lambda_{i}^{2} |\langle v, v_{i} \rangle|^{2} \leq \left(\max_{1 \leq i \leq n} \lambda_{i}^2 \right) \|v\|^{2}$. Hence $\|A\| = \max_{1 \leq i \leq n} |\lambda_{i}|$.
\end{itemize}

\section{Matrix functions}

\subsection{Functional calculus}

Given the spectral theorem, it is rather clear how one should define general matrix functions. We denote by $\H_{(a, b)}$ the elements of $\H$, the spectra of which lie in $(a, b)$.

\begin{maar}
	For any $f : (a, b) \to \R$ the \textbf{matrix function lift} of $f$ to $V$ is the map $f_{V} : \H_{(a, b)}(V) \to \H(V)$ given by
	\begin{align*}
		f_{V}\left(A\right) = \sum_{\lambda \in \spec(A)} f(\lambda) P_{E_{\lambda}}
	\end{align*}
	if $A = \sum_{\lambda \in \spec(A)} \lambda P_{E_{\lambda}}$.
\end{maar}
Note that as the spectral representation is unique this definition makes sense. Matrix functions enjoy many natural and useful properties.

\begin{prop}\label{basic_matrix}
	Let $f : (a,b) \to \R$ and $A \in \H_{(a, b)}$
	\begin{enumerate}
		\item If $f[(a, b)] \subset (c, d)$ then $f_{V}(A) \in \H_{(c, d)}$.
		\item If also $g : (a, b) \to \R$ then $(f + g)_{V} = f_{V} + g_{V}$ and $(fg)_{V} = f_{V}g_{V}$.
		\item $f_{V_{1} \oplus V_{2}} = f_{V_{1}} \oplus f_{V_{2}}$.
		\item If $g : (a, b) \to \R$ and $f$ and $g$ agree on spectrum of $A$, then $f(A) = g(A)$.
		\item If $f[(a, b)] \subset (c, d)$ and $g : (c, d) \to \R$ then $(g \circ f)_{V} = g_{V} \circ f_{V}$.
		\item If $f_{n} : (a, b) \to \R$ converge pointwise to $f$, then the same holds true for $(f_{n})_{V}$'s.
	\end{enumerate}
\end{prop}

These properties make it clear that such definition is natural. We will drop the subscript $V$ and identify $f$ with its lift $f_{V}$ if there is no fear of confusion.

There is one more property which is not all that trivial.

\begin{prop}\label{matrix_continuous}
	If $f : (a, b) \to \R$ is continuous, then so is $f_{V}$.
\end{prop}

Ultimately this is a statement about eigenvalue dynamics: if two real maps are close to each other, so are their eigenvalues.
\begin{lem}\label{eig_lem1}
	For any $A, H \in \H$ we have $\spec(A + H) \subset \spec(A) + [-\|H\|, \|H\|]$.
\end{lem}
\begin{proof}
	By a suitable translation the claim is reduced to the following: if all eigenvalues of $A$ are greater than $\|H\|$ in absolute value, then $A + H$ is invertible. Note that in this case the eigenvalues of $A^{-1}$ are less than $\|H\|^{-1}$ in absolute value and hence $\|A^{-1} H\| \leq \|A^{-1}\| \|H\| < 1$. It follows that all the eigenvalues of $A^{-1} H$ are less than $1$ in absolute value, so $I + A^{-1} H$ is invertible: hence is also $A (I + A^{-1} H) = A + H$.
\end{proof}

Note that the previous lemma implies that $\H_{(a, b)}$ or more generally $\H_{U}$ is an open set (in $\H)$ for any open $U \subset \R$, where $\H_{U}$ is defined as one would expect.

\begin{proof}[Proof of proposition \ref{matrix_continuous}]
	By Lemma \ref{eig_lem1} $f_{V}$ is clearly continuous at $A \in \H_{(a, b)}$ at least if $f(A) = 0$. But if this is not the case, we may interpolate $f_{V}$ by a polynomial: find a polynomial $p$ with $p(\lambda) = f(\lambda)$ for $\lambda \in \spec(A)$ and write $f = p + g$. Now $g_{V}$ is continuous at $A$. Also, as
	\begin{align*}
		(A + H)^{k} = A^{k} + O(\|H\|)
	\end{align*}
	for any $k \geq 0$, so is $p_{V}$ and hence also $f_{V}$.
\end{proof}

Previous argument also implies a bit more general claim: $f_{V}$ is continuous at $A$, if and only if $f$ is continuous at $\spec(A)$.

\subsection{Holomorphic functional calculus}

If $f$ is entire, there's another way to appoach the concept of matrix functions. Since $f$ can be written as
\begin{align*}
	f(z) = \sum_{n = 0}^{\infty} a_{n} z^{n},
\end{align*}
power series convergent for any $z \in \C$, we should have
\begin{align*}
	f_{V}(A) = \sum_{n = 0}^{\infty} a_{n} A^{n}.
\end{align*}
This matrix power series indeed converges as $\|A^{n}\| \leq \|A\|^{n}$. Also, this definition coincides with the spectral one. Indeed, if one writes $f_{n} : z \mapsto  \sum_{k = 0}^{n} a_{k} z^{k}$, we have
\begin{align*}
	\sum_{n = 0}^{\infty}a_{n} A^{n} = \lim_{n \to \infty} \left[(f_{n})_{V}(A) \right] = f_{V}(A)
\end{align*}
by part (6) of proposition (\ref{basic_matrix}).

Note that the power series definition makes perfect sense even if $a_{n} \notin \R$ or if $A$ is not real.

If $f$ is not entire, the power series might not converge for every $A \in \H_{(a, b)}$. Instead, we can use Cauchy's integral formula for matrix functions. One has
\begin{align*}
	f_{V}(A) = \frac{1}{2 \pi i}\int_{\gamma} (z I - A)^{-1} f(z) dz,
\end{align*}
where $\gamma$ is simple closed curve enclosing the spectrum of $A$ (contained in the domain of $f$). This formula is immediate when viewed in a eigenbasis of $A$ and again, the formula makes perfect sense even if $A$ is not real.

\section{Real maps and composition}

\subsection{Commuting real maps}

\textbf{Warning!} Composition of positive maps need not be positive!

Since for any $A, B \in \H(V)$ we have $(A B)^{*} = B^{*} A^{*} = B A$, product of two real maps is real, if and only if the maps commute. So: when do two real maps commute?

It turns out that real maps commute, if and only if they are \textbf{simultaneously diagonalizable}, i.e. if there exists vectors $v_{1}, v_{2}, \ldots, v_{n}$ and numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$ and $\lambda'_{1}, \lambda'_{2}, \ldots, \lambda'_{n}$ such that
\begin{align*}
	A = \sum_{1 \leq i \leq n} \lambda_{i} P_{v_{i}} \; \text{ and } \; B = \sum_{1 \leq i \leq n} \lambda'_{i} P_{v_{i}}.
\end{align*}

A similar statement holds for arbitrary families of commuting real maps.

\begin{lause}\label{commuting_real_maps}
	Let $\mathcal{A} = (A_{j})_{j \in J}$ be an arbitrary family of commuting real maps. Then there exists $m \geq 1$ and a decomposition $V = \bigoplus_{i = 1}^{m} E_{i}$, such that
	\begin{align*}
		\mathcal{A} \subset \vspan \{P_{E_{i}} | 1 \leq i \leq m \}.
	\end{align*}
\end{lause}
\begin{proof}
	As with the spectral theorem, the main difficulty is finding the ``eigenspaces" $E_{i}$. Trick is the following: recall that we may find eigenprojections of a real map as certain polynomials of the map itself. This motivates us to investigate all multivariate polynomials of maps in $\mathcal{A}$: these maps should include also the maps $P_{E_{i}}$. So let $\mathcal{A}' \subset \L(V)$ be the smallest (unital) $\R$-algebra containing $\mathcal{A}$. Note that $\mathcal{A}'$ is commutative and elements of it are real maps. As $\mathcal{A}'$ is finite dimensional and closed under taking polynomials, it is spanned by projections to a finite set of subspaces of $V$. To find the subspaces $E_{i}$, we should find a spanning set of projections which is minimal in some sense. It turns out one minimizing $\sum \dim(E_{i})$ works. If we manage to prove that in such minimal spanning set the subspaces $E_{i}$ are orthogonal, we are done, as the respective projections also span $I$ and the family $\mathcal{A}$.

	Note that for any $1 \leq i < j \leq m$ the map $A := P_{E_{i}} P_{E_{j}}$ is a projection, as it is real ($P_{E_{i}}$ and $P_{E_{j}}$ commute) and $A^{2} = P_{E_{i}} P_{E_{j}} P_{E_{i}} P_{E_{j}} = P_{E_{i}}^2 P_{E_{j}}^2 = P_{E_{i}} P_{E_{j}} = A$. But as $\image(A) \subset E_{i}, E_{j}$, the maps $P_{E_{0}} := A, P_{E_{i}'} := P_{E_{i}} - A$ and $P_{E_{j}'} := P_{E_{j}} - A$, are projections in $\mathcal{A}'$ spanning $P_{E_{i}}$ and $P_{E_{j}}$. Since $\dim(E_{0}) + \dim(E_{i}') + \dim(E_{j}') = \dim(E_{i}) + \dim(E_{j}) - \dim(E_{0})$, by the minimality of $(E_{i})_{i = 1}^{m}$ we must have $\dim(E_{0}) = 0$. Indeed, otherwise we could replace $E_{i}$ and $E_{j}$ by $E_{0}$, $E_{i}'$ and $E_{j}'$. Consequently $P_{E_{i}} P_{E_{j}} = 0$, and hence $E_{i} \perp E_{j}$.
\end{proof}

It is not very hard to see that the decomposition with minimal $m \geq 1$ is unique and attained by the previous construction.

The message is: if one wants products to preserve positivity, everything degenerates to $\R^{m}$, i.e. diagonal maps.

\begin{phil}
	Commutativity kills the exciting phenomena.
\end{phil}

Conversely, if one wants exciting things to happen, one should make things very non-commutative.

As a corollary to Theorem \ref{commuting_real_maps} we have

\begin{kor}
	If $A, B \geq 0$ and $A$ and $B$ commute, then $AB \geq 0$.
\end{kor}

\subsection{Symmetric product}

As normal product doesn't quite work with positivity, next attempt might be symmetrized product
\begin{align*}
	S(A, B) = AB + BA,
\end{align*}
(or maybe with normalizing constant $\frac{1}{2}$ in the front), instead of the usual one. It turns out that even this doesn't fix positivity.

For one-dimensional projections things go as badly as they possibly can.

\begin{prop}\label{symmetric_projection}
	If $v, w \in V \setminus \{0\}$, then
	\begin{align*}
		P_{v} P_{w} + P_{w} P_{v} \geq 0,
	\end{align*}
	if and only if $v$ and $w$ are parallel or orthogonal, i.e. if and only if $P_{v}$ and $P_{w}$ commute.
\end{prop}
\begin{proof}
	Since everything is happening in a (at most) two-dimensional subspace of $V$, we may assume that $V$ is two-dimensional in the first place. Note that
	\begin{align*}
		P_{v} P_{w} + P_{w} P_{v} = (P_{v} + P_{w})^2 - P_{v}^2 - P_{w}^2 = (P_{v} + P_{w})^2 - P_{v} - P_{w} = A^2 - A = A (A - I),
	\end{align*}
	where $A := P_{v} + P_{w}$. This is positive, if and only if the eigenvalues of $A$ lie outside the interval $(0, 1)$. But since $\tr(A) = 2$ and $A \geq 0$, the only way this can happen is that either $A$ has double eigenvalue $1$ or $A$ has eigenvalues $0$ and $2$. To conclude the claim itself, we are left to do two reality checks:
	\begin{lem}
		If $A = P_{v} + P_{w} = I$, then $v$ and $w$ are orthogonal.
	\end{lem}
	\begin{proof}
		Note that $\langle v, v \rangle = \langle A v, v \rangle = \langle P_{v} v, v \rangle + \langle P_{w} v, v \rangle = \langle v, v \rangle + |\langle v, w \rangle|^2/\langle v, v\rangle$, so $\langle v, w \rangle = 0$.
	\end{proof}
	\begin{lem}
		If $A = P_{v} + P_{w} = 2 P_{u}$ for some $u \in V$, then $v, w$ and $u$ are all parallel.
	\end{lem}
	\begin{proof}
		Take $u' \in (u)^{\perp}$. Since $0 = \langle 2 P_{u} u', u' \rangle =  \langle P_{v} u', u' \rangle +  \langle P_{w} u', u' \rangle = |\langle v, u' \rangle|^2/\langle v, v\rangle + |\langle w, u' \rangle|^2/\langle w, w\rangle \geq 0$, we have $\langle v, u' \rangle = 0 = \langle w, u' \rangle$. Consequently $v, u \in ((u)^{\perp})^{\perp} = (u)$.
	\end{proof}
\end{proof}

For more general positive maps things aren't much better. One could for instance prove that

\begin{prop}
	Let $A \in \H$ such that $A B + B A \geq 0$ for any $B \geq 0$. Then $A = \alpha I$ for some $\alpha \geq 0$.
\end{prop}

\subsection{$*$-conjugation}

Despite all the negative news, there's one non-trivial non-commutative way to produce positive maps from others, called $*$-conjugation. Given any two positive maps $A$ and $B$, their composition need not be positive, but the map $BAB$ is. First of all, it is real as $(BAB)^{*} = B^{*} A^{*} B^{*} = BAB$. Also $\langle BAB v, v \rangle = \langle A (B v), (B v) \rangle \geq 0$ for any $v \in V$. An identical argument shows that one may replace $B$ by an arbitrary $C \in \L(V)$ in the following sense.

\begin{maar}
	Let $A, B \in \H$. We say that $B$ is \textbf{$*$-conjugate} of $A$ if for some $C \in \L(V)$ we have $B = C^{*} A C$.
\end{maar}

\begin{prop}
	If $A \geq 0$ and $B$ is $*$-conjugate of $A$, then also $B \geq 0$.
\end{prop}

\section{Loewner order}

\begin{maar}
	If $A, B \in \H(V)$, we write that $A \leq B$ if $B - A \geq 0$. If $B - A > 0$, we write $A < B$.
\end{maar}

Proposition \ref{basic_positive} tells us that $\leq$, is indeed a partial order on the $\R$-vector space of real maps: this partial order is called \textbf{Loewner order}. More explicitly, we have the following properties:

\begin{prop}\label{basic_loewner}
	\begin{enumerate}[(i)]
		\item If $A \leq B$ then $\alpha A \leq \alpha B$ for any $\alpha \geq 0$.
		\item If $A \leq B$ and $B \leq C$ then $A \leq C$.
		\item If $A \leq B$ and $B \leq A$ then $A = B$.
		\item $\lambda I \leq A$, if and only if all the eigenvalues of $A$ are at least $\lambda$. Similarly $A \leq \lambda I$, if and only if all the eigenvalues of $A$ are at most $\lambda$.
	\end{enumerate}
\end{prop}

\begin{esim}\label{projection_order}
	If $W_{1}, W_{2} \subset V$ are two subspaces of $V$ we have $P_{W_{1}} \leq P_{W_{2}}$ if and only if $W_{1} \subset W_{2}$. Indeed if $W_{1} \subset W_{2}$ then $W_{2} = W_{1} \oplus W_{3}$ for some $W_{3} \subset V$ and hence $P_{W_{2}} = P_{W_{1}} + P_{W_{3}} \geq P_{W_{1}}$. Converse follows as soon as one notes that by \ref{spectral_zero_lemma} for any $W \subset V$ we have
	\begin{align*}
		\{v \in V | \langle P_{W} v, v\rangle = 0\} = W^{\perp}.
	\end{align*}
\end{esim}

Key thing here is to note that multiplication by positive map doesn't preserve Loewner order. This is the reason why many standard arguments don't work for general real maps.

For example if $0 < a \leq b$, with real numbers one could multiply the inequality by the positive number $(a b)^{-1}$ to get $0 < b^{-1} \leq a^{-1}$. This doesn't quite work with linear maps anymore.

$*$-conjugation is a way to partially fix this deficiency: it works almost like multiplying by a positive number.

\begin{prop}
	If $A \leq B$, then for any $C \in \L(V)$ we have $C^{*} A C \leq C^{*} B C$.
\end{prop}

Using the previous observation one can salvage the above argument.

\begin{lause}\label{inverse_decreasing}
	If $0 < A \leq B$, then $B^{-1} \leq A^{-1}$.
\end{lause}
\begin{proof}
	As mentioned, we can't really multiply by $(A B)^{-1}$, as it does not preserve the order and doesn't even need to be positive. We can almost multiply by $A^{-1}$ though: $*$-conjugate by $A^{-\frac{1}{2}}$. This preserves the order, and we get
	\begin{align*}
		I \leq A^{-\frac{1}{2}} B A^{-\frac{1}{2}}.
	\end{align*}
	Now, one would want to multiply by $B^{-1}$, that is $*$-conjugate by $B^{-\frac{1}{2}}$, but $B$ is in the middle, so that doesn't seem be too helpful. But we can continue with the original strategy instead: since $I \leq X := A^{-\frac{1}{2}} B A^{-\frac{1}{2}}$ we have $X^{-1} \leq I$ (by \ref{basic_loewner} $(iv)$), that is
	\begin{align*}
		A^{\frac{1}{2}} B^{-1} A^{\frac{1}{2}} \leq I.
	\end{align*}
	Now simply $*$-conjugate by $A^{-\frac{1}{2}}$.
\end{proof}

\begin{huom}
Alternatively, as the middle step we could conjugate both sides by $X^{-\frac{1}{2}}$. By doing this we have only used $*$-conjugation in the proof: actually we have $*$-conjugated altogether with 
\begin{align*}
	A^{-\frac{1}{2}} (A^{-\frac{1}{2}}B A^{-\frac{1}{2}})^{-\frac{1}{2}} A^{-\frac{1}{2}} = (A^{\frac{1}{2}} (A^{-\frac{1}{2}}B A^{-\frac{1}{2}})^{\frac{1}{2}} A^{\frac{1}{2}})^{-1}.
\end{align*}
The map $A^{\frac{1}{2}} (A^{-\frac{1}{2}}B A^{-\frac{1}{2}})^{\frac{1}{2}} A^{\frac{1}{2}}$, which is real, is usually called the geometric mean of $A$ and $B$. The point is: somewhat curiously we can almost do the original proof: just replace multiplication by $*$-conjugation by square root, and replace square root of the product by geometric mean.
\end{huom}

\section{Notes and references}

All results in this chapter are classic and can be found in numerous books on linear algebra, for instance in \cite{Bhatia2}, \cite{Bhatia} and \cite{Zhang}.

\begin{comment}

\section{Ideas}

\begin{itemize}
	\item Normal maps
	\item Square root of a matrix
	\item Ellipses map to ellipses
	\item adjoints of vectors
	\item Moore-Penrose pseudoinverse
	\item (canonical, löwdin) orthogonalization, polar decomposition and orthogonal Procrustes problem
	\item projection matrices
	\item Hilbert-Schmidt norm ($\to$ matrix functions?) and inner product
	\item Hilbert spaces
	\item Real vs. complex
	\item Positive definite kernels
	\item Weakly positive matrices
	\item Hlawka inequality for determinant %http://mathoverflow.net/questions/182181/hlawka-inequality-for-determinants-of-positive-definite-matrices?rq=1
	\item Trace-characterization of positive maps.
	\item Splitting positive maps to pseudo square roots
	\item Product of maps
	\item Exponential formula for geometric mean?
	\item Maximum of matrices with powerlimit
	\item If $A, B$ are Hermitian, what eigenvalues $AB$ can have? What if the eigenvalues are known? What about $AB + BA$. What eigenvalues $A$ can have if eigenvalues of $\Re(A)$ are known.
	\item It seems to be the case that if $n = 2$, and $A$ is Hermitian with $\spec(A) = \{\lambda_{1}, \lambda_{2}\}$ ($\lambda_{1} \leq \lambda_{2}$), then there exists linear $B$ such that $\Re(B) = A$, and $\spec(B) = \{\mu_{1}, \mu_{2} \}$ if and only if $\Re(\mu_{1} + \mu_{2}) = \lambda_{1} + \lambda_{2}$ and $\lambda_{1} \leq \Re(\mu_{i}) \leq \lambda_{2}$. In general this is known as Ky-Fan theorem, according to \cite{Ando3}.
	\item Let's define $A \leq_{2} B$ if $\tr(A) = \tr(B)$ and for any $t \in \R$ we have $\tr(|A - t I|) \leq \tr(|B - t I|)$. Similarly we can define $A \leq_{k} B$. This easily (?) defines a partial order on matrices. But know we lose all the data about the eigenvectors? Is there a way to bring it back? Is there some nice interpretation.
	\item One would like to get such order with restrictions. Maybe this is related to sectional curvature.
	\item What happens if $n = 2$, $A, B \in \H$, and $\tr(B) = 0$ and $\tr(AB) \geq 0$. What can be said about the relation between $A$ and $A + B$.
	\item We have up to first order that if $\lambda_{1} \geq \lambda_{2} \geq \ldots \geq \lambda_{n}$ are the eigenvalues of $A$, with respective eigenvectors $v_{i}$, then we should have
	\begin{align*}
		\sum_{i = 1}^{k} \langle \dot{A} v_{i}, v_{i}\rangle \geq 0,
	\end{align*}
	for any $1 \leq k \leq n$ with equality for $k = n$.
	\item Is the right condition something like: for any $t \in \R$ we should have $A \cdot \chi_{(t, \infty)} \leq B \cdot \chi_{(t, \infty)}$ or something like that.
	\item Does the following work? We say that $A \leq_{2} B$ if for any orthonormal basis $(e_{i})_{i = 1}^{n}$ we have
	\begin{align*}
		(\langle A e_{i}, e_{i} \rangle)_{i = 1}^{n} \prec_{2} (\langle B e_{i}, e_{i} \rangle)_{i = 1}^{n}.
	\end{align*}
	Does this correspond to the case $n = 1$? This probably doesn't work: if $n = 2$, $\tr(A) = \tr(B) = 0$ and $e_{1}$ is in Kernel of $B$, then the right-hand sequence is zero sequence.
	\item Lorenz order?
	\item BMV-conjecture (theorem)
	\item Proof difficulties
	\item Proof ``sketch" (as in joke)
	\item Positive linear functions $\H \to \R$.
	\item What about positive linear functionals form $\H^{n} \to \H^{m}$?
	\item Power series for positivity of inverse function.
	\item Two notions of positivity: spectral and quadratic form. First works well with functional calculus and second with linear phenomena, but one shouldn't mix these two things.
\end{itemize}

\end{comment}




