\chapter{Positive matrices}

This chapter is titled ``positive matrices", although ``positive maps" might be more appropriate title. We are mostly going to deal with finite-dimensional objects, but many of the ideas could be generalized infinite-dimensional settings, where matrices lose their edge. Also, one should always ask whether it really clarifies the situation to introduce concrete matrices: matrices are good at hiding the truly important properties of linear mappings. The words ``matrix" and ``linear map" are used somewhat synonymously, although one should always remember that the former are just special representations for the latter.

\section{Motivation}

How should one order matrices? What should we require from ordering anyway?

We would definitely like to have natural total order on the space of matrices, but it turns out there are no natural choices for that. Partial order is the next best thing. Recall that a partial order on a set $X$ is a binary relation $\leq$ on such that
\begin{enumerate}
	\item $x \leq x$ for any $x \in X$.
	\item For any $x, y \in X$ for which $x \leq y$ and $y \leq x$, necessarily $x = y$.
	\item If for some $x, y, z \in X$ we have both $x \leq y$ and $y \leq z$, also $x \leq z$.
\end{enumerate}

The third point is the main point, the first two are just there preventing us from doing something crazy. But we can do better: this partial order on matrices should also respect addition.
\begin{enumerate}
\item[4.] For any $x, y, z \in X$ such that $x \leq y$, we should also have $x + z \leq y + z$.
\end{enumerate}

There's another way to think about this last point. Instead of specifying order among all the pairs, we just say which matrices are positive: matrix is positive if and only it's at least $0$.

If we know all the positive matrices, we know all the ``orderings". To figure out whether $x \leq y$, we just check whether $0 = x - x \leq y - x$, i.e. whether $y - x$ is positive. Also, positive matrices are just differences of the form $y - x$ where $x \leq y$. Now, conditions on the partial order are reflected to the set of positive matrices.
\begin{enumerate}
	\item[1'.] $0$ (zero matrix) is positive.
	\item[2'.] If both $x$ and $-x$ are positive, then $x = 0$.
	\item[3'.] If both $x$ and $y$ are positive, so is their sum $x + y$.
\end{enumerate}
Here $3'$ is kind of combination of $3$ and $4$.

The terminology here is rather unfortunate. Natural ordering of the reals satisfies all of the above with obvious interpretation of positive numbers, which however differs from the standard definition: $0$ is itself positive in our above definition. This is undoubtedly confusing, but what can you do? For real numbers we have total order, so every number is either zero, strictly positive or strictly negative, so when we say non-negative, it literally means ``not negative": we get all the positive numbers and zero. But with partial orders we might get more. So the main reasons why we are using this terminology are
\begin{enumerate}
	\item It's short.
\end{enumerate}
Also, now that we have decided to preserve the word ``positive" for ``at least zero" one might be tempted to preserve ``strictly positive" for ``at least zero, but not zero". We won't do that, we save that phrase for something more important.

To figure out a correct notion for positive maps, let's start simple. If we are in a $1$-dimensional vector space $V$ over $\R$ there's rather canonical choice for positivity. Any linear map is of the form $v \mapsto a v$ for some $a \in \R$ and we should obviously say that a map is positive if $a \geq 0$ (note our non-standard terminology concerning positivity). More generally, if a map if scalar multiple of identity, map should be positive if and only if the corresponding scalar is non-negative.

Natural extension of this idea could be try the following: map is positive if all of its eigenvalues are non-negative. Of course, this doesn't quite work: not every map has real eigenvalues. But even if we restrict to maps with real eigenvalues, this property is not preserved in addition. Consider for example the pair
\[
	\begin{bmatrix}
		0 & 4 \\
		1 & 0
	\end{bmatrix}
	\text{ and }
	\begin{bmatrix}
		0 & -1 \\
		-4 & 0
	\end{bmatrix}
\]
The two matrices have both distinct eigenvalues $- 2$ and $2$ and are hence diagonlizable, but their sum has characteristic polynomial $x^2 + 9$, which most definitely has no real zeros. In general one should not except summation and eigenvalues go very well together.

\section{Real maps}

\subsection{Restricting linear maps}

There's however quite clever way to go around this. Instead of requiring non-negativity of eigenvalues, we require that map ``restricts" to positive map. The idea is: we already know which maps should be positive in one-dimensional spaces, or more generally, which scalar multiples of identity should be positive. Now we should require that when we restrict our look to one-dimensional subspaces, we should get a positive map.

Of course, one should first understand what restricting linear maps means. Usually if we have a linear map $A : V \to V$, we could take subspace $W \subset V$ and consider the usual restriction map $\restr{A}{W} : W \to V$ given by $\restr{A}{W}(w) = Aw$ for any $w \in W \subset V$. In other words $\restr{A}{W} = A \circ \incl{V}{W}$, where $\incl{V}{W}$ denotes the natural inclusion from $W$ to $V$. But this map is going to wrong space. Instead we would like to define something satisfying
\begin{itemize}
	\item Restriction is a linear map $\arestr{(\cdot)}{V, W} = \arestr{(\cdot)}{W} : \L(V) \to \L(W)$.
	\item If $A \in \L(V)$ and $A(W) \subset W$, restriction should coincide with the original map, in the sense that $A = \incl{V}{W} \circ \arestr{A}{W}$.
	\item If $W' \subset W \subset V$, we should have $\arestr{(\cdot)}{W'} = \arestr{(\arestr{(\cdot)}{W})}{W'}$.
\end{itemize}

These properties don't uniquely define a linear map but they say that $\arestr{A}{W}$ should be of the form $P_{V, W} \circ A \circ \incl{V}{W}$ where $P_{V, W}$ is a projection, i.e. a map for which $P_{V, W} \circ \incl{V}{W} = I_{W}$. Moreover, these projections should satisfy $P_{V, W'} = P_{W, W'} \circ P_{V, W}$.

If we are working in a inner-product space, there's rather natural choice for the map $P_{V, W}$: orthogonal projections. Orthogonal projections are projections with $\ker(P) = \image(P)^{\perp}$. Such maps are easily seen to satisfy all the requirements. Finally, we will call our new concept \textit{compression} instead of restriction, to distinguish between the two.

\begin{maar}
	Let $(V, \langle \cdot, \cdot \rangle )$ be an inner product space and $W \subset V$ a subset. We define the map $\arestr{A}{W}$, \textit{compression} of $A$ to $W$ to be the linear map given by $P_{W} \circ A \circ \incl{V}{W}$.
\end{maar}

\begin{lause}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an inner product space and $W \subset V$ subspace of $V$. Then the compression to $W$ is unique linear contraction from $\L(V)$ to $\L(W)$, such that for any $A \in L(V, W)$ we have $\arestr{(\incl{V}{W} \circ A)}{W} = A \circ \incl{V}{W}$. Moreover, if $W' \subset W$, we have $\arestr{(\cdot)}{W'} = \arestr{(\arestr{(\cdot)}{W})}{W'}$.
\end{lause}
\begin{proof}
	Trust me, it's true. Also, even if it's not, this theorem is clearly here just to convince the reader that orthogonal projections are the only sensible choice; but does that really need convincing?

	Okay, we'll come back to the proof.
\end{proof}

For one-dimensional compressions we have convenient representation. As one easily checks, one dimensional projection onto subspace spanned by vector $v$ is given by
\[
	P_{(v)} = \frac{\langle \cdot, v \rangle}{\langle v, v \rangle} v,
\]
as long as $v \neq 0$, and thus
\[
	\arestr{A}{(v)} = \frac{\langle A \cdot, v \rangle}{\langle v, v \rangle} v.
\]

If $w \in (v) \setminus \{0\}$, we could rewrite the previous in the form
\[
	\arestr{A}{(v)}(w) = \frac{\langle A v, v\rangle}{\langle v, v\rangle} w = \frac{\langle A w, w\rangle}{\langle w, w \rangle} w.
\]
This gives rise to so called Rayleigh quotient $R(A, \cdot) : V \setminus \{0\} \to \C$, given by
\[
	R(A, v) = \frac{\langle A v, v \rangle}{\langle v, v \rangle}.
\]
Compression in the direction of $v$ is given by scaling by the corresponding Rayleigh quotient.

We will call $\langle A v, v \rangle$ the quadratic form of $A$, and denote it by $Q_{A}(v)$.

There's one more important property of compression we need. When map is compressed to a subspace, we naturally lose some information about the map. Knowing about all of the compressions, however, we can get our map back.

\begin{lem}[Injectivity of compression]
	If $A, B \in \L(V)$, $A = B$ if and only if $\arestr{A}{W} = \arestr{B}{W}$ for any one-dimensional subspace $W \subset V$.
\end{lem} 
\begin{proof}
	By linearity, it is sufficient to prove that if $Q_{A}(v) = 0$ for any $v \in V$, then $A = 0$. TODO polarization identity.
\end{proof}

\subsection{Positive maps}

Now that we have defined the compression we are ready define positive maps.

\begin{maar}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an inner product space. We say that a map $A \in \L(V)$ is positive, and write $A \geq 0$, if for any one-dimensional subspace $W \subset V$ the map $\arestr{A}{W}$ is positive, i.e. is induced by a non-negative real.
\end{maar}

We denote the space of positive maps by $\H_{+}(V)$. Positive maps have the following useful properties.

\begin{prop}\label{basic_positive}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an inner product space over $\C$. Then
	\begin{enumerate}[(i)]
		\item $A \in \L(V)$ is positive if and only if $\arestr{A}{W}$ is positive for every subspace $W \subset V$.
		\item If $A, B \in \L(V)$ are positive and $\alpha, \beta \geq 0$, also $\alpha A + \beta B$ is positive.
		\item If $(A_{i})_{i = 1}^{\infty}$ are positive and $\lim_{i \to \infty} A_{i} = A$, also $A$ is positive.
		\item $A \in \L(V)$, $A$ is positive if and only for any $v \in V$, or for any $v \in V$ with $|v| = 1$ we have $\langle A v, v \rangle \geq 0$, or still equivalently, for any $v \in V \setminus \{0\}$ the Rayleigh quotient $R(A, v)$ is non-negative.
		\item If both $A$ and $-A$ are positive, then $A = 0$.
		\item If $A$ is positive, all of its eigenvalues are non-negative.
	\end{enumerate}
\end{prop}
\begin{proof}
	\begin{enumerate}[(i)]
		\item Other direction is immediate. Also if for any subspace $W \subset V$ take any one-dimensional $W' \subset W$. Now $\arestr{(\arestr{A}{W})}{W'} = \arestr{A}{W'}$, is positive by assumption, and so is $\arestr{A}{W}$.
		\item The claim evidently holds for one-dimensional spaces. Now for any one-dimesional $W \subset V$ we have $\arestr{(\alpha A + \beta B)}{W} = \alpha \arestr{A}{W} + \beta \arestr{B}{W} \geq 0$, by the one-dimensional case, so $\alpha A + \beta B \geq 0$.
		\item Again, the claim evidently holds for one-dimensional spaces. Now for any one-dimesional $W \subset V$ we have $\arestr{(\lim_{i \to \infty} A_{i})}{W} = \lim_{i \to \infty}\arestr{(A_{i})}{W}\geq 0$, by the one-dimensional case, so $A \geq 0$.
		\item These claims are immediate from our representation for one-dimensional compressions.
		\item If $A, -A \geq 0$, all the one-dimensional compressions of $A$ are both non-negative and non-positive, so zero. But by the injectivity of compression, it follows that $A = 0$.
		\item Note that if $v$ is any eigenvector of $v$, $\arestr{A}{(v)} w = \frac{\langle A v, v\rangle}{\langle v, v\rangle} w = \lambda w$, so by assumption $\lambda \geq 0$.
	\end{enumerate}
\end{proof}

The map $v \mapsto \langle A v, v \rangle$ is called the quadratic form of $A$, and is denoted by $Q_{A}$.

We can also lift other important notions.

\begin{maar}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an inner product space. We say that a map $A \in \L(V)$ is real, if for any one dimensional subspace $W \subset V$ the map $\arestr{A}{W}$ is real, i.e. is induced by real number.
\end{maar}

\begin{maar}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an inner product space. We say that a map $A \in \L(V)$ is imaginary, if for any one dimensional subspace $W \subset V$ the map $\arestr{A}{W}$ is imaginary, i.e. is induced by imaginary number.
\end{maar}

The previous two families of maps are usually called Hermitian and Skew-Hermitian and as with positive maps, many of their properties are lifted form usual complex numbers. Hermitian maps will have a special role in our discussion. They form a vector space over $\R$, which is denoted by $\H(V)$. Of course, every imaginary map is just $i$ times real map, and we won't preserve any special notation for such maps.

\subsection{Adjoint}

We can also lift the notion of complex conjugate. If $V$ is one-dimensional, $\overline{A}$, conjugate of $A$ should be a linear map which is induced by the complex conjugate of the scalar inducing $A$.

\begin{lause}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an inner product space. Then for any $A \in \L(V)$ there exists unique map $A^{*} \in \L(V)$, which we will call adjoint of $A$, for which for any one-dimensional subspace $W$ we have $\arestr{(A^{*})}{W} = \overline{\arestr{A}{W}}$.
\end{lause}
\begin{proof}
	The uniqueness of adjoint is immediate from the injectivity of compression. The map $(\cdot)^{*} : \L(V) \to \L(V)$ should evidently be conjugate linear, so for existence it suffices to find adjoint for suitable basis elements of $\L(V)$: the maps of the form $A = (x \mapsto \langle x, v \rangle w)$ for $v, w \in V$ will do.

	Note that the Rayleigh quotient for such map is given by
	\[
		R(A, x) = \frac{\langle x, v \rangle \langle w, x \rangle}{\langle x, x \rangle}.
	\]
	But if we define $A^{*} = (x \mapsto \langle x, w \rangle v)$, we definitely have
	\[
		R(A^{*}, x) = \frac{\langle x, w \rangle \langle v, x \rangle}{\langle x, x \rangle} = \overline{\frac{\langle w, x \rangle \langle x, w \rangle}{\langle x, x \rangle}} = \overline{R(A, x)}.
	\]
\end{proof}

Real maps are their own adjoints, and that is why they are often called \textit{self-adjoint}.

There is also a meaningful way to extend the notion of adjoint for general (non-endomorphism) linear maps. In general setting, we don't have a notion of compression of linear map: there's no canonical way to restrict the codomain. We can however interpret a map in a bigger space. Indeed, any map $A \in \L(V, W)$ can be canonically interpreted as a map $\tilde{A} \in \L(V \oplus W)$: define $\tilde{A}(v, w) = (0, A v)$. We call this map the \textit{symmetrization} of $A$. Now it makes sense to consider $(\tilde{A})^{*}$, the symmetrization has a unique adjoint. This adjoint does not in general live in $\L(V, W)$ anymore: but it turns out that it does live in $\L(W, V)$!

\begin{lause}
	For any $A \in \L(V, W)$ there exists a unique map $A^{*} \in \L(W, V)$ which we call the adjoint of $A$, such that $(\tilde{A})^{*} = \tilde{(A^{*})}$. Moreover, if $V = W$, the new notion of adjoint coincides with the old one.
\end{lause}

\begin{proof}
	Of course, strictly speaking $\tilde{A^{*}}$ would be map in $\L(W \oplus V)$, not in $\L(V \oplus W)$, but the two spaces are canonically isomorphic.

	The uniqueness follows from the already known uniqueness for the old notion. The map $(\cdot)^{*} : \L(V, W) \to \L(W, V)$ should again evidently be conjugate linear. Also, the same construction for basis elements of the form $A = (x \mapsto \langle x, v \rangle_{V} w)$ (where $v \in V$ and $w \in W$) works again. Indeed, for any $(x, y) \in V \oplus W$ the corresponding Rayleigh quotient is given by
	\[
		R(\tilde{A}, (x, y)) = \frac{\langle (0, A x), (x, y)\rangle_{V \oplus W} }{\langle (x, y), (x, y) \rangle_{V \oplus W}} = \frac{\langle A x, y\rangle_{W} }{\langle x, x \rangle_{V} + \langle y, y \rangle_{W}} = \frac{\langle x, v\rangle_{V} \langle w, y\rangle_{W} }{\langle x, x \rangle_{V} + \langle y, y \rangle_{W}},
	\]
	and it's clear that we may set $A^{*} = (x \mapsto \langle x, w \rangle_{W} v)$. Our construction also makes it clear that this notion coincides with the old one.
\end{proof}

The previous proof also gives a convenient corollary, which is the most common definition for adjoint.

\begin{kor}
	For any $A \in \L(V, W)$, the adjoint $A^{*} \in \L(W, V)$ is unique linear map such that for any $v \in V$ and $w \in W$ we have
	\[
		\langle A v, w \rangle_{W} = \langle v, A^{*} w \rangle_{V}.
	\]
	In particular, map $A \in \L(V)$ is real for any $v, w \in V$ we have
	\[
		\langle A v, w \rangle = \langle v, A w \rangle.
	\]
	and imaginary if for any $v, w \in V$
	\[
		\langle A v, w \rangle = -\langle v, A w \rangle.
	\]
\end{kor}

The previous corollary makes many of the basic properties of adjoint, which we collect below, evident.

\begin{lause}\label{basic_adjoint}
	For any linear maps $A$ and $B$, with appropriate domains and codomains, and $\lambda \in \C$ we have
	\begin{enumerate}[i)]
		\item Matrix of $A^{*}$ with respect to any orthonormal basis is conjugate transpose of matrix of $A$, i.e. $A^{*}_{i, j} = \overline{A_{j, i}}$.
		\item $(A^{*})^{*} = A$
		\item $(A + B)^{*} = A^{*} + B^{*}$
		\item $(\lambda I)^{*} = \overline{\lambda} I$
		\item $(AB)^{*} = B^{*}A^{*}$.
		\item $\kernel(A) = \image(A)^{\perp}$
	\end{enumerate}
\end{lause}

\subsection{Examples}

It's high time to have some examples.

Most obvious, although not very interesting, representatives of real/imaginary/positive maps are real/imaginary/positive scalar multiples of the identity. Projections are stereotypical examples of positive and hence real maps. \textcolor{red}{Projections, proper definition}. Indeed, one-dimensional projections are given by $A = (x \mapsto \langle x, v \rangle v)$ for some $v \in V$ with $|v| = 1$. For such maps $\langle A x, x \rangle = \langle x, v \rangle \langle v, x \rangle = |\langle x, v \rangle|^{2} \geq 0$.
Higher dimensional projections are simply sums of one-dimensional ones, so they are also positive and real. More generally one could take any positive linear combination of projections to get much more positive maps, and real linear combination of projections to get real maps.

As we earlier noticed, however, not every map with real eigenvalues is real, and not every map non-negative eigenvalues is positive. It turns out that the basis elements of the form $A = (x \mapsto \langle x, v \rangle w)$ are real if and only if $v$ and $w$ are real multiples of each other, or to be precise, if there exists $\alpha, \beta \in \R$, not both $0$, such that $\alpha v + \beta w = 0$. Indeed, by the corollary, the map is real if for any $x, y \in V$ we have
\[
	\langle x, v \rangle \langle w, x \rangle = \langle A x, y \rangle = \langle x, A y \rangle = \langle x, w \rangle \langle v, x \rangle.
\]
Now if $v$ and $w$ are not parallel, we can find $x$ such that $\langle x, v \rangle = 0 \neq \langle x, w \rangle$, which contradicts the previous. The case of parallel $v$ and $w$ is easy to check.

While hunting for examples, it's worthwhile to note that in some sense $\H(V)$ is not essentially bigger than $\H_{+}(V)$: if $A \in \H(V)$ we can always find a positive real number $\lambda$ such that $A + \lambda I \in \H_{+}(V)$.
To this end, note that the quadratic form of $A + \lambda I$ at $v \in V$ is $\langle (A + \lambda I) v, v \rangle = \langle A v, v \rangle + \lambda \langle v, v \rangle$. But if $\lambda \geq \|A\|$, the operator norm of $A$, the previous quantity is non-negative for any $v \in V$.

TODO: $2 \times 2$ case.

\section{Spectral theorem}

One might wonder if there are other examples of positive maps than positive linear combination of projections. Rather surprisingly, there are none.

\begin{lause}\label{cheapSpectral}
	$A \in \L(V)$ is positive if and only for some $m \geq 0$, $\lambda_{i} > 0$ and $v_{i} \in V$ for $1 \leq i \leq m$ we have
	\[
		A = \sum_{1 \leq i \leq m} \lambda_{i} P_{v_{i}}.
	\]
\end{lause}
\begin{proof}
	We already proved one direction: every map of the previous form is positive.

	The other direction is tricky. The idea is to somehow find the vectors $v_{i}$. The problem is that such representation is by no means unique. If $A$ is any projection for instance, we could let $v_{i}$'s by any orthonormal basis of the corresponding subspace and $\lambda_{i}$'s all equal to one. There's no vector one has to choose.

	But we can think in reverse: there could be many vectors we cannot choose, depending on the map $A$. If $A$ is any non-identity projection to subspace $W$, say, we can only choose $v_{i}$'s in $W$ itself. Indeed, if $x \in W^{\perp}$, we have $A x = 0$, and hence $\langle A x, x \rangle = 0$. By comparing the quadratic form it follows $\langle P_{v_{i}} x, x \rangle = |\langle v_{i}, x \rangle|^{2}$ for any $1 \leq i \leq m$. But this means that $v_{i} \perp W^{\perp}$ and hence $v_{i} \in W$.

	More generally, if it so happens that for some $v \in V$ we have $\langle A v, v \rangle = 0$, we must have $v_{i} \perp v$ for any $1 \leq i \leq m$. But this means that were there such representation, we should have the following.

	\begin{lem}\label{spectralZeroLemma}
		If $A \in \H_{+}(V)$ and $\langle A v, v \rangle = 0$ for some $v \in V$, then $A v = 0$ and $A w \perp v$ for any $w \in v$.
	\end{lem}

	Before proving the Lemma, we complete the proof given the Lemma.

	Proof is by induction on $n$, the dimension of the space. If $n = 0$, the claim is evident. For induction step assume first that there exists $v \in V$ such that $\langle A v, v \rangle = 0$. Then by the Lemma for any $w \in v^{\perp}$ we have $A w \in v^{\perp} =: W$. But this means that $A = \incl{V}{W} \circ \arestr{A}{W} \circ P_{W} = A$. Now $\arestr{A}{W}$ is also positive, and $\dim(W) < n$. By induction assumption we have numbers $\lambda_{i}$ and vectors $v_{i} \in V$ for the map $\arestr{A}{W}$, but such representation for $\arestr{A}{W}$ immediately gives representation for $A$ also.

	We just have to get rid of the extra assumption on the existence of such $v$. But for this, note that if $\lambda = \inf_{|v| = 1} \langle A v, v \rangle$, consider $B = A - \lambda I$. Now $\inf_{|v| = 1} \langle B v, v \rangle = 0$, and $B$ is hence positive. Also, by compactness, the infimum is attained at some point $v$, so $B$ satisfies our assumptions. Now cook up a representation for $B$ and add orthonormal basis of $V$ with $\lambda_{i}$'s equal to $\lambda$: this is required representation for $A$. 
\end{proof}

TODO: image of the proof process in $\R^{3}$.

\begin{proof}[Proof of lemma \ref{spectralZeroLemma}]
	Take any $w \in V$. Now by assumption for any $c \in \C$ we have
	\[
		Q_{A}(c v + w) = \langle A (c v + w), c v + w \rangle = |c| \langle A v, v \rangle + c \langle A v, w \rangle + \overline{c} \langle A w, v \rangle + \langle A w, w \rangle \geq 0
	\]
	But this easily implies that $\langle A v, w \rangle = 0 = \langle A w, v \rangle$ for any $w \in V$. The first equality implies that $A v = 0$ and the second that $A w \perp v$ for any $w \in V$.
\end{proof}

As discussed, there should be obvious generalization for real maps: they real linear combinations for projections. We can however still improve the statement: we can take $v_{i}$'s to be orthogonal, and hence $m \leq n$. We have thus arrived at Spectral theorem.

\begin{lause}[Spectral theorem]
	Let $A \in \H(V)$. Then there exists real numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$ and orthonormal vectors $v_{1}, v_{2}, \ldots, v_{n}$ such that
	\begin{align}\label{spectralrepr}
		A = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}}.
	\end{align}
\end{lause}
\begin{proof}
	Let's first check case of positive $A$. There's not very many things to change from the proof of theorem \ref{cheapSpectral}. Indeed, we again argue by induction. The case $n = 0$ is again clear. In the induction step we found that induction assumption applies to $A - \lambda I$ compressed to suitable $(n - 1)$-subspace. There we can cook up required representation, and bring back the representation for $A - \lambda I$ itself. That is we have
	\[
		A - \lambda I = \sum_{i = 1}^{n - 1} \lambda_{i} P_{v_{i}}
	\]
	for orthonormal $v_{i}$'s and non-negative $\lambda_{i}$'s. But then if $v_{n}$ is a missing orthonormal vector, we find that
	\[
		A = \sum_{i = 1}^{n} (\lambda_{i} + \lambda) P_{v_{i}},
	\]
	where $\lambda_{n} = 0$. But this is what we wanted.

	For non-positive $A$, simply add suitable multiple of identity to get $B := A + \lambda I \geq 0$ and apply what we have proved to $B$. If we have representation for $B$, we can easily cook up one for $A$: just subtract $\lambda$ for $\lambda_{i}$'s in the representation of $B$.
\end{proof}

In the representation \ref{spectralrepr} the numbers $\lambda_{i}$ are evidently the eigenvalues of $A$ and vectors $v_{i}$ the corresponding eigenvectors; this is why we call it the \textit{Spectral representation}. Such representation is of course not unique: if $A = I$, we could again choose $v_{i}$'s to be any orthonormal basis of $V$.

There is way to make the Spectral representation unique, however. For this we have to change $v_{i}$ to corresponding eigenspaces.

\begin{lause}[Spectral theorem]
	Let $A \in \H(V)$. Then there exists unique non-negative integer $m$, distinct real numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}$ and non-trivial orthogonal subspaces of $V$, $E_{\lambda_{1}}, E_{\lambda_{2}}, \ldots E_{\lambda_{m}}$ with $E_{\lambda_{1}} + E_{\lambda_{2}} + \ldots + E_{\lambda_{m}} = V$, such that
	\begin{align*}\label{spectralrepr2}
		A = \sum_{i = 1}^{m} \lambda_{i} P_{E_{\lambda_{i}}}.
	\end{align*}
	Moreover, this representation is unique.
\end{lause}
\begin{proof}
	Existence of such representation immediately follows from the previous form of Spectral theorem. For uniqueness, note that $\lambda_{i}$'s are necessarily the eigenvalues of $A$ and $E_{\lambda_{i}}$'s the corresponding eigenspaces.
\end{proof}

Although the latter version is definitely of theoretical importance, we will mostly stick the former: it only contains one-dimensional projections.

Spectral representation makes many of the properties of real maps obvious. For instance any power of real map is real: indeed, if $A = \sum_{1 \leq i \leq n} \lambda_{i} P_{v_{i}}$, then
\[
	A^{2} = \left(\sum_{i = 1} \lambda_{i} P_{v_{i}}\right) \left(\sum_{j = 1} \lambda_{j} P_{v_{j}}\right) = \sum_{i = 1}^{n} \sum_{j = 1}^{n}\lambda_{i} \lambda_{j} P_{v_{i}} P_{v_{j}} = \sum_{i = 1}^{n} \lambda_{i}^2 P_{v_{i}},
\]
since $P_{v} P_{w} = 0$ for $v \perp w$. By induction one can extend the previous for higher powers. In other words: eigenspaces are preserved under compositional powers, and eigenvalues are ones to get powered up. From the original definition this is not all that clear. One could even extend to polynomials. If $p(x) = c_{n} x^{n} + c_{n- 1} x^{n - 1} + \ldots c_{1} x + c_{0}$, with $c_{i} \in \R$, we should write
\begin{align}\label{polynomial_matrix_function}
	p(A) = c_{n} A^{n} + c_{n - 1} A^{n - 1} + \ldots c_{1} A + c_{0} = \sum_{1 \leq i \leq n} p(\lambda_{i}) P_{v_{i}}.
\end{align}
This implies that if $p$ is the characteristic polynomial of $A$, then $p(A) = 0$: the special case of Cayley Hamilton theorem. Moreover, the minimal polynomial of $A$ is the polynomial with the eigenvalues of $A$ as single roots.

But even better, if $p$ is polynomial with all except one, say $\lambda_{i}$, of the eigenvalues of $A$ as roots, then $p(A) = p(\lambda_{i}) P_{E_{\lambda_{i}}}$. In particular, the projections to eigenspaces of $A$ are actually polynomials of $A$.

Also, given $A \in \H(V)$, we may write any $x \in V$ in the form $v = \sum_{1 \leq i \leq n} x_{i} v_{i}$, where $(v_{i})_{i = 1}^{n}$ is a eigenbasis for $A$ and $x_{i} = \langle x, v_{i} \rangle$. Now $A x = \sum_{1 \leq i \leq n} \lambda_{i} x_{i} v_{i}$, so for instance

\begin{itemize}
\item $Q_{A}(x) = \langle A x, x \rangle = \sum_{1 \leq i \leq n} \lambda_{i} x_{i}^{2}$. Thus $Q_{A}$ is just a positive linear combination of eigenvalues, and $R(A, \cdot)$ convex combination.
\item $\|A x\|^{2} = \langle A x, A x \rangle = \sum_{1 \leq i \leq n} \lambda_{i}^{2} x_{i}^{2} \leq \left(\max_{1 \leq i \leq n} \lambda_{i}^2 \right) \sum_{1 \leq i \leq n} x_{i}^2 = \left(\max_{1 \leq i \leq n} \lambda_{i}^2 \right) \|x\|^{2}$. It follows that $\|A\| = \max_{1 \leq i \leq n} |\lambda_{i}|$.
\end{itemize}

Similarly, if $A \geq 0$, $A$ has a unique positive square root, which we denote by $A^{\frac{1}{2}}$: map such that $A^{\frac{1}{2}} A^{\frac{1}{2}} = A$. Given the spectral reprsentation $A = \sum_{1 \leq i \leq n} \lambda_{i} P_{v_{i}}$, we can simply set $A^{\frac{1}{2}} = \sum_{1 \leq i \leq n} \lambda_{i}^{\frac{1}{2}} P_{v_{i}}$. As for the uniqueness, note that if $B$ is a positive square root for $A$ and $B = \sum_{1 \leq i \leq n} \lambda_{i}' P_{v_{i}'}$, then $B^2 = \sum_{1 \leq i \leq n} \lambda_{i}'^{2} P_{v_{i}'}$. It follows that eigenvalues of $B$ are simply square roots of eigenvalues of $A$ and the corresponding eigenspaces are equal. Of course, the whole uniqueness argument floats more naturally with unique spectral representation.

\subsection{Commuting real maps}

\textbf{Warning!} Composition of positive maps need not be positive!

If $A, B \in \H_{+}(V)$, then, as we noticed, $(A B)^{*} = B^{*} A^{*} = B A$, so for $A B$ to be even real, $A$ and $B$ would at least need to commute. Natural question follows: when do two positive maps commute? Since $(c_{1} I + A)$ and $(c_{2} I + B)$ commute if and only if $A$ and $B$ do, this is same as asking when do two real maps commute.

It turns out that real maps commute only if they ``trivially" commute, in the following sense. If there exists vectors $v_{1}, v_{2}, \ldots, v_{n}$ and numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$ and $\lambda'_{1}, \lambda'_{2}, \ldots, \lambda'_{n}$ such that
\[
	A = \sum_{1 \leq i \leq n} \lambda_{i} P_{v_{i}} \; \text{ and } \; B = \sum_{1 \leq i \leq n} \lambda'_{i} P_{v_{i}},
\]
then $A$ and $B$ are said to be \textit{simultaneously diagonalizable}. Simultaneosly diagonalizable maps trivially commute, and it turns out that if two real maps commute, they are indeed simultenously diagonalizable.

To prove this statement, we start with a lemma, simplest non-trivial case of the statement.

\begin{lem}\label{projectionLemma}
	Let $W_{1}, W_{2} \subset V$ be two subspaces. Then $P_{W_{1}}$ and $P_{W_{2}}$ commute if and only if there exists orthogonal subspaces $U_{1}, U_{2}$ and $U_{0}$ such that
	\[
		W_{1} = U_{1} + U_{0}  \; \text{ and } \; W_{2} = U_{2} + U_{0}.
	\]
	We then have $P_{W_{1}} = P_{U_{1}} + P_{U_{0}}$ and $P_{W_{2}} = P_{U_{2}} + P_{U_{0}}$, and $U_{0} = W_{1} \cap W_{2}$.
\end{lem}
\begin{proof}
	Write $U_{0} := W_{1} \cap W_{2}$ and $W_{i} = U_{0} + U_{i}$ for some $U_{i} \perp U_{0}$ for $i \in \{1, 2\}$. Now $P_{W_{i}} = P_{U_{i}} + P_{U_{0}}$ for $i \in \{1, 2\}$ so it suffices to check that $U_{1} \perp U_{2}$. Equivalently, it suffices to prove that if $W_{1} \cap W_{2} = \{0\}$, and $P_{W_{1}}$ and $P_{W_{2}}$ commute, then $W_{1} \perp W_{2}$ or equivalently $P_{W_{1}}P_{W_{2}} = 0 = P_{W_{2}}P_{W_{1}}$. But for any $v \in V$ we have $W_{1} \ni P_{W_{1}}P_{W_{2}}v = P_{W_{2}}P_{W_{1}}v \in W_{2}$, so indeed $P_{W_{1}}P_{W_{2}} = 0 = P_{W_{2}}P_{W_{1}}$.
\end{proof}

\begin{maar}
	We say that two $W_{1}, W_{2} \subset V$ subspaces commute if the respective projections commute.
\end{maar}

\begin{lause}\label{commuting_real_maps}
	Let $\mathcal{A} = (A_{j})_{j \in J}$ by an arbitrary family of commuting real maps. Then there exists non-trivial orthogonal subspaces of $V$, $E_{1}, E_{2}, \ldots E_{m}$ with $E_{1} + E_{2} + \ldots + E_{m} = V$ and numbers $\lambda_{i, j}$ for $j \in J$ and $1 \leq i \leq n$ such that
	\[
		A_{j} = \sum_{1 \leq i \leq m} \lambda_{i, j} P_{E_{i}}
	\]
	for any $j \in J$.
\end{lause}
\begin{proof}
	The main idea is the following: like in the spectral theorem, we would like to somehow find the subspaces $E_{1}, E_{2}, \ldots E_{m}$. Also, at least for finite families, we could probably use induction, so we should get far just by proving the theorem for a family of only two maps. For two projections we have already proved the statement as lemma \ref{projectionLemma}.

	Now here's the trick: if two maps commute, so do all their polynomials. Hence if we have two commuting $A$ and $B$, we know that all the respective eigenspaces commute. Now if we could prove the statement at least for finite families of projections, we could conclude the case of two general maps. Indeed we could write any eigenprojection of $A$ or $B$ as a linear combination of sum finite family of orthogonal (orthogonal) projections, but those projections would then also span $A$ and $B$.

	More generally, if we could prove the statement for arbitrary families of projections, the same argument would yield it for any family of more general linear maps, so we can safely assume that all the maps $A_{j}$ are projections.

	Let's first deal with the finite case by induction. As mentioned, we already dealt with the case $|J| = 2$, but we can draw better conclusions. If we have two commuting projections $P_{W_{1}}$ and $P_{W_{2}}$ in $\mathcal{A}$. Now by the lemma we may write $P_{W_{1}} = P_{U_{1}} + P_{U_{0}}$ and $P_{W_{2}} = P_{U_{2}} + P_{U_{0}}$. The nice things is that any map in $\mathcal{A}$ also commutes with $P_{W_{1}} + P_{W_{2}} = P_{U_{1}} + P_{U_{2}} + 2 P_{U_{0}}$, so also with it's eigenprojections, $P_{U_{0}}$ and $P_{U_{1} + U_{2}}$. It follows that any map in $\mathcal{A}$ commutes with $U_{0}, U_{1}$ and $U_{2}$.

	We have split the subspaces $W_{1}$ and $W_{2}$ in pieces, and we could actually forget $W_{1}$ and $W_{2}$ altogether and replace them by $U_{0}, U_{1}$ and $U_{2}$: note that all the same assumption hold for this new family, and $U_{0}, U_{1}$ and $U_{2}$ span $W_{1}$ and $W_{2}$.

	Problem here is of course: it's not clear that the new family, say $\mathcal{A}'$ is any simpler than $\mathcal{A}$! It could well have more elements than $\mathcal{A}$ so we can't just do straightforward induction. What could happen also is that some of the subspaces $U_{0}, U_{1}, U_{2}$ coincide with the subspaces already present in the family, so the size of the family doesn't increase, and it could even decrease. This will indeed happen. One way to see this is to look at the sum of dimensions of all the projections of the family: if we change the family this sum cannot increase. Moreover, if we pick two subspaces $W_{1}$ and $W_{2}$ which are not orthogonal, this sum will decrease!

	The conclusion is: pick pairs projections with non-orthogonal subspaces and do the replacing procedure as explained before; this process will eventually stop since the sum of dimesions can't drop below zero. But the only reason this process could stop is that all subspaces are pairwise orthogonal in which case we are done. The proof of finite case is complete.

	There are many ways to bootstrap the previous argument for arbitrary families. For any finite subfamily we can form the set of generating projections. If add one more map, the set projections get refined: some of the subspaces get split to pieces. Now sizes of all these generating families are bounded by $n$ so we may pick one with most number of elements. Now if $A$ is any projection in $\mathcal{A}$, by maximality, adding it to the family does not refine the generating set. But this means that the generating set generates any element of $\mathcal{A}$ and we are done.

	We also see that there exists unique minimal family of generating projections TODO.

	Alternative approach to the theorem could be to look at the commutative $\R$-algebra of real maps generated by $\mathcal{A}$: generating projections will be in some sense minimal projections in this algebra.
\end{proof}

The previous theorem sends a very important message.

\begin{phil}
	Commutativity kills the exciting phenomena.
\end{phil}

One would naturally hope that product of positive maps is still positive, but as soon as we try to make such restriction, everything degenerates to $\R^{m}$, or to diagonal maps. Dealing with diagonal maps is then again just dealing with many real numbers at the same time: of course this makes sense and all, but doesn't lead to very interesting concept.

Conversely, if one wants exciting things to happen, one should make things very non-commutative.

As another corollary of theorem \ref{commuting_real_maps} we have

\begin{kor}
	If $A, B \geq 0$ and $A$ and $B$ commute, then $AB \geq 0$.
\end{kor}

Also in the general case we can say something positive:

\begin{prop}
	If $A, B \geq 0$, then $AB$ has non-negative eigenvalues. Conversely, if $C$ has non-negative eigenvalues, then it's of the form $AB$ for some positive $A$ and $B$. TODO: details
\end{prop}
\begin{proof}
	Postponed.
\end{proof}

TODO: independence of random variables.

\subsection{Symmetric product}

As normal product doesn't quite work with positivity, next attempt might be symmetrized product
\[
	S(A, B) = AB + BA,
\]
(or maybe with normalizing constant $\frac{1}{2}$ in the front), instead of the usual one. It turns out that even this doesn't fix positivity.

\begin{prop}
	Let $\alpha, \beta, \gamma \in \C$ and $n \geq 2$. Then the expression $\alpha A^{2} + \beta AB + \overline{\beta} BA + \gamma B^{2}$ is positive for any $A, B \geq 0$ if and only if $\alpha, \gamma \in [0, \infty)$ and $|\beta|^{2} \leq \alpha \gamma$.
\end{prop}
\begin{proof}
	TODO
\end{proof}

So in some sense, by taking non-commutative products, we really lose most of the structure.

\section{Congruence}

\subsection{$*$-conjugation}

There is one very important way to produce positive maps from others, called congruence. Given any two positive maps $A$ and $B$, their composition need not be positive, but the map $BAB$ is. First of all, it is real as $(BAB)^{*} = B^{*} A^{*} B^{*} = BAB$. Also $Q_{BAB}(v) = \langle BAB v, v \rangle = \langle A (B v), (B v) \rangle \geq 0$ for any $v \in V$. We didn't really need the assumption on the positivity of $B$, but realness was not that important either. Namely for arbitrary linear $B$ we could consider the product $B^{*}AB$ instead: this is positive whenever $A$ is. If $C = B^{*}AB$ for some $B \in \L(V)$, we say that $C$ is $*$-conjugate of $A$.

We also see that $Q_{B^{*}AB} = Q_{A} \circ B$: conjugation is a change of basis in the quadratic form. This is the main motivation for the definition of the $*$-conjugation. We have already seen that the quadratic form of a map is a good way to characterize many of its good properties, so to some extent to understand maps, we just to need to understand structure of their quadratic forms. By change of basis of the quadratic form we have a good control of what happens. We might however lose some information: if $B = 0$, for instance, the quadratic form after $*$-conjugation by $B$ doesn't tell much about $A$. But if $B$ is invertible, or equivalently if $C$ and $B$ are $*$-conjugates of each other, we shouldn't lose any information. If this is the case, we say that $A$ and $C$ are congruent. It is easily verified that congruence is a equivalence relation.

The construction of $*$-conjugation makes also sense for general linear map $A$, i.e. we could just as well $*$-conjugate non-positive, or even non-real maps. The result then need not be positive or real, and in general, $*$-conjugation loses its usefulness.

The previous construction can be also performed between two spaces $V$ and $W$: given any map $B \in \L(V, W)$ and $A \in \H_{+}(W)/\H(W)/\L(W)$, we note that $B^{*}AB \in \H_{+}(V)/\H(V)/\L(V)$. For real maps we can say a lot more: while congruence doesn't in general preserve eigenvalues, it preserves their signs.

\begin{lause}[Sylvester's Law of Inertia]
	$A, B \in \H(V)$ are congruent, if and only if $A$ and $B$ have equally many positive, negative and zero eigenvalues, counted with multiplicity.
\end{lause}
\begin{proof}
	Let's start with the ``if" part. Let's denote the eigenvalues of $A$ and $B$ by $\lambda_{1} \leq \lambda _{2} \leq \ldots \leq \lambda_{n}$ and $\lambda_{1}' \leq \lambda _{2}' \leq \ldots \leq \lambda_{n}'$, respectively, and the corresponding eigenvectors with $v_{1}, v_{2}, \ldots, v_{n}$ and $v_{1}', v_{2}', \ldots, v_{n}'$. By assumption $\lambda_{i}$ and $\lambda_{i}'$ have the same sign (or are both zero) for any $1 \leq i \leq n$, so we may find non-zero real numbers $t_{1}, t_{2}, \ldots, t_{n}$ such that $\lambda_{i} = \lambda_{i}' t_{i}^{2}$. Now consider a linear map $C$ with $C v_{i} = t_{i} v_{i}'$. $C$ is clearly a surjection and hence a bijection. Also if $v = \sum_{i = 1}^{n} x_{i} v_{i}$ $(Q_{B} \circ C)(v) = Q_{B}(\sum_{i = 1}^{n} x_{i} t_{i} v_{i}') = \sum_{i = 1}^{n} |x_{i}|^{2} t_{i}^2 \lambda_{i}' = \sum_{i = 1}^{n} |x_{i}|^{2} \lambda_{i} = Q_{A}(v)$ so $Q_{C^{*}BC} = Q_{B} \circ C = Q_{A}$. It follows that $C^{*}BC = A$ and hence $A$ and $B$ are congruent.

	The ``only if" - part is a bit trickier. The idea is to find a good description for the number of positive non-negative eigenvalues. We noticed before that we can write quadratic forms in the form $Q_{A}(v) = \sum_{i = 1}^{n} \lambda_{i} |x_{i}|^{2}$ if $v = \sum_{i = 1}^{n} x_{i}v_{i}$, and $v_{i}$ are the eigenvectos of $A$ with $\lambda_{i}'s$ as the corresponding eigenvectors. In particular if say first $k$ eigenvalues are negative, $Q_{A}$ will be negative on $\vspan\{v_{i} | 1 \leq i \leq k\}$, a $k$-dimensional subspace, minus zero. Similarly, now $n - k$ of the eigenvalues are non-negative, so the quadratic form is non-negative on a subspace of dimension of at least $n - k$. But the dimensions can't be any bigger: if $Q_{A}$ were for instance negative on some $k + 1$ dimesional subspace, this subspace would necessarily intersect a subspace where $Q_{A}$ is non-negative, which is non-sense.

	Congruence preserves the previous notion: if $Q_{B}$ is negative on a subspace of dimension $k$, so is $Q_{B} \circ C$ for any invertible $C$; namely in the inverse image. Same reasoning holds for the the subspace on which $Q_{B}$ is non-negative, so again, $Q_{B} \circ C$ has to have similar structure. We are done.
\end{proof}

In the proof we used the following useful linear algebra fact.

\begin{lem}
	Let $V$ be $n$-dimensional and $W_{1}, W_{2} \subset V$ subspaces such that $\dim(W_{1}) + \dim(W_{2}) > n$. Then $W_{1} \cap W_{2} \neq \{0\}$.
\end{lem}
\begin{proof}
	We find non-trivial element $v \in W_{1} \cap W_{2}$. Take bases for $W_{1}$ and $W_{2}$, say $(e_{i})_{i = 1}^{n_{1}}$ and $(f_{i})_{i = 1}^{n_{2}}$ with $n_{1} + n_{2} > n$. Since $(e_{i})_{i = 1}^{n_{1}} \cup (f_{i})_{i = 1}^{n_{2}}$ can't be linearly independent, as that would mean $\dim(V) \geq \dim(W_{1}) + \dim(W_{2}) > n$, we can find non-trivial pair of sequence $(a_{i})_{i = 1}^{n_{1}}$'s and $(b_{i})_{i = 1}^{n_{2}}$ such that $\sum_{i = 1}^{n_{1}} a_{i} e_{i} + \sum_{i = 1}^{n_{2}} b_{i} f_{i} = 0$. But $W_{1} \ni \sum_{i = 1}^{n_{1}} a_{i} e_{i} = v = -\sum_{i = 1}^{n_{2}} b_{i} f_{i} \in W_{2}$, and since sequences are non-trivial, $v$ is non-trivial element in the intersection.
\end{proof}

If $n_{0}, n_{-}$ and $n_{+}$ denote the number of zero, negative and positive eigenvalues of $A$, \textit{inertia} of $A$ is the triplet $\{n_{0}, n_{-}, n_{+} \} := \{n_{0}(A), n_{-}(A), n_{+}(A) \}$. The previous theorem can be hence restated, that inertia is invariant under congruence.

The proof also gives a useful characterization for the number of non-negative eigenvalues.

\begin{kor}\label{subspace_lemma}
	If $A \in \H(V)$, number of non-negative eigenvalues of $A$ equals largest non-negative integer $k$ such that for some subspace $W \subset V$ of dimension $k$ the quadratic form $Q_{A}$ is non-negative on $W$, or equivalently, $\arestr{A}{W} \geq 0$.
\end{kor}

Sylvester's Law of inertia gives another proof of the fact that strictly positive maps are exactly the maps congruent to the identity, and positive maps are the maps congruent to some projection. More precisely, the positive maps are partitioned to $n + 1$ congruence classes depending on their rank, $k$:th congruence class containing the projections to $k$-dimensional subspaces. $0$:th class contains only the zero map, the only rank $0$ positive map, and the $n$:th class is the class of strictly positive maps.

If one $*$-conjugates with non-invertible, the inertia may change, but in quite obvious way only: some eigenvalues may move to $0$. In particular, we have the following even a bit more general version of the law.

\begin{lause}[General Sylvester's Law of Inertia]
	For $A, B \in \normal(V)$ and $A$ is $*$-conjugate of $B$, if and only if $n_{\pm}(A) \leq n_{\pm}(B)$.
\end{lause}
\begin{proof}
	TODO
\end{proof}

This extension draws a picture about the relation of previously mentioned congcruence classes. We can move to the congruence classes of lower indeces by $*$-conjugation, but cannot move up the ladder: the complexity of quadratic forms cannot increase. One could also think that $*$-congruence for linear maps corresponds to multiplication by non-negative real for real numbers.

\subsection{Block decomposition}

Congruence is a convenient to tool to investigate positivity. The idea is that with conguence we can perform sort of a Gaussian elimination. If $n = 2$ for instance, we can write any real map in the matrix form
\[
	M =
	\begin{bmatrix}
		a & b \\
		\overline{b} & c
	\end{bmatrix}
\]
for some $a, c \in \R$ and $b \in \C$. Now if $a \neq 0$, we could eliminate with
\[
	D =
	\begin{bmatrix}
		1 & -\frac{b}{a} \\
		0 & 1
	\end{bmatrix}
\]
to get
\[
	M D =
	\begin{bmatrix}
		a & b \\
		\overline{b} & c
	\end{bmatrix}
	\begin{bmatrix}
		1 & -\frac{b}{a} \\
		0 & 1
	\end{bmatrix}
	=
	\begin{bmatrix}
		a & 0 \\
		\overline{b} & \frac{a c - |b|^2}{a}.
	\end{bmatrix}
\]
The resulting map of course need not be real, but if we also eliminate from the other side by $D^{*}$, we get
\[
	D^{*} M D =
	\begin{bmatrix}
		a & 0 \\
		0 & \frac{a c - |b|^2}{a}.
	\end{bmatrix}
	=: M'
\]
Now $D$ is evidently invertible, it's determinant being $1$, so $M$ and $M'$ are congruent. Sylvester's law of inertia tell's us hence that that if $a > 0$ and $\det(M) \geq 0$, then $M \geq 0$.

We can generalize this thinking. For general $n$ if we have decomposition $V = W_{1} \oplus W_{2}$, then we can decompose any mapping $M$ as
\[
	M =
	\begin{bmatrix}
		A & B \\
		B^{*} & C
	\end{bmatrix},
\]
where $A, B$ and $C$ are the \textit{blocks} of $M$ given by $A = P_{W_{1}} \circ M \circ \incl{V}{W_{1}} = \arestr{M}{W_{1}}$, $B = P_{W_{1}} \circ M \circ \incl{V}{W_{2}}$ and $C = P_{W_{2}} \circ M \circ \incl{V}{W_{2}} = \arestr{M}{W_{2}}$. Now we can generalize the previous elimination: if $A$ happens to be invertible and we let
\[
	D =
	\begin{bmatrix}
		I & -A^{-1} B \\
		0 & I
	\end{bmatrix}
\]
then
\[
	D^{*} =
	\begin{bmatrix}
		I & 0 \\
		-B^{*} A^{-1}  & I
	\end{bmatrix}
\]
and
\begin{align}\label{schur_complement}
	D^{*} M D =
	\begin{bmatrix}
		I & 0 \\
		-B^{*} A^{-1}  & I
	\end{bmatrix}
	\begin{bmatrix}
		A & B \\
		B^{*} & C
	\end{bmatrix}
	\begin{bmatrix}
		I & -A^{-1} B \\
		0 & I
	\end{bmatrix}
	=
	\begin{bmatrix}
		A & 0 \\
		0 & C - B^{*} A^{-1} B
	\end{bmatrix}.
\end{align}
The map $(C - B^{*} A^{-1} B) : W_{2} \to W_{2}$ is called the \textit{Schur complement} of block $A$ of $M$, or maybe one should say Schur complement of $M$ with respect to $W_{1}$. We denote the Schur complement by $M/A$.

Now again if $A$ is invertible, $M \geq 0$ if and only if $A > 0$ and $M/A \geq 0$.

This observations leads to convenient characterization for strictly positivity, called Sylvester's criterion. If $W_{2}$ is $1$-dimensional, $M/A$ is just a real number and $M$ is stricly positive if and only if $A > 0$ and this real number is positive. On the other hand, by computing determinants we see that
\[
	\det(M) = \det \left(
	\begin{bmatrix}
		A & 0 \\
		0 & M/A
	\end{bmatrix}
	\right)
	=
	\det(A) \det(M/A),
\]
as $\det(D) = 1$. Hence $M$ is positive if and only if $\det(M)$ is positive and $A > 0$. Applying the same idea inductively we arrive at
\begin{lause}[Sylvester's criterion]
	$A \in \H(V)$ is stricly positive if and only for some (and then for any) sequence of subspaces $W_{1} \subset W_{2} \subset \ldots \subset W_{n - 1} \subset W_{n} = V$ with $\dim(W_{m}) = m$ we have $\det(A_{W_{m}}) > 0$ for any $1 \leq m \leq n$.
\end{lause}

TODO: Explain what happend with non-strict case.

One can solve $M$ from \ref{schur_complement} to arrive at so-called \textit{LDL-decomposition} of $M$:
\begin{align}\label{ldl_decomposition}
	M =
	\begin{bmatrix}
		A & B \\
		B^{*} & C
	\end{bmatrix}
	=
	\begin{bmatrix}
		I & 0 \\
		B^{*} A^{-1}  & I
	\end{bmatrix}
	\begin{bmatrix}
		A & 0 \\
		0 & C - B^{*} A^{-1} B
	\end{bmatrix}
	\begin{bmatrix}
		I & A^{-1} B \\
		0 & I
	\end{bmatrix}.
\end{align}

LDL-decomposition leads to many interesting identities. First of all, (given that $A$ is invertible), $M$ is invertible if and only if $C - B^{*} A^{-1} B$ is and its inverse is given by
\begin{align*}
	M^{-1} &=
	\begin{bmatrix}
		I & -A^{-1} B \\
		0 & I
	\end{bmatrix}
	\begin{bmatrix}
		A^{-1} & 0 \\
		0 & (C - B^{*} A^{-1} B)^{-1}
	\end{bmatrix}
	\begin{bmatrix}
		I & 0 \\
		-B^{*} A^{-1}  & I
	\end{bmatrix} \\
	&=
	\begin{bmatrix}
		A^{-1} + A^{-1} B (C - B^{*} A^{-1} B)^{-1} B^{*} A^{-1} & -A^{-1} B (C - B^{*} A^{-1} B)^{-1} \\
		-(C - B^{*} A^{-1} B)^{-1} B^{*} A^{-1} & (C - B^{*} A^{-1} B)^{-1}
	\end{bmatrix}.
\end{align*}

If one take Schur complement with respect to $C$ instead one arrives at
\begin{align*}
	M^{-1}
	&=
	\begin{bmatrix}
		(A - B C^{-1} B^{*})^{-1} & (A - B C^{-1} B^{*})^{-1} B C^{-1} \\
		- C^{-1} B^{*} (A - B C^{-1} B^{*})^{-1} & C^{-1} + C^{-1} B^{*} (A - B C^{-1} B^{*})^{-1} B C^{-1}
	\end{bmatrix},
\end{align*}
so by comparing blocks we see that for instance
\begin{align}\label{woodbury_identity}
	A^{-1} + A^{-1} B (C - B^{*} A^{-1} B)^{-1} B^{*} A^{-1} = (A - B C^{-1} B^{*})^{-1},
\end{align}
\textit{Woodbury matrix identity}. Why might such identity be useful? The idea is that if $\dim(W_{2}) \ll \dim(W_{1})$, the identity is way to connect inverse of $A - B C^{-1} B^{*}$, low rank update of $A$, and $A$. If $\dim(W_{2}) = 1$ for instance, by setting $C = -1$ for some $c > 0$ and $B = v$ for some $v \in V$ we get
\begin{align*}
	A^{-1} - \frac{A^{-1} v v^{*} A^{-1}}{1 + \langle A^{-1}v, v\rangle} = (A + v v^{*})^{-1}:
\end{align*}
inverse of rank 1 update can be easily calculated if one knows the inverse of the original map.

In a similar vein one obtains formulas for determinants. Starting with $\det(M) = \det(A) \det(C - B^{*} A^{-1} B)$, if we happen to know determinant of a map and need determinant of a compression, it is sufficient to find it for a schur complement. This is particularly useful when $W_{2}$ is low dimensional. If $\dim(W_{2}) = 1$ and $W_{2} = \vspan(v)$, then
\begin{align*}
	\det(M) &= \det(A) \left(C - B^{*} A B\right) \\
	&= \frac{\det(A) |v|^2}{\langle M^{-1} v, v \rangle}:
\end{align*}
Schur complement is inverse of compression $M$ to $W_{2}$. It follows that if $A$ is invertible, we have
\begin{align}\label{compression_determinant}
	\det(\arestr{A}{W}) = \det(A) \langle A^{-1} v, v \rangle.
\end{align}
By comparing determinants from two LDL-decompositions we arrive at
\begin{align}\label{matrix_determinant_lemma}
	\det(A) \det(C - B^{*} A^{-1} B) = \det(C) \det(A - B C^{-1} B^{*}),
\end{align}
\textit{matrix determinant lemma}. Again, by the choices for $B = v$ and $C = -1$ we arrive at
\begin{align*}
	\det(A) \left(1 + \langle A^{-1}v, v\rangle\right) = \det(A + v v^{*}):
\end{align*}
determinant of rank 1 update can be also easily calculated.

Of course, once one knows the statements, such identities could also be easily verified by multiplying everything out, for instance, but this is how one might stumble upon them.

\section{Loewner order}

\begin{maar}
	If $A, B \in \H(V)$, we write that $A \leq B$ ($A$ is smaller than $B$) if $B - A \geq 0$, $B - A$ is positive. If $B - A$ is strictly positive, we write $A < B$.
\end{maar}

We could of course have made such definition immediately after defining positive maps, but now we have proper tools to investigate such order. Proposition \ref{basic_positive} tells us that such order is indeed partial order on the $\R$-vector space of real maps. More explicitly, we have the following properties:


\begin{prop}
\begin{enumerate}[(i)]
		\item If $A \leq B$ then $\alpha A \leq \alpha B$ for any $\alpha \geq 0$.
		\item If $A \leq B$ and $B \leq C$ then $A \leq C$.
		\item If $A \leq B$ and $B \leq A$ then $A = B$.
		\item If $\lambda I \leq A$, then all the eigenvalues of $A$ are at least $\lambda$. Similarly if $A \leq \lambda I$, all the eigenvalues of $A$ are at most $\lambda$.
\end{enumerate}
\end{prop}

\begin{esim}
	If $W_{1}, W_{2} \subset V$ are two subspaces of $V$ we have $P_{W_{1}} \leq P_{W_{2}}$ if and only if $W_{1} \subset W_{2}$. Indeed if $W_{1} \subset W_{2}$ then $W_{2} = W_{1} + W_{3}$ for some $W_{3} \perp W_{1}$ and hence $P_{W_{2}} = P_{W_{1}} + P_{W_{3}} \geq P_{W_{1}}$. Conversely if $P_{W_{1}} \leq P_{W_{2}}$, for any $v \in W_{1}$ we have $Q_{{P_{W_{1}}}}(v) = \|v\|^{2} \leq \langle P_{W_{2}} v, v \rangle = Q_{P_{W_{2}}}(v)$, where the inequality can hold if and only if $v \in W_{2}$.
\end{esim}

Key thing here is to note what is missing from the standard real ordering: multiplication by positive map doesn't preserve usual ordering. This is the reason many standard arguments don't work for general real maps.

For example if $0 < a \leq b$, with real numbers one could multiply the inequalities by the positive number $(a b)^{-1}$ to get $0 < b^{-1} \leq a^{-1}$. This doesn't quite work with linear maps anymore.

Congruence is way to at least partially fix this deficit: it's almost like multiplying by positive number. We have
\begin{prop}
	If $A \leq B$, then for any $C$ we have $C^{*} A C \leq C^{*} B C$.
\end{prop}

Using the previous we can mimic the previous proof to make it work.

\begin{lause}
	If $0 < A \leq B$, then $B^{-1} \leq A^{-1}$.
\end{lause}
\begin{proof}
	As mentioned, we can't really multiply by $(A B)^{-1}$, as it does not preserve the order and doesn't even need to be positive. If $A$ and $B$ commute, this would work though. We can almost multiply by $A^{-1}$: $*$-conjugate by $A^{-\frac{1}{2}}$. This preserves the order, and we get
	\[
		I \leq A^{-\frac{1}{2}} B A^{-\frac{1}{2}}.
	\]
	Now one would sort of want to multiply $B^{-1}$; so $*$-conjugate by $B^{-\frac{1}{2}}$, but $B$ is in the middle, so this doesn't quite work. But now we can follow the original strategy: since $I \leq X := A^{-\frac{1}{2}} B A^{-\frac{1}{2}}$ we have $X^{-1} \leq I$, that is
	\[
		A^{\frac{1}{2}} B^{-1} A^{\frac{1}{2}} \leq I.
	\]
	This is already almost what we wanted: simply $*$-conjugate by $A^{-\frac{1}{2}}$.
\end{proof}

There's one wee bit non-trivial part in the proof: if $I \leq X$ then $X^{-1} \leq I$. But if $I \leq X$, all the eigenvalues of $X$ are at least $1$, so all the eigenvalues of its inverse are at most $1$, so $X \leq I$.

\begin{huom}

Alternatively, we could conjugate both sides by $X^{-\frac{1}{2}}$ to arrive at the conclusion. Note that by doing this we have only used $*$-conjugation in the proof: actually we have $*$-conjugated altogether with 
\[
	A^{-\frac{1}{2}} (A^{-\frac{1}{2}}B A^{-\frac{1}{2}})^{-\frac{1}{2}} A^{-\frac{1}{2}} = (A^{\frac{1}{2}} (A^{-\frac{1}{2}}B A^{-\frac{1}{2}})^{\frac{1}{2}} A^{\frac{1}{2}})^{-1}.
\]
The map $A^{\frac{1}{2}} (A^{-\frac{1}{2}}B A^{-\frac{1}{2}})^{\frac{1}{2}} A^{\frac{1}{2}}$, which is real, is usually called the geometric mean of $A$ and $B$. It turns out that this mean, denoted by $G(A, B)$ satifies
\[
	G(A, B) = G(B, A) \;\;\; \text{ and } \;\;\; G(A, B)^{-1} = G(A^{-1}, B^{-1}),
\]
and if $A$ and $B$ commute we have $G(A, B) = (A B)^{\frac{1}{2}}$. The defining property of it we used it was that $G(A, B)$ is unique real map with
\[
	B = G(A, B) A^{-1} G(A, B).
\]

The point is: somewhat curiously we can almost do the original proof: just replace multiplication by congruence by square root, and replace square root of product by geometric mean.

\end{huom}

To further highlight the importance of congruence, we can use it to change map inequalities to usual real inequalities. For instance, one can generalize so called (two variable) arithmetic-harmonic mean inequality, which states that for any two positive real numbers $a$ and $b$ we have
\[
	\frac{a + b}{2} \geq \frac{2}{\frac{1}{a} + \frac{1}{b}}.
\]
This classic inequality, which can be seen as a restatement of the convexity of the map $x \mapsto \frac{1}{x}$, can be verified for instance by multiplying out the denominator and rewriting it as $\frac{(a - b)^{2}}{ab} \geq 0$.

To prove the matrix version, namely
\[
	\frac{A + B}{2} \geq (A^{-1} + B^{-1})^{-1}
\]
for any $A, B > 0$, we can $*$-conjugate both sides by $A^{-\frac{1}{2}}$ to arrive at
\[
	\frac{I + A^{-\frac{1}{2}}B A^{-\frac{1}{2}}}{2} \geq 2 (I + A^{\frac{1}{2}}B^{-1} A^{\frac{1}{2}})^{-1}.
\]
If one writes $X = A^{-\frac{1}{2}}B A^{-\frac{1}{2}}$, this rewrites to
\[
	\frac{I + X}{2} \geq 2 (I + X^{-1})^{-1}.
\]
But now since $I$ and $X$ commute, the claim is evident form the scalar inequality. In a similar manner one could also prove that the geometric mean lies between arithmetic and harmonic.

\section{Eigenvalue inequalities}

There's great deal of things to be said about relationship between eigenvalues and Loewner order. Let's denote the eigenvalues of real map $A$ by $\lambda_{1}(A) \geq \lambda_{2} \geq \ldots \geq \lambda_{n}(A)$. One of the most basic result is the following.

\begin{prop}\label{loewner_eigenvalues}
	Assume that $A \leq B$. Then for any $1 \leq k \leq n$ we have $\lambda_{k}(A) \leq \lambda_{k}(B)$.
\end{prop}
\begin{proof}
	We first claim that $A$ has at most as many non-negative eigenvalues as $B$: if we manage to do this, we can apply the observation for the maps $A - \lambda I$ and $B - \lambda I$ and conclude that $B$ has at least $k$ eigenvalues in $[\lambda_{k}(A), \infty)$, which implies that $\lambda_{k}(A) \leq \lambda_{k}(B)$.

	To prove the claim note that if $A$ has $k$ non-negative eigenvalues, by lemma \ref{subspace_lemma} it's restriction to some $k$-dimensional subspace is positive. But then also the compression of $B$ to this subspace is positive, so also $B$ has at least $k$ non-negative eigenvalues.
\end{proof}

In general that's all one can say: if numbers $a_{1} \geq a_{2} \geq \ldots a_{n}$ and $b_{1} \geq b_{2} \geq \ldots \geq b_{n}$ satisfy $a_{k} \leq b_{k}$, then we can definitely find $A$ and $B$ with $A \leq B$ and $a_{i}$'s and $b_{i}$'s as eigenvalues: simply take $A = \sum_{i = 1}^{n} a_{i} P_{e_{i}}$ and $B = \sum_{i = 1}^{n} b_{i} P_{e_{i}}$ where $(e_{i})_{i = 1}^{n}$ is an orthonormal basis.

Eigenvalues work also desirably with compression.

\begin{prop}[Cauchy interlacing theorem]\label{compression_eigenvalues}
	If $A \in \H^{n}(V)$ and $W \subset V$ is of dimension $n - 1$, then we have
	\begin{align*}
		\lambda_{1}(A) \geq \lambda_{1}(\arestr{A}{W}) \geq \lambda_{2}(A) \geq \lambda_{2}(\arestr{A}{W}) \geq \ldots \geq \lambda_{n - 1}(A) \geq \lambda_{n - 1}(\arestr{A}{W}) \geq \lambda_{n}(A).
	\end{align*}
\end{prop}

\begin{proof}
	We use the same appoach and first prove that $A$ has at least as many non-negative eigenvalues as $\arestr{A}{W}$: again if we know this, we get inequalities of the form $\lambda_{k}(A) \geq \lambda_{k}(\arestr{A}{W})$. Then applying the idea for the $-A$, we get the reverse inequalities, and finally the complete chain.

	To prove the claim, note again that if $\arestr{A}{W}$ has $k$ non-negative eigenvalues, by lemma \ref{subspace_lemma} it's compression to some $k$-dimensional subspace is positive. But then also compression of $A$ to this same subspace is positive and hence it has $k$ non-negative eigenvalues.
\end{proof}

TODO picture of eigenvalues changing when compressed

Again one can prove that this result is strongest possible.

\begin{prop}\label{compression_eigenvalues_con}
	For any $a_{1} \geq b_{1} \geq a_{2} \geq \ldots \geq b_{n - 1} \geq a_{n}$ we may find $A \in \H^{n}(V)$ with $a_{i}$'s as spectra and $(n - 1)$-dimensional subspace $W$ of $V$ such that eigenvalues of $\arestr{A}{W}$ are the $b_{i}$'s.
\end{prop}

Before approaching the proof we note an interesting corollary.

Let us call pair $(A, B) \in \H(V)^{2}$ a \textit{projection pair} if $B - A = v v^{*}$ for some $v \in V$. Note that such $v$ is always unique up to phase. Let us say that a projection pair $(A, B)$ is strict, if whenever $B - A = v v^{*}$ then $v$ is not orthogonal to any eigenvector of $A$. 

\begin{kor}\label{projection_eigenvalues}
	Let $(A, B)$ be a projection pair. Then
	\begin{align*}
		\lambda_{1}(B) \geq \lambda_{1}(A) \geq \lambda_{2}(B) \geq \lambda_{2}(A) \geq \ldots \geq \lambda_{n}(B) \geq \lambda_{n}(A).
	\end{align*}
	$(A, B)$ is strict if and only if all the inequalities are strict. 
\end{kor}

\begin{proof}
	By proposition \ref{loewner_eigenvalues} $\lambda_{k}(A) \leq \lambda_{k}(B)$, so we just need to prove that $\lambda_{k + 1}(B) \leq \lambda_{k}(A)$. Let $W$ be orthocomplement of $\vspan\{v\}$. Then $\arestr{A}{W} = \arestr{B}{W}$ and $W$ is $(n - 1)$-dimensional. Hence by lemma \ref{compression_eigenvalues} we have $\lambda_{k + 1}(B) \leq \lambda_{k}(\arestr{B}{W}) = \lambda_{k}(\arestr{A}{W}) \leq \lambda_{k}(A)$, which is what we wanted. TODO
\end{proof}

One could now use induction to make similar but more complicated statements about inequalities when compression is to subspace of bigger codimension or when $B - A$ is or larger rank. One could also ask what happens $B - A$ multiple of projection to $k$-dimensional subspace (TODO: what happens?).

One also has a similar converse as in the compression case.

\begin{prop}\label{projection_eigenvalues_con}
	For any $b_{1} \geq a_{1} \geq b_{2} \geq a_{2} \ldots \geq b_{n} \geq a_{n}$ we may find projection pair $A, B \in \H^{n}(V)$ with $a_{i}$'s and $b_{i}$'s as spectra.
\end{prop}

We will first prove this converse. The idea is the following: the eigenvalues of roots of the characteristic polynomial, hence to control eigenvalues, we should control characteristic polyomials. It turns out that if two maps differ by map rank $1$, their characteristic polynomials are intimately related.

\begin{lem}\label{projection_characteristic_polynomial}
	Let $A, B \in \H$ be a projection pair. Then
	\begin{align*}
		\det(B - z I) = \det(A - z I) \left(1 + \langle (A - z I)^{-1}v, v\rangle\right).
	\end{align*}
\end{lem}
\begin{proof}
	This is just direct application of rank 1 version of matrix determinant lemma \ref{matrix_determinant_lemma}.
\end{proof}

\begin{proof}[Proof of propostion \ref{projection_eigenvalues_con}]
	If $a_{i} = b_{j}$ for some $1 \leq i, j \leq n$ we can forget $a_{i}$ and $b_{j}$, solve the remaining problem on smaller space to get $A'$ and $v'$ and take $A : V' \oplus \C \to V' \oplus \C$ to be $A' \oplus a_{i}$ and $v = v' \oplus 0$. We may hence assume that the numbers are distinct.

	First take $A$ with the given eigenvalues. By the previous lemma we just want to choose $v$ in such a way that
	\begin{align*}
		\frac{p_{B}(z)}{p_{A}(z)} = 1 + \langle (A - z I)^{-1}v, v\rangle= 1 + \sum_{i = 1}^{n} \frac{|\langle v, e_{i} \rangle|^2}{a_{i} - z},
	\end{align*}
	where $e_{i}$'s are the eigenvectors of $A$ and $p_{A}$ and $p_{B}$ are polynomials with $a_{i}$'s and $b_{i}$'s as roots. But this is easily achieveable if can show that the residues of $p_{B}(z)/p_{A}(z)$ are negative, which follows easily from the interlacing property.

	From the identity we can also easily deduce the other direction. If $\langle v, e_{i} \rangle \neq 0$ for any $1 \leq i \leq n$ the function
	\begin{align*}
		z \mapsto 1 + \sum_{i = 1}^{n} \frac{|\langle v, e_{i} \rangle|^2}{a_{i} - z}
	\end{align*}
	has $n$ poles of negative residue so it has a root between any two poles. Also it tends to $1$ at infinity so it has also root on $(a_{1}, \infty)$.
\end{proof}

The proof of \ref{compression_eigenvalues_con} is similar: the aim to first connect the characteristic polynomials of $A$ and its compression and then do similar observations.

\begin{lem}
	Let $A \in \H(V)$ and $W \subset V$ a subspace of codimension $1$, orthocomplement of subspace spanned by unit vector $v$. Then
	\begin{align*}
		\det(\arestr{A}{W} - z I) = \det(A - z I) \langle (A - z I)^{-1}v, v\rangle
	\end{align*}
\end{lem}
\begin{proof}
	This is direct application of \ref{compression_determinant}.
\end{proof}

\begin{proof}[Proof of proposition \ref{compression_eigenvalues_con}]
	Proof is just an easier version of the proof of \ref{projection_eigenvalues_con}
\end{proof}

TODO: change order of compression and projection eigenvalues converses.

\section{Notes and references}

\section{Ideas}

\begin{itemize}
	\item Normal maps
	\item Square root of a matrix
	\item Ellipses map to ellipses
	\item adjoints of vectors
	\item Moore-Penrose pseudoinverse
	\item (canonical, löwdin) orthogonalization, polar decomposition and orthogonal Procrustes problem
	\item projection matrices
	\item Hilbert-Schmidt norm ($\to$ matrix functions?) and inner product
	\item Hilbert spaces
	\item Real vs. complex
	\item Positive definite kernels
	\item Weakly positive matrices
	\item Hlawka inequality for determinant %http://mathoverflow.net/questions/182181/hlawka-inequality-for-determinants-of-positive-definite-matrices?rq=1
	\item Trace-characterization of positive maps.
	\item Splitting positive maps to pseudo square roots
	\item Product of maps
	\item Exponential formula for geometric mean?
	\item Maximum of matrices with powerlimit
	\item If $A, B$ are Hermitian, what eigenvalues $AB$ can have? What if the eigenvalues are known? What about $AB + BA$. What eigenvalues $A$ can have if eigenvalues of $\Re(A)$ are known.
	\item It seems to be the case that if $n = 2$, and $A$ is Hermitian with $\spec(A) = \{\lambda_{1}, \lambda_{2}\}$ ($\lambda_{1} \leq \lambda_{2}$), then there exists linear $B$ such that $\Re(B) = A$, and $\spec(B) = \{\mu_{1}, \mu_{2} \}$ if and only if $\Re(\mu_{1} + \mu_{2}) = \lambda_{1} + \lambda_{2}$ and $\lambda_{1} \leq \Re(\mu_{i}) \leq \lambda_{2}$. In general this is known as Ky-Fan theorem, according to \cite{Ando3}.
	\item Let's define $A \leq_{2} B$ if $\tr(A) = \tr(B)$ and for any $t \in \R$ we have $\tr(|A - t I|) \leq \tr(|B - t I|)$. Similarly we can define $A \leq_{k} B$. This easily (?) defines a partial order on matrices. But know we lose all the data about the eigenvectors? Is there a way to bring it back? Is there some nice interpretation.
	\item One would like to get such order with restrictions. Maybe this is related to sectional curvature.
	\item What happens if $n = 2$, $A, B \in \H$, and $\tr(B) = 0$ and $\tr(AB) \geq 0$. What can be said about the relation between $A$ and $A + B$.
	\item We have up to first order that if $\lambda_{1} \geq \lambda_{2} \geq \ldots \geq \lambda_{n}$ are the eigenvalues of $A$, with respective eigenvectors $v_{i}$, then we should have
	\[
		\sum_{i = 1}^{k} \langle \dot{A} v_{i}, v_{i}\rangle \geq 0,
	\]
	for any $1 \leq k \leq n$ with equality for $k = n$.
	\item Is the right condition something like: for any $t \in \R$ we should have $A \cdot \chi_{(t, \infty)} \leq B \cdot \chi_{(t, \infty)}$ or something like that.
	\item Does the following work? We say that $A \leq_{2} B$ if for any orthonormal basis $(e_{i})_{i = 1}^{n}$ we have
	\[
		(\langle A e_{i}, e_{i} \rangle)_{i = 1}^{n} \prec_{2} (\langle B e_{i}, e_{i} \rangle)_{i = 1}^{n}.
	\]
	Does this correspond to the case $n = 1$? This probably doesn't work: if $n = 2$, $\tr(A) = \tr(B) = 0$ and $e_{1}$ is in Kernel of $B$, then the right-hand sequence is zero sequence.
	\item Lorenz order?
	\item BMV-conjecture (theorem)
	\item Proof difficulties
	\item Proof ``sketch" (as in joke)
	\item Positive linear functions $\H \to \R$.
	\item What about positive linear functionals form $\H^{n} \to \H^{m}$?
	\item Power series for positivity of inverse function.
	\item Two notions of positivity: spectral and quadratic form. First works well with functional calculus and second with linear phenomena, but one shouldn't mix these two things.
\end{itemize}




