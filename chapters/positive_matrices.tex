\chapter{Positive matrices}

This chapter is titled ``positive matrices", although ``positive maps" might be more appropriate title. We are mostly going to deal with finite-dimensional objects, but many of the ideas could be generalized infinite-dimensional settings, where matrices lose their edge. Also, one should always ask whether it really clarifies the situation to introduce concrete matrices: matrices are good at hiding the truly important properties of linear mappings. The words ``matrix" and ``linear map" are used somewhat synonymously, although one should always remember that the former are just special representations for the latter.

\section{Motivation}

How should one order matrices? What should we require from ordering anyway?

I could just give you the answer, but instead I try to explain why it is standard in the first place.

We would definitely like to have natural total order on the space of matrices, but it turns out that are no natural choices for that. Partial order is next best thing. Recall that a partial order on a set $X$ is a binary relation $\leq$ on such that
\begin{enumerate}
	\item $x \leq x$ for any $x \in X$.
	\item For any $x, y \in X$ for which $x \leq y$ and $y \leq x$, necessarily $x = y$.
	\item If for some $x, y, z \in X$ we have both $x \leq y$ and $y \leq z$, also $x \leq z$.
\end{enumerate}

The third point is the main point, the first two are just there preventing us from doing something crazy. But we can do better: this partial order on matrices should also respect addition.
\begin{enumerate}
\item[4.] For any $x, y, z \in X$ such that $x \leq y$, we should also have $x + z \leq y + z$.
\end{enumerate}

There's another way to think about this last point. Instead of specifying order among all the pairs, we just say which matrices are positive: matrix is positive if and only it's at least $0$.

If we know all the positive matrices, we know all the ``orderings". To figure out whether $x \leq y$, we just check whether $0 = x - x \leq y - x$, i.e. whether $y - x$ is positive. Also, positive matrices are just differences of the form $y - x$ where $x \leq y$. Now, conditions on the partial order are reflected to the set of positive matrices.
\begin{enumerate}
	\item[1'.] $0$ (zero matrix) is positive.
	\item[2'.] If both $x$ and $-x$ are positive, then $x = 0$.
	\item[3'.] If both $x$ and $y$ are positive, so is their sum $x + y$.
\end{enumerate}
Here $3'$ is kind of combination of $3$ and $4$.

The terminology here is rather unfortunate. Natural ordering of the reals satisfies all of the above with obvious interpretation of positive numbers, which however differs from the standard definition: $0$ is itself positive in our above definition. This is undoubtedly confusing, but what can you do? For real numbers we have total order, so every number is either zero, strictly positive or strictly negative, so when we say non-negative, it literally means ``not negative": we get all the positive numbers and zero. But with partial orders we might get more. So the main reasons why we are using this terminology are
\begin{enumerate}
	\item It's short.
\end{enumerate}
Also, now that we have decided to preserve the word ``positive" for ``at least zero" one might be tempted to preserve ``strictly positive" for ``at least zero, but not zero". We won't do that, we save that phrase for something more important.

Back to business. When we try to define ordering of the matrices, everything of course depends on the ground field. It hardly makes any sense to order matrices over $\F_{p}$: even $1 \times 1$ matrices, namely (canonically) the elements of $\F_{p}$ defy reasonable ordering. But real numbers, for instance, have ordering, so there's a serious change that all real matrices could be ordered.

We will first try to order all real square matrices. (Actually, we won't even try to order non-square matrices.) $1 \times 1$ matrices are easy to order, but as soon one moves to larger matrices, one faces difficult decisions:

\[
	\text{Is }
	\begin{bmatrix}
		1 & 0 \\
		0 & -1 \\
	\end{bmatrix}
	\leq
	\begin{bmatrix}
		0 & 0 \\
		0 & 0 \\
	\end{bmatrix}
	\text{ or }
	\begin{bmatrix}
		1 & 0 \\
		0 & -1 \\
	\end{bmatrix}
	\geq
	\begin{bmatrix}
		0 & 0 \\
		0 & 0 \\
	\end{bmatrix}
	\text{?}
\]

That's okay, we don't necessarily have to order all pairs of matrices. But there are other problems. We would like the ordering of the matrices to be independent of the choice matrix representation. If we flip basis vectors we change the rows and colums of the matrix, so change it's sign! So if we want
\begin{itemize}
	\item Positivity is readable from the matrix with respect to an arbitrary basis, alone.
	\item Positivity doesn't depend on the chosen basis.
\end{itemize}
there is no way to assign sign to the previous matrix.

Let's consider diagonalizable matrices: by change of basis these can be written as diagonal matrices, but these diagonal values, eigenvalues, need not be real anymore. One could try to ignore the complex bases, but how often really is ignoring the complex structure satisfactory. Another approach would be only to consider matrices with real eigenvalues. There's problem here though: sum of two matrices with real eigenvalues need not have real eigenvalues, even if the former are diagonalizable! Sad as it may sound, it shouldn't be too surprising since it's not very clear that eigenvalues should have any reasonable behaviour with respect to addition. Of course, one shouldn't just take my word for it: here are congrete examples:
\[
	\begin{bmatrix}
		0 & 4 \\
		1 & 0
	\end{bmatrix}
	\text{ and }
	\begin{bmatrix}
		0 & -1 \\
		-4 & 0
	\end{bmatrix}
\]
The two matrices have both distinct eigenvalues $- 2$ and $2$ and are hence diagonlizable, but their sum has characteristic polynomial $x^2 + 9$, which most definitely has no real zeros.

There is however very special class of matrices, correponding to special class of linear mappings, which satisfy our requirements.
\begin{itemize}
	\item They have real eigenvalues.
	\item They are all diagonalizable.
	\item They are closed under sum, and (real) scalar multiplication.
\end{itemize}

\section{Hermitian maps}

Hermitian mappings will be our non-commutative playing ground in which we can define rather natural partial order. Fix any finite-dimensional inner-product space $(V, \langle \cdot, \cdot \rangle )$ over $\R$ or $\C$. For any $v \in V \setminus \{0\}$ we may define the corresponding projection, denoted by $P_{v}$ by setting $P_{v}(x) = \langle x , v \rangle /\langle v, v \rangle v$.

\begin{maar}
	Let $V$ be an finite-dimensional innerproduct space over $\R$ or $\C$. Now set of \textit{Hermitian maps} of $V$, denoted by $\H(V)$, is defined as
	\[
		\vspan \{P_{v} \mid v \in V \setminus \{0\}\},
	\]
	where the span is $\R$-linear, i.e. Hermitian maps are the maps of the form
	\[
		\sum_{i = 1}^{m} \lambda_{i} P_{v_{i}},
	\]
	for some positive integer $m$, $v_{i} \in V \setminus \{0\}$ and $\lambda_{i} \in \R$.
\end{maar}

Also note that since for any non-zero $\alpha$ and $v \in V \setminus \{0\}$ we have $P_{v} = P_{\alpha v}$, we could just as well only allow vectors of norm one in our definition of Hermitian maps. This is of course just to simplify notation. This is not the standard definition, and in a way it's horrible, but it very directly answers our needs. Remember that we want to have diagonalizable maps with real eigenvalues. Projections are very prototypical examples of such maps and since we want real eigenvalues that is simply what we require in the definition (span over $\R$). Lastly, one of course wants to take the span to have any linear structure in the first place.

It is however very surprising that such construction works: basis vectors (projections) satisfy our requirements, but there are no reasons to expect that these would be preserved in addition. This is guaranteed by the following theorem.

\begin{lause}[Spectral theorem]
	Let $V$ be an $n$-dimensional inner-product space over $\R$ or $\C$, and $A \in \H(V)$. Then there exists real numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$ and orthonormal vectors $v_{1}, v_{2}, \ldots, v_{n}$ such that
	\begin{align}\label{spectralrepr}
		A = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}}.
	\end{align}
\end{lause}

It follows that $A$ is diagonalizable, vectors $v_{1}, v_{2}, \ldots, v_{n}$ are eigenvectors of $A$ with corresponding eigenvalues $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$. Indeed, for any $v_{j}$ we have
\begin{align}
	A v_{j} = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}} v_{j} = \sum_{i = 1}^{n} \lambda_{i} \langle v_{j}, v_{i}\rangle v_{i} = \sum_{i = 1}^{n} \lambda_{i} \delta_{ij} v_{i} = \lambda_{j} v_{j}.
\end{align}

Moreover, the rank of $A$ is simply the number of non-zero eigenvalues of $A$, and the all the eigenvalues appear in the representation exactly as many times the characteristic polynomial allows.

\begin{proof}[Proof (of the Spectral theorem)]
	We prove the statement by induction on $n$ the dimension of the space $V$. The case $n = 0$ is trivial: the span itself is trivial, and it's very easy to express $0$-mapping as a empty sum.

	Now fix a positive integer $n$. 

	\textit{Step 1.} First note $A$ has at least one eigenvector over $\C$, as every other linear mapping. Let $v_{1}$ be that eigenvector (of lenght one) with corresponding eigenvalue $\lambda_{1}$.

	\textit{Step 2.} Write $A = \sum_{i = 1}^{m} c_{i} P_{u_{i}}$ for $c_{1}, c_{2}, \ldots, c_{m} \in \R$ and $u_{1}, u_{2}, \ldots, u_{m} \in V \setminus \{0\}$ are of norm $1$. Now the definition of the eigenvector and eigenvalue rewrites to
	\[
		A v_{1} = \sum_{i = 1}^{m} c_{i} \langle v_{1}, u_{i}\rangle u_{i} = \lambda_{i} v_{1}.
	\]
	Take inner product with $v_{1}$ from both sides to get
	\[
		\sum_{i = 1}^{m} c_{i} \langle v_{1}, u_{i}\rangle \langle u_{i}, v_{1} \rangle = \lambda_{1} \langle v_{1}, v_{1} \rangle.
	\]
	Since $\langle v_{1}, u_{i}\rangle \langle u_{i}, v_{1} \rangle = |\langle v_{1}, u_{i}\rangle|^{2}$, the left-hand side is real, so is right-hand side, and finally so is $\lambda_{1}$.

	\textit{Step 3.} Now we have found $\lambda_{1}$ and $v_{1}$ as in the theorem statement. The idea is then to factorize $A$ to two parts: projection corresponding to $v_{1}$ and an orthogonal part living in $v_{1}^{\perp}$. If our choices for $\lambda_{1}$ and $v_{1}$ work, anything orthogonal to $v_{1}$ maps to something orthogonal to $v_{1}$. We verify this.

	Take any $v \perp v_{1}$. Now,
	\begin{eqnarray*}
		\langle A v, v_{1} \rangle &=& \langle \sum_{i = 1}^{m} c_{i} \langle v, u_{i}\rangle u_{i}, v_{1} \rangle \\
		= \sum_{i = 1}^{m} c_{i} \langle v, u_{i}\rangle \langle u_{i}, v_{1} \rangle &=& \overline{\sum_{i = 1}^{m} c_{i} \langle v_{1}, u_{i}\rangle \langle u_{i}, v \rangle} \\
		= \overline{\langle \sum_{i = 1}^{m} c_{i} \langle v_{1}, u_{i}\rangle u_{i}, v \rangle} &=& \overline{\langle A v_{1}, v \rangle} \\
		= \overline{\langle \lambda_{1} v_{1}, v \rangle} &=& 0,
	\end{eqnarray*}
	so also $A v \perp v_{1}$. It follows that we get a map $A' : v_{1}^{\perp} \to v_{1}^{\perp}$ which extends to $A$.

	\textit{Step 4.} We would naturally like to use our induction hypothesis for this map, but for that we need it to be of special form: although the map is well defined, there is no reason to expect that we could just pick some of the $u_{i}$:s for our representation. But we can do something else.

	Write $u_{i} = u_{i}' + \alpha_{i} v_{1} = P_{v_{1}^{\perp}} + P_{v_{1}}$. Now if $v \perp v_{1}$, we have
	\begin{eqnarray*}
		A v &=& \sum_{i = 1}^{m} c_{i} \langle v, u_{i}\rangle u_{i} \\
		&=& \sum_{i = 1}^{m} c_{i} \langle v, u_{i}' + \alpha_{i} v_{1}\rangle (u_{i}' + \alpha_{i} v_{1}) \\
		&=& \sum_{i = 1}^{m} c_{i} \langle v, u_{i}'\rangle u_{i}' +  v_{1} \sum_{i = 1}^{m} c_{i} \langle v, u_{i}'\rangle \alpha_{i}.
	\end{eqnarray*}
	Now since $u_{i}' \perp v_{1}$, the latter sum vanishes, and
	\[
		A' = \sum_{i = 1}^{m} c_{i} P_{u_{i}'}.
	\]
	This representation meets our requirements, so we can write $A' = \sum_{i = 2}^{n} \lambda_{i} P_{v_{i}}$, for some $\lambda_{2}, \ldots, \lambda_{n} \in \R$ and $v_{2}, \ldots, v_{n} \in v_{1}^{\perp}$, and adjoining $v_{1}$ and $\lambda_{1}$, we get the required representation.
\end{proof}

Representation \ref{spectralrepr}, which we will call \textit{spectral representation} is by no means unique. If $A = I$ for instance, we could choose the vectors $v_{i}$ rather arbitrarily. It also should be made clear why such representation is useful. First of all, calculating with spectral representation is easy. If $A = \sum_{i = 1} \lambda_{i} P_{v_{i}}$,
\[
	A^{2} = \left(\sum_{i = 1} \lambda_{i} P_{v_{i}}\right) \left(\sum_{j = 1} \lambda_{j} P_{v_{j}}\right) = \sum_{i = 1}^{n} \sum_{j = 1}^{n}\lambda_{i} \lambda_{j} P_{v_{i}} P_{v_{j}} = \sum_{i = 1}^{n} \lambda_{i}^2 P_{v_{i}},
\]
since $P_{v}P_{w}$ for any orthogonal $v, w \in V$. There is an obvious generalization for higher powers, and polynomials too. Similar identities work for more general diagonalizable maps, but now we have additionally projection representation. Spectral representation has however many other nice properties not shared with the more general diagonalizable maps: operator norm of map is determined by its eigenvalues only. More precisely $\|A\| = \max_{i = 1}^{n} |\lambda_{i}|$. We will come back to this in a minute.

It should be noted that we only used one special property of ($\R$ linear combination of) projections $A$ in the proof:
\[
	\langle A v, w \rangle = \overline{\langle A w, v \rangle} = \langle v, A w \rangle,
\]
for any $v, w \in V$. In the first step we didn't use any properties of $A$. In the step 3 this is exactly what we do in the manipulation. Also in the step 2 the idea is to show that
\[
	\lambda_{1} \langle v_{1}, v_{1} \rangle = \langle A v_{1}, v_{1} \rangle \in \R,
\]
but $\langle A v_{1}, v_{1} \rangle = \overline{\langle A v_{1}, v_{1} \rangle}$ so $\langle A v_{1}, v_{1} \rangle \in \R$. Of course here we didn't need the fact that $v_{1}$ is an eigenvector in any way. Step 4 is just simpler: once we have constructed $A'$, it will obviously have the same property as $A$.

In all of the above it doesn't make much difference whether we have $\R$ or $\C$ as the ground field, since the eigenvalues will be anyway real. Especially when one is working over $\R$, one might whether there is a more straightforward way to get through step $1$: get rid of the complex numbers. There is. Another way to find the eigenvector and eigenvalue is to look at the so called \textit{Rayleigh quotient}
\[
	R(A, v) = \frac{\langle A v, v \rangle}{\langle v, v \rangle},
\]
when $v \in V \setminus \{0\}$. This is scale-invariant bounded real quantity so it attains maximum somewhere outside zero, say at $v$. We claim that $v$ is an eigenvector of $A$. Looking at $v_{t} = v + t w$ and differentiating at zero yields
\begin{eqnarray*}
	0 = \frac{d}{dt} R(A, v_{t})\Big|_{t = 0} &=& \frac{(\langle A v, w \rangle + \langle A w, v \rangle) \langle v, v \rangle - (\langle v, w \rangle + \langle w, v \rangle) \langle Av, v \rangle}{\langle v, v \rangle^2} \\
	&=& \frac{\Re\left(\langle A v, w \rangle \langle v, v \rangle -  \langle v, w \rangle \langle Av, v \rangle \right)}{\langle v, v \rangle^2}.
\end{eqnarray*}
We write $A v = \lambda v + v'$ where $v \perp v'$ and set $w = v'$. Now
\begin{eqnarray*}
	0 &=& \frac{\Re\left(\langle \lambda v + v', v' \rangle \langle v, v \rangle -  \langle v, v' \rangle \langle \lambda v + v', v \rangle \right)}{\langle v, v \rangle^2} \\
	&=& \frac{\Re\langle v', v' \rangle \langle v, v \rangle}{\langle v, v \rangle^2},
\end{eqnarray*}
so $v' = 0$ and $v$ is an eigenvector.

\section{Self-adjoint maps}

\begin{maar}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an finite-dimensional inner-product space. \textit{Adjoint} of a linear map $A : V \to V$ is a linear map $A^{*} : V \to V$ such that
	\[
		\langle A v, w \rangle = \langle v, A^{*} w \rangle
	\]
	for any $v, w \in V$.
\end{maar}

\begin{maar}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an finite-dimensional inner-product space. A linear map $A : V \to V$ is \textit{self-adjoint} if it is its own adjoint, i.e.
	\[
		\langle A v, w \rangle = \langle v, A w \rangle
	\]
	for any $v, w \in V$.
\end{maar}

The spectral theorem shows that Hermitian maps correspond exactly to the self-adjoint maps. It is the good property of projections that made the Spectral theorem work: we got to move mappings from one side of inner-product to another. Generally the mapping changes, and adjoint is what comes out. We gather here many of the important properties of adjoint.

\begin{lause}\label{basic_adjoint}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an $n$-dimensional inner-product space, $A, B \in \L(V)$ linear, and $\lambda \in \C$. Then
	\begin{enumerate}[i)]
		\item $A$ has an unique adjoint
		\item Matrix of $A^{*}$ with respect to any orthonormal basis is conjugate transpose of matrix of $A$, i.e. $A^{*}_{i, j} = \overline{A_{j, i}}$.
		\item $(A^{*})^{*} = A$
		\item $(A + B)^{*} = A^{*} + B^{*}$
		\item $(\lambda I)^{*} = \overline{\lambda} I$
		\item $(AB)^{*} = B^{*}A^{*}$.
		\item $\kernel(A) = \image(A)^{\perp}$
	\end{enumerate}
\end{lause}
\begin{proof}
	\begin{enumerate}[i)]
		\item Fix any orthonormal basis of $V$, $(e_{i})_{i = 1}^{n}$. Now for any $v, w \in V$
		\begin{eqnarray*}
			\langle A v, w \rangle &=& \langle A \sum_{i = 1}^{n} \langle v, e_{i} \rangle e_{i}, w\rangle\\
			&=& \sum_{i = 1}^{n} \langle A e_{i}, w \rangle \langle v, e_{i} \rangle \\
			&=& \langle v, \sum_{i = 1}^{n} \langle w, A e_{i} \rangle e_{i} \rangle,
		\end{eqnarray*}
		so we should set $A^{*} w = \sum_{i = 1}^{n} \langle w, A e_{i} \rangle e_{i}$. This clearly defines an linear mapping $A^{*}$. Also, if $A$ has two adjoints, their difference $C$ is linear map such that $\langle v, C w \rangle = 0$ for any $v, w \in V$. Setting $v = C w$ we get that $C w = 0$ for any $w \in V$ so the adjoints are equal.
		\item Observe that $A^{*}_{i, j} = \langle A^{*}e_{j}, e_{i} \rangle = \overline{\langle e_{i}, A^{*} e_{j} \rangle} = \overline{ \langle A e_{i}, e_{j} \rangle} = \overline{A_{j, i}}$.
		\item We have for any $v, w \in V$ that $\langle A^{*} v, w \rangle = \overline{\langle w, A^{*} v \rangle} = \overline{\langle A w, v \rangle} = \langle v, A w \rangle$, so $A$ is adjoint of $A^{*}$.
		\item Since $\langle (A + B) v, w \rangle = \langle A v, w \rangle + \langle B v, w \rangle = \langle v, A^{*} w \rangle + \langle v, B^{*} w \rangle = \langle v, (A^{*} + B^{*})w \rangle$, $A^{*} + B^{*}$ is adjoint of $A + B$.
		\item We simply calculate that $\langle \lambda v, w \rangle = \lambda \langle v, w \rangle = \langle v, \overline{\lambda} v \rangle$.
		\item Note that $\langle A B v, w \rangle = \langle B v, A^{*}w \rangle = \langle v, B^{*} A^{*} w \rangle$.
		\item We have $v \in \kernel(A)$ i.e. $A v = 0$ if and only $\langle A v, w \rangle = \langle v, A w \rangle$ for any $w \in V$, which is to say that $v \perp \image(A)$.
	\end{enumerate}
\end{proof}

\section{Operator norm}

As we noticed, if $A \in \L(V)$ is self-adjoint $\langle A v, v \rangle \in \R$ for any $v \in V$. This motivates us to define the quadratic form of $A$, a map $Q_{A} : V \to \R$, by setting $Q_{A}(v) = \langle A v, v \rangle$. Similarly, if $Q_{A}$ is real, $A$ is self-adjoint. Indeed, looking at $Q_{A}(v + t w) = Q_{A}(v) + |t|^2 Q_{A}(w) + \overline{t} \langle A v, w \rangle + t \langle A w, v \rangle$ we see that $\overline{t} \langle A v, w \rangle + t \langle A w, v \rangle \in \R$ for any $v, w \in V$ and $t \in \C$. Still $\overline{t} \langle A v, w \rangle + t \langle A w, v \rangle = \overline{t} \langle A v, w \rangle + t \overline{\langle A v, w \rangle} - t \langle w, A v \rangle   + t \langle A w, v \rangle = \Re(\overline{t} \langle A v, w \rangle) + t (\langle A w, v \rangle  - \langle w, A v \rangle) $ so we must have $\langle A w, v \rangle  = \langle w, A v \rangle$.

Rayleigh quotient is sort of a scale-invariant version of the quadratic form, and that's why it captures many of the properties of self-adjoint maps. It is also a good way to think about the operator norm of a linear map. Take any $\H(V) \ni A = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}}$ and also any $V \setminus \{0\} \ni x = \sum_{i = 1}^{n} x_{i} v_{i}$. Now
\[
	R(A, x) = \frac{\langle A x, x \rangle}{\langle x, x \rangle} = \frac{\sum_{i = 1}^{n} \lambda_{i} |x_{i}|^2}{\sum_{i = 1}^{n} |x_{i}|^2},
\]
the Rayleigh quotient is a weighted average of the eigenvalues of $A$. Now if $A$ is any linear map, and $x \in V \setminus \{0\}$
\[
	\|A x\|^{2} = \langle A x, A x \rangle = \langle A^{*} A x, x \rangle = R(A^{*} A, x) \|x\|^{2}
\]
Now $A^{*} A$ is self-adjoint. Also its eigenvalues must be non-negative, since the Rayleigh quotient is: square roots of these values are called the \textit{singular values} of $A$. These considerations make it also clear that the largest singular value of $A$ is the operator norm of $A$. If $A$ is self-adjoint itself, singular values are simply the absolute values of the eigenvalues, and the operator norm is the maximum of these absolute values, as claimed before. More generally, if $A$ is normal, i.e. we can write $A = \sum_{i = 1}^{n}\lambda_{i} P_{v_{i}}$ with $\lambda_{i} \in \C$ and $v_{i}$'s orthonormal, we have
\[
	A^{*} A = \left(\sum_{i = 1}^{n} \overline{\lambda_{i}} P_{v_{i}}\right) \left( \sum_{j = 1}^{n} \lambda_{j} P_{v_{j}} \right) = \sum_{i = 1}^{n} |\lambda_{i}|^2 P_{v_{i}},
\]
so again singular values are the absulute values of the eigenvalues and $\|A\| = \max_{i = 1}^{n} |\lambda_{i}|$. Rayleigh quotient also has interpratation for normal maps, it is again weighted average of the eigenvalues. When $x$ ranges over non-zero vectors, or equivalently over some sphere, $R(A, x)$ ranges over the convex full of the eigenvalues.

Many of the previous properties don't hold for general linear maps. Also, singular values are absolute values of eigenvalues exactly for the normal maps.


\section{Intuition}

One way to understand adjoints is to look at a bit more general case. Fix any two finite-dimensional inner-product spaces, $(V, \langle \cdot, \cdot \rangle_{V})$ and $(W, \langle \cdot, \cdot \rangle_{W})$, not necessarily of the same dimension, over, say, $\C$.
Now given any linear $A \in \L(V, W)$, adjoint is a linear map $A^{*} \in \L(W, V)$ such that for any $v \in V$ and $w \in W$ we have
\[
	\langle A v, w \rangle_{W} = \langle v, A^{*} w \rangle_{V}.
\]
Again, adjoint exists and it's unique. Fix any $w \in W$. Vector $w$ induces a linear mapping $W \to \C$ by $w' \mapsto \langle w', w \rangle_{W}$. Hence we get a map $\Phi_{W} : W \to W^{*}$ where $V^{*}$ is the dual of $V$. It turns out that $\Phi_{W}$ is anti-linear bijection. Anti-linearity and injectivity are easy to check, and since both spaces have same dimension, map has to be also surjection.

Composing with $A$, $\Phi_{W}$ induces a map $W \to V^{*}$ given by $w \mapsto \Phi_{W}(w) \circ A$. Now we finally compose this with $\Phi^{-1}_{V}$ to get a map $W \to V$, the adjoint. So $A^{*}(w) = \Phi^{-1}_{V}(\Phi_{W}(w) \circ A)$.

We can still clean the definition a bit. Any linear map $A \in \L(V, W)$ induces a mapping between the correponding duals (with the order reversed) ${}^{t}A : W^{*} \to V^{*}$, given by $({}^{t}A)(\phi) = \phi \circ A$ for any $\phi \in W^{*}$. With this interpretation in mind we rewrite $w \mapsto \Phi_{W}(w) \circ A = {}^{t}A \circ \Phi_{W}$, and finally
\[
	A^{*} = \Phi^{-1}_{V} \circ {}^{t} A \circ \Phi_{W}.
\]
So the point is: from map $A \in \L(V, W)$ we (canonically) get a map ${}^{t}A : W^{*} \to V^{*}$ and since we can, thanks to the inner product, identify inner-product spaces with their duals, this gives us a map $A^{*} \in \L(W,V)$.

It should be noted that all parts of the theorem \ref{basic_adjoint} carry directly to this more general setup with obvious modifications.

Inner products give also other canonical isomorphism: one between linear maps and sesquilinear forms. Sesquilinear forms are complex generalizations of bilinear forms. Sesquilinear form $B : V \times W \to \C$ is a mapping linear in its first entry and anti-linear in the second. The vector space of all sesquilinear forms on $V \times W$ is denoted by $\sesqui(V, W)$. Any linear mapping $A \in \L(V, W)$ gives rise to a sesquilinear form given by $\langle A \cdot, \cdot \rangle_{W}$. This gives us a map $\Psi_{V, W} : \L(V, W) \to \sesqui(V, W)$, and we also denote $\Psi_{V, W} A = B_{A}$. This map is an isomorphism: it can be easily seen that is linear, and injection, and since both spaces are of dimension $\dim(V) \cdot \dim(W)$, the map is also an surjection.


We also have a natural map $(\cdot)^{H} : \sesqui(V, W) \to \sesqui(W, V)$, \textit{conjugate transpose} given by $B^{H}(w, v) = \overline{B(v, w)}$: we simply exchange the arguments, and conjugate to preserve the sesquilinearity. Now it's not hard to check that $(\cdot)^{*} = \Psi^{-1}_{W, V} \circ (\cdot)^{H} \circ \Psi_{V, W}$, so we get an another route to adjoint.

Self-adjoint maps of course only make sense when $V = W$. When $A \in \L(V)$ is a self-adjoint, $B_{A}^{H} = B_{A}$: such sesquilinear forms are called Hermitian. The diagonal of $B$ is the of course just the quadratic form, i.e. $B(v, v) = Q_{A}(v)$ for any $v \in V$.

Adjoint also behaves in many ways like (complex) conjugate.
\begin{itemize}
	\item Notions agree in $1$-dimensional spaces.
	\item Taking adjoint is an anti-linear involution.
	\item Eigenvalues of the adjoint are conjugates of the eigenvalues of the original map.
	\item If $A \in \L(V)$ has an orthonormal eigenbasis, $A^{*}$ has the same eigenbasis with conjugated eigenvalues.
	\item Self-adjoint maps correspond to real numbers: they have real eigenvalues.
\end{itemize}

Maps for which the fourth point holds are called \textit{normal}. The class of normal maps on $V$ is denoted by $\normal(V)$

One of the very good properties of conjugation is that it commutes with many operations, taking inverse, for instance. The same carries to adjoints:
\[
	I = (A^{-1}A)^{*} = A^{*} (A^{-1})^{*},
\]
so $(A^{-1})^{*} = (A^{*})^{-1}$. There's a natural generalization we'll come back to later.

\section{Commuting self-adjoint maps}

\textbf{Warning!} Composition of self-adjoint maps need not be self-adjoint!

It's rather surprising that they are closed under sums in the first place. Of course both Hermitian and self-adjoint maps are by definition, but it's rather surprising that such class has such nice properties (most of which are left to discuss).

The problem is that even if $A, B \in \H(V)$, $(AB)^{*} = B^{*} A^{*} = B A$, which is in general different from $A B$: two self-adjoint maps need not commute. Again, one should have a counterexample and almost anything works for that. We need to have $n > 1$. Also we should be able to find counterexamples that are projetions, since if any two projections commute, so do maps in their span, Hermitian maps. Finally we should not take parallel or orthogonal vectors: projections are idemponent and if one applies two orthogonal projections, result if the zero map. Actually anything else works as a counterexample. Indeed, when $v$ and $w$ are not parallel or orthogonal. Now $P_{w} P_{v} v = P_{w} v =$ something non-zero parallel to $w$, while $P_{v} P_{w}$ is something non-zero parallel to $v$ or at least not parallel to $w$. Hence $P_{v}$ and $P_{w}$ do not commute.

From the previous we see that projections commute exactly when the corresponding vectors are parallel or orthogonal. More generally one would like to determine exactly which products of self-adjoint maps are self-adjoint, i.e. which products commute. Large family of examples is given by pairs of self-adjoint maps with the same eigenvectors. Namely if $A = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}}$ and $B = \sum_{i = 1}^{n} \lambda'_{i} P_{v_{i}}$,
\[
	A B = \left(\sum_{i = 1}^{n} \lambda_{i} P_{v_{i}} \right) \left(\sum_{j = 1}^{n} \lambda'_{j} P_{v_{j}}\right) = \sum_{i = 1}^{n} \sum_{j = 1}^{n}\left(\lambda_{i} P_{v_{i}}\right)\left(\lambda'_{j} P_{v_{j}}\right) = \sum_{i = 1}^{n}\lambda_{i}\lambda'_{i} P_{v_{i}} = BA.
\]

In this case eigenvalues of the product are also products of the eigenvalus of $A$ and $B$, but not necessarily all of them. It turns out that that's all. Before going through the proof, let's get back to the spectral theorem. One of the unfortunate aspects of it is that the spectral representation is not unique. We can however make it unique. Essentially only choices we get to make in the representation are the choices for the orthonormal bases of the eigenspaces of the map, so if we replace the projections to vectors with projections to the eigenspaces, we have the uniqueness.

Recall that for arbitrary subspace of $V$, say $T$, (orthogonal) projection to $T$, denoted by $P_{T}$ is the unique linear map such that $P_{T}^2 = P_{T}$, $\image(P_{T}) = T$, $\kernel(P_{T}) = \image(P_{T})^{\perp}$. The first condition is the projection part, the second projetion being to $T$ and the third is the orthogonality. Orthogonality can be also replaced by requiring $P_{T}$ to be self-adjoint. Equivalently for any $v \in V$ $P_{T}v$ is the unique vector in $T$, such that $v - P_{T} v \perp T$. Or still equivalently, $P_{T} v$ is the unique vector in $T$ closest to $v$. Equivalence of these definitions is easy to check, and also the fact that projections to vectors used before are just projections to corresponding subspaces. Finally, if $e_{1}, e_{2}, \ldots, e_{m}$ is an orthonormal basis for an subspace $T$, $P_{T} = \sum_{i = 1}^{m} P_{e_{i}}$.

\begin{lause}[Spectral theorem]
	Let $V$ be an $n$-dimensional inner-product space and $A \in \H(V)$. Then there exists unique non-negative integer $m$, real numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}$ and non-trivial orthonormal subspace of $V$, $E_{\lambda_{1}}, E_{\lambda_{2}}, \ldots E_{\lambda_{m}}$ with $E_{\lambda_{1}} + E_{\lambda_{2}} + \ldots + E_{\lambda_{m}} = V$, such that
	\[
		A = \sum_{i = 1}^{m} \lambda_{i} P_{E_{\lambda_{i}}}.
	\]
	Moreover, this representation is unique.
\end{lause}

Here $m$ is the number of distinct eigenvalues of $A$, $\lambda_{i}$'s are the eigenvalues, and $E_{\lambda_{i}}$'s are the corresponding eigenspaces: representation is necessarily unique. Existence of such representation is immediate from the previous version of the spectral theorem.

We can also reinterpret the original proof a bit further. One of the key facts we used was that if $v$ is an eigenvector, $v^{\perp}$ is mapped to itself. This fact has rather natural generalization: if $A \in \H(V)$ and $T$ is a subspace of $V$ which maps to itself, so does its orthocomplement. Now we can decompose $A$ to $A_{T}$ and $A_{T^{\perp}}$. Put a bit differently: every self-adjoint map is canonically a product of real multiplication maps.

\begin{lause}
	Let $V$ be an $n$-dimensional inner-product space and $A_{j} \in \H(V)$ for $j \in J$ be an arbitrary family of (pairwise) commuting self-adjoint maps on $V$. Then there exists on-negative integer $m$, non-trivial orthogonal subspaces $T_{1}, T_{2}, \ldots, T_{m}$ of $V$ with $T_{1} + T_{2} + \ldots + T_{m} = V$, and sequences of real numbers $(\lambda_{i, j})_{i = 1}^{m}$ for $j \in J$ such that for any $j \in J$ we have
	\[
		A_{j} = \sum_{i = 1}^{m} \lambda_{i, j} P_{T_{i}}
	\]
	and if $i \neq i'$, for some $j \in J$ we have $\lambda_{i, j} \neq \lambda_{i', j}$. Moreover, this representation is unique.
\end{lause}

When such a representation exists, we call the family $\{ A_{j} | j \in J \}$ \textit{simultaneously diagonalizable}. We again see that the $\lambda_{i, j}$'s are eigenvalues of the corresponding $A_{j}$'s. The last requirement on the subspaces and $\lambda_{i, j}$'s is just technical minimality condition to ensure uniqueness. If we have any representation ignoring this constraint, we can just combine the subspaces if all the eigenvalues are equal.

\begin{proof}

	Let us first check the uniqueness. It is clear that if two representations share subspaces, also the corresponding $\lambda_{i, j}$'s are equal. So we may assume to the contrary that there are two such representations with subspaces $T_{1}, T_{2}, \ldots, T_{m}$ and $T'_{1}, T'_{2}, \ldots, T'_{m'}$ and $T_{1} \not\subset T_{1}'$, but $T_{1}$ and $T_{1}'$ are not orthogonal. Take $v \in T_{1}' \setminus (T_{1} \cup T_{1}^{\perp})$ and write it in the form $v_{1} + v_{1}'$ where $v_{1} \in T_{1}$ and $v_{1}' \in T_{1}^{\perp}$. For any $A_{j}$ both $v$ and $v_{1}'$ are eigenvalues not orthogonal, so they correspond to the same eigenvalue. Hence $v_{1}'$ is also a eigenvector for any $A_{j}$ corresponding to the same eigenvalue. Now $v_{1}'$ can't be orthogonal to everything in $T_{1}'$ so it is not orthogonal to some vector $v_{2}$ in $T_{2}$, say. As before we see that for any $A_{j}$ the eigenvalue corresponding to $T_{1}$ is same as the eigenvalue corresponding to $v_{2}$, and so $T_{2}$. This means that the first representation is not minimal, we could have combined the $T_{1}$ and $T_{2}$, a contradiction.
 
	We proceed by induction on $n$, the dimension of the space. The case $n = 0$ is trivial. When $n > 0$ if we look at the spectral representations of the maps $A_{j}$. If all of them simple representation ($m = 1$), i.e. they are multiples of identity map, we can also choose $m = 1$, let $\lambda_{1, j}$'s be the corresponding unique eigenvalue of $A_{j}$ and let $T_{1} = V$.

	Assume then that some $A_{j}$ has non-trivial decomposition and let $T$ be an subspace in the decomposition, eigenspace of $A_{j}$, with eigenvalue $\lambda$. We know that $A_{j}$ itself can be decomposed into maps living in $T$ and $T^{\perp}$: we shall prove that same is true about all the other maps $A_{j'}$.

	Take any $j' \in J$. We want to prove that $A_{j'} T \subset T$ Since $A_{j'}$ and $A_{j}$, for any $v \in T$ we have
	\[
		\lambda A_{j'} v = A_{j'} A_{j} v = A_{j} A_{j'} v.
	\]
	If $A_{j'} v = 0$, everything's fine. If not, $A_{j'}v$ is an eigenvector of $A_{j}$ with eigenvalue $\lambda$: it lives by definition in $T$. It follows that also $A_{j'} T^{\perp} \subset T^{\perp}$, and $A_{j'}$ factorizes to maps in $T$ and $T^{\perp}$.

	Once we perform this factorization for all the maps, we have reduced our problem to smaller case: $T$ was a proper non-trivial subspace of $V$. It follows by induction that we can find factorizations as in the statement of the theorem for both sides, and we simply put them together to get final representation. This representation is also minimal, as is rather is easy to check, put even if it wasn't, we could cook up such as explained before the proof.
\end{proof}

From the proof we also see that any representation ignoring the minimality condition is just a subdivision of a minimal one: the subspaces are further divided to subspaces.

Previous theorem also sheds some light to the normal maps. Take any normal map $A = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}}$. When the eigenvalues are split to real and imaginary parts $\lambda_{i} = a_{i} + i b_{i}$, we can write $A = \sum_{i = 1}^{n}a_{i} P_{v_{i}} + i \sum_{i = 1}^{n}b_{i} P_{v_{i}}$, where now $\sum_{i = 1}^{n}a_{i} P_{v_{i}}$ and $\sum_{i = 1}^{n}b_{i} P_{v_{i}}$ are self-adjoint: we have decomposed normal map itself to its real and imaginary part. This construction can be done in general too.  We will denote $\Re(A) = \frac{1}{2}(A + A^{*})$ and $\Im(A) = \frac{1}{2 i}(A - A^{*})$. For general linear map $A$, we can write $A = \Re(A) + i \Im(A)$. While again both $\Re(A)$ and $\Im(A) = \Re(A/i)$ are self-adjoint, $i \Im(A)$ is ``purely imaginary": such maps are called \textit{skew-Hermitian}: they are maps satisfying $A^{*} = -A$. We see that if $A$ is normal, the eigenvalues of $\Re(A)$ and $\Im(A)$ are exactly the real and imaginary parts of the eigenvalues of $A$, respectively.

For any linear map $A$ we can definitely find spectral representation for its real and imaginary parts. If they are simultaneously diagonalizable, they give rise to the spectral representation of the original map. By the previous theorem this is equivalent to real and imaginary parts commuting, which by short computation is equivalent to $A^{*}A = A A^{*}$: $A$ commutes with its adjoint. It follows that if $A$ commutes with its adjoint, $A$ is normal. Also if $A$ is normal, $A$ definitely commutes with its adjoint, so these conditions are equivalent. The previous is actually a very common definition of normal map.

\section{Unitary maps}

Besides self-adjoint maps, there is other very nice subfamily of normal maps: unitary maps. Unitary maps are normal maps with eigenvalues on unit circle. Such maps satisfy
\[
	A^{*} A = \sum_{i = 1}^{n} \overline{\lambda_{i}} P_{v_{i}} \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}} = \sum_{i = 1}^{n} |\lambda_{i}|^{2} P_{v_{i}} = I,
\]
On the other hand, if $A \in \L(V)$ with $A^{*}A = I$, then $A^{*}$ is inverse of $A^{-1}$, and $A^{*}$ commutes with $A$: $A$ is normal. By previous computation we see that all the eigenvalues of $A$ are on unit circle, hence map is unitary, if and only if $A^{*} A = I$. Finally if $A$ is unitary, for any $v, w \in V$ we have
\[
	\langle A v, A w \rangle = \langle v, A^{*}A w \rangle = \langle v, w \rangle,
\]
so $A$ preserves innerproduct. Again, if $A$ preserves innerproducts, for any $v, w \in V$ we have
\[
	\langle v, w \rangle = \langle A v, A w \rangle = \langle v, A^{*} A w \rangle,
\]
so $A^{*}A = I$: we have third equivalent definition for unitary maps. This definition might be also most enlightening: unitary maps are exactly the automorphisms of the inner-product space $V$.

There is also fourth equivalent definition for unitary maps: they are the isometries of $V$. It's clear that any unitary map is isometry, but the other direction might not be entirely obvious. The innerproduct can however be recovered from the norm. If $V$ is over $\R$, we have
\[
\langle v, w \rangle = \frac{1}{4} \left(\|v + w\|^{2} - \|v - w\|^{2} \right).
\]
It follows that if $A$ is an isometry, for any $v, w \in V$ we have
\[
\langle Av, Aw \rangle = \frac{1}{4} \left(\|Av + Aw\|^{2} - \|Av - Aw\|^{2} \right) = \frac{1}{2} \left(\|v + w\|^{2} - \|v - w\|^{2} \right) = \langle v, w \rangle.
\]
If $V$ is over $\C$ instead, we have to modify our polarization identity a little:
\[
\langle v, w \rangle = \frac{1}{4} \left(\|v + w\|^{2} - \|v - w\|^{2} + i \|v + i w \|^{2} - i \|v - i w\|^{2}\right).
\]

Both the third and the fourth definitions make it clear that the unitary maps on $V$ form a group. This group is often denoted by $\text{Hilb}(V)$, but we will use $\unitary(V)$, for brevity, and because $\H$ is preserved.

TODO: sesquilinear forms vs. quadratic form correspondence.

TODO: Unitary maps and matrices

TODO: Cayley transform

\section{Positive maps}

\begin{maar}
	Hermitian map is said to be \textit{positive} if all of its eigenvalues are non-negative.
\end{maar}

Such maps are usually called \textit{positive semi-definite}: the term \textit{positive definite} is usually preserved for the following class.

\begin{maar}
	Hermitian map is said to be \textit{strictly positive} if all of its eigenvalues are positive.
\end{maar}

``$A$ is positive" is denoted by $A \geq 0$, and ``$A$ is stricly positive" by $A > 0$. One could adopt analogous notation for (strictly) negative maps, definition of which should be clear. The set of positive maps is denoted by $\H_{+}(V)$, shortened to $\H_{+}$. Here is a big list of important facts and equivalent definitions for the previous.

\begin{itemize}
	\item A map is positive if and only if its quadratic form, or equivalently the Rayleigh quotient is non-negative. This is clear since we noticed the Rayleigh is just a weighted average of the eigenvalues, and quadratic form is just a positively scaled version of Rayleigh quotient. Similarly, a map is strictly positive if and only if the Rayleigh quotient is positive, of the quadratic form is positive, except at $0$. In both instances one could replace the non-negativity/positivity on everywhere/outside zero by requiring it only on unit sphere, for instance.
	\item Previous point makes it clear that sum of (strictly) positive maps is (stricly) positive. Analogous claim is true for positive scalar multiple.
	\item A map is strictly positive if and only if the respective sesquilinear form is inner-product. Inner-products can hence also be naturally called stricly positive.

	Positive map don't necessarily lead to inner-products, but to a so called semi-definite sesquilinear forms, which are here also called positive (sesquilinear) forms.
	\item Composition of positive maps need not be positive, but this is just because it need not be even Hermitian. If it is, however, by our previous results, the maps commute, are simultaneosly diagonalizable and they're composition is positive: its eigenvalues are (some of the) products of the eigenvalues of the original maps. Similar statement holds for striclty positive maps.
	\item The class of positive maps is topological closure (with operator norm topology) of the class of stricly positive maps. This is clear, and the closedness is very useful. Also, the class of stricly positive maps is the interior the class of positive maps. The reason is the following: if $A \geq 0$, $Q_{A} \geq \delta_{K} > 0$ for any compact set $K \subset V$ not containing $0$, so in particular on unit sphere. Now if $B \in \L(V)$ has operator norm $< \delta$, $Q_{A + B}(v) \geq Q(A)(v) - |Q_{B}(v)| > \delta - \delta$, so $Q$ is positive on unit circle, and thus outside zero. 
	\item Squares are positive, i.e. $A^{2} \geq 0$ for any Hermitian $A$. Also, any positive map is a square, and has an unique positive square root. These claims follows easily from the better version of the spectral theorem. Positive square root of $A \geq 0$ is denoted by $A^{\frac{1}{2}}$, as one would hope.
	%\item One can rewrite $Q_{A}(v) = \langle A v, v \rangle = \langle A^{\frac{1}{2}} A^{\frac{1}{2}} v, v \rangle = \langle A^{\frac{1}{2}} v, A^{\frac{1}{2}} v \rangle= \|A^{\frac{1}{2}} v\|^{2}$.
	\item Projections are positive: their eigenvalues belong to $\{0, 1\}$.
	\item We already noticed that maps of the form $A^{*}A$ are positive: their Rayleigh quotient is non-negative. Every positive map is also of this form (square root fits the bill), but not uniquely. 
	\item Strictly positive maps are central at the study of the local maxima and mimima of multivariate functions. Let $V$ be real inner-product space and $f : V \to \R$ be a twice continuously differentiable. Now me write $f(x + y) = f(x) + G_{x}(y)  + B_{x}(y, y) + o(\|y\|^{2})$, where $G_{x}$ is the derivative of $f$ at $x$, and $H_{x}$ is the hessian, the bilinear form correponding to the second derivative. Now if $G_{x} = 0$ and the Hessian $H_{x}$ is strictly positive, the function $f$ has a local minimum at $x$. If $H_{x}$ is merely positive, such conclusions can't be drawn. If, however, $H_{\cdot}$ is positive in a neighbourhood of $x$, $f$ has again a local minimum at $x$.

	\item One way to think about positive maps is that they don't turn vectors too much. If we are working in real inner product space, the angle between two non-zero vectors $v$ and $w$ is the unique real number $\alpha \in [0, \pi]$ such that
	\[
		\cos(\alpha) = \frac{\langle v, w\rangle}{\|v\| \|w\|}.
	\]
	The angle is less than $\frac{\pi}{2}$ if and only if $\cos(\alpha) > 0$, or equivalently if $\langle v, w \rangle > 0$. This means that a Hermitian map is positive if and only if the angle between $v$ and $A v$ is less than $\frac{\pi}{2}$: positive map doesn't turn a vector too much. This intuition can be somewhat pushed to complex spaces, but then the concept of an angle is harder to grasp.

	FIGURE: this intuition visualized in $\R^{2}$ or $\R^{3}$
\end{itemize}

\section{Congruence and Sylvesters criterion}

\subsection{$*$-conjugation}

There is one very important way to produce positive maps from others, called congruence. Given any two positive maps $A$ and $B$, their composition need not be positive, but the map $BAB$ is. First of all, it is self-adjoint, as $(BAB)^{*} = B^{*} A^{*} B^{*} = BAB$. Also $Q_{BAB}(v) = \langle BAB v, v \rangle = \langle A (B v), (B v) \rangle \geq 0$ for any $v \in V$. We didn't really need the assumption on the positivity of $B$, but self-adjointness was not that important either. Namely for arbitrary linear $B$ we could consider the product $B^{*}AB$ instead: this is positive whenever $A$ is. If $C = B^{*}AB$ for some $B \in \L(V)$, we say that $C$ is $*$-conjugate of $A$.

We also see that $Q_{B^{*}AB} = Q_{A} \circ B$: conjugation is a change of basis in the quadratic form. Similar statement is evidently true for the respective sesquilinear form. This is the main motivation for the definition of the $*$-conjugation. We have already seen that the quadratic form of a map is a good way to characterize many of its good properties, so to some extent to undestand maps, we just to need to understand structure of their quadratic forms. By change of basis of the quadratic form we have a good control of what happens. We might however lose some information: if $B = 0$, for instance, the quadratic form after $*$-conjugation by $B$ doesn't tell much about $A$. But if $B$ is invertible, or equivalently if $C$ and $B$ are $*$-conjugates of each other, we shouldn't lose any information. If this is the case, we say that $A$ and $C$ are congruent. It is easily verified that congruence is a equivalence relation.

The construction of $*$-conjugation also sense for general linear map $A$, i.e. we could just as well $*$-conjugate non-positive, or even non-self-adjoint maps. The result then need not be positive or self-adjoint, and in general, $*$-conjugation loses its usefulness. Also, even if $A$ is normal, $*$-conjugate of $A$ need not be normal. TODO

The previous construction can be also performed between two spaces $V$ and $W$: given any map $B \in \L(V, W)$ and $A \in \H_{+}(W)/\H(W)/\L(W)$, we note that $B^{*}AB \in \H_{+}(V)/\H(W)/\L(W)$. For self-adjoint maps we can say a lot more: while congruence doesn't in general preserve eigenvalues, it preserves their signs.

\begin{lause}[Sylvester's Law of Inertia]
	$A, B \in \H(V)$ are congruent, if and only if $A$ and $B$ have equally many positive, negative and zero eigenvalues, counted with multiplicity.
\end{lause}
\begin{proof}
	Let's start with the ``if" part. Let's denote the eigenvalues of $A$ and $B$ by $\lambda_{1} \leq \lambda _{2} \leq \ldots \leq \lambda_{n}$ and $\lambda_{1}' \leq \lambda _{2}' \leq \ldots \leq \lambda_{n}'$, respectively, and the corresponding eigenvectors with $v_{1}, v_{2}, \ldots, v_{n}$ and $v_{1}', v_{2}', \ldots, v_{n}'$. By assumption $\lambda_{i}$ and $\lambda_{i}'$ have the same sign (or are both zero) for any $1 \leq i \leq n$, so we may find non-zero real numbers $t_{1}, t_{2}, \ldots, t_{n}$ such that $\lambda_{i} = \lambda_{i}' t_{i}^{2}$. Now consider a linear map $C$ with $C v_{i} = t_{i} v_{i}'$. $C$ is clearly a surjection and hence a bijection. Also if $v = \sum_{i = 1}^{n} x_{i} v_{i}$ $(Q_{B} \circ C)(v) = Q_{B}(\sum_{i = 1}^{n} x_{i} t_{i} v_{i}') = \sum_{i = 1}^{n} |x_{i}|^{2} t_{i}^2 \lambda_{i}' = \sum_{i = 1}^{n} |x_{i}|^{2} \lambda_{i} = Q_{A}(v)$ so $Q_{C^{*}BC} = Q_{B} \circ C = Q_{A}$. It follows that $C^{*}BC = A$ and hence $A$ and $B$ are congruent.

	The ``only if" - part is a bit trickier. The idea is to find a good description for the number of positive non-negative eigenvalues. We noticed before that we can write quadratic forms in the form $Q_{A}(v) = \sum_{i = 1}^{n} \lambda_{i} |x_{i}|^{2}$ if $v = \sum_{i = 1}^{n} x_{i}v_{i}$, and $v_{i}$ are the eigenvectos of $A$ with $\lambda_{i}'s$ as the corresponding eigenvectors. In particular if say first $k$ eigenvalues are negative, $Q_{A}$ will be negative on $\vspan\{v_{i} | 1 \leq i \leq k\}$, a $k$-dimensional subspace, minus zero. Similarly, now $n - k$ of the eigenvalues are non-negative, so the quadratic form is non-negative on a subspace of dimension of at least $n - k$. But the dimensions can't be any bigger: if $Q_{A}$ were for instance negative on some $k + 1$ dimesional subspace, this subspace would necessarily intersect a subspace where $Q_{A}$ is non-negative, which is non-sense.

	Congruence preserves the previous notion: if $Q_{B}$ is negative on a subspace of dimension $k$, so is $Q_{B} \circ C$ for any invertible $C$; namely in the inverse image. Same reasoning holds for the the subspace on which $Q_{B}$ is non-negative, so again, $Q_{B} \circ C$ has to have similar structure. We are done.
\end{proof}

If $n_{0}, n_{-}$ and $n_{+}$ denote the number of zero, negative and positive eigenvalues of $A$, \textit{inertia} of $A$ is the triplet $\{n_{0}, n_{-}, n_{+} \}$. The previous theorem can be hence restated, that inertia is invariant under congruence.

It gets better: if two normal maps are congruent, we can vastly generalize the previous result: for normal maps, congruence preserves the number of eigenvalues on any open ray.

For any $A \in \normal(V)$ define the mapping $\theta[A] : S^{1} \to \N$ by setting $\theta[A](z)$ be the number of eigenvalues (counted with multiplicity) on ray $\{rz | r > 0\}$. This map is called the \textit{angularity} of $A$.

TODO: figure of change of eigenvalues on congruence.

\begin{lause}[Generalized Sylvester's Law of Inertia]
	$A, B \in \normal(V)$ are congruent, if and only if $\theta[A] = \theta[B]$.
\end{lause}
\begin{proof}
	The ``if"-part can be proven almost identically to proof of the self-adjoint case. For the ``only if" -part, note that if $A = C^{*}BC$, the also $\Re(A) = C^{*}\Re(B)C$. Now both sides are self-adjoint, and eigenvalues of $\Re(A)$ and $\Re(B)$ are just the real parts of the eigenvalues of $A$ and $B$. Applying the previous version Law of Inertia for these maps, we see that $A$ and $B$ have equally many eigenvalues on closed right half-plane. 

	Carrying out the previous reasoning for the maps $e^{i t}A$ and $e^{i t} B$ for real $t$ we see that these maps have equally many eigenvalues on any closed half plane containing origin. If one considers function sending $t$ to the number of eigenvalues of $e^{i t} A$ on closed right half-plane, we see that this function has jump discontinuitys, and the value, and the left and the right limits at these point reveals number of eigenvalues at any open ray.

	TODO: figure of functions $t \mapsto$ number of eigenvalues of $e^{it}A$ on closed right half-plane.
\end{proof}

Sylvester's Law of inertia gives another proof of the fact that strictly positive maps are exactly the maps congruent to the identity, and positive maps are the maps congruent to some projection. More precisely, the positive maps are partitioned to $n + 1$ congruene classes depending on their rank, $k$:th congruence class containing the projections to $k$-dimensional subspaces. $0$:th class contains only the zero map, the only rank $0$ (positive) map, and the $n$:th class is the class of strictly positive maps.

If one $*$-conjugates with non-invertible, the angularity may change, but in quite obvious way only: some eigenvalues may move to $0$. In particular, we have the following supplement even a bit more general version of the law.

\begin{lause}[General Sylvester's Law of Inertia]
	For $A, B \in \normal(V)$ and $A$ is $*$-conjugate of $B$, if and only if $\theta[A] \leq \theta[B]$.
\end{lause}
\begin{proof}
	TODO
\end{proof}

This extension draws a picture about the relation of previously mentioned congcruence classes. We can move to the congruence classes of lower indeces by $*$-conjugation, but cannot move up the ladder: the complexity of quadratic forms cannot increase.

The important property of $*$-conjugates we have used repeatedly is that it commutes with taking adjoint (and hence with real and imaginary parts). This is not of course immediate from the definition but is also clear since taking adjoint corresponds to taking (complex) conjugate of quadratic form, and complex conjugation commutes with pretty much anything anyway.

\subsection{Unitary congruence}

There is a very important class of congruence called unitary congruence: congruence by unitary map. Since adjoint of an unitary map is just its inverse, $*$-conjugation by unitary map, or unitary conjugation is also conjugation in the usual sense. Unitary congruence hence also preserves eigenvalues.

TODO

\section{Fake products}

\section{Absolute value and Polar decompositions}

\section{Matrices and computation}

\section{Positivity without inner-product}

TODO:
\begin{itemize}
	\item adjoints of vectors
	\item (canonical, lwdin) orthogonalization, polar decomposition and orthogonal Procrustes problem
	\item projection matrices
	\item Hilbert-Schmidt norm ($\to$ matrix functions?) and inner product
	\item Real vs. complex
\end{itemize}












