\chapter{Positive maps}

\section{Motivation}

\subsection{The right definition}

\begin{maar}
	Let $(V, \langle \cdot, \cdot \rangle)$ be an inner product space and $A \in \L(V)$. We say that $A$ is \textit{positive map}, or simply \textit{positive}, and write $A \geq 0$, if for any $v \in V$ we have
	\begin{align*}
		\langle A v, v \rangle \geq 0.
	\end{align*}
\end{maar}

Why is this the right definition for positivity? Do we really need an inner product to define positivity?

While these are both excellent questions (and one should definitely think about them), there is no way to satisfactorily answer them in the scope of this thesis. Instead, I just try to explain why the definition is pretty damn good.

But first things first: we should check that this notion gives us a proper cone.

\begin{lause}\label{basic_positive}
	The set
	\begin{align*}
		\{A \in \L(V) | \text{ $A$ is positive} \}
	\end{align*}
	is a proper cone (of $\L(V)$).
\end{lause}
\begin{proof}
	We verify the five conditions.
	\begin{enumerate}[(i)]
		\item Take any $A \geq 0$. Then for every $\alpha > 0$ and $v \in V$ we have $\langle \alpha A v, v \rangle = \alpha \langle A v, v \rangle \geq 0$.
		\item Take any two $A, B \geq 0$. Then for any $v \in V$ we have $\langle \alpha (A + B) v, v \rangle = \langle A v, v \rangle + \langle B v, v \rangle \geq 0$.
		\item For any $v \subset V$ we have $\langle 0 v, v\rangle = 0 \geq 0$.
		\item If $A, -A \geq 0$, then $\langle A v, v\rangle = 0$ for every $v \in V$. We should show that $A = 0$. We'll get back to this in a minute.
		\item We should check that if $(A_{i})_{i = 1}^{\infty}$ are positive and $\lim_{i \to \infty} A_{i} = A$, also $A$ is positive. But as the inner product is continuous, for every $v \subset V$ we have $\langle (\lim_{i \to \infty} A_{i}) v, v \rangle = \lim_{i \to \infty}\langle A_{i} v, v \rangle \geq 0$.
	\end{enumerate}
\end{proof}

We denote the cone of positive maps by $\H_{+}(V)$.

Before completing the proof, one should stop and appreciate how elegent and general the construction really is. Indeed, it carries directly to a much more general setting:

\begin{lause}\label{positive_machine}
	Let $V$ be a topological vector space (over $\R$ or $\C$) and $C^{*}$ a subset of its continuous dual. Assume that
	\begin{align*}
		\{v \in V | w^{*}(v) = 0 \text{ for every $w^{*} \in C^{*}$} \} = \{0\}.
	\end{align*}
	Then
	\begin{align*}
		\{v \in V | w^{*}(v) \geq 0 \text{ for every $w^{*} \in C^{*}$}\}
	\end{align*}
	is a proper convex cone of $V$.
\end{lause}

In our case the subset of the linear functional are the mappings of the form $A \mapsto \langle A v, v \rangle$: they are called \textit{quadratic functionals}. For fixed $A \in \L(V)$ the map $v \mapsto \langle A v, v \rangle$ is the \textit{quadratic form} of $A$. Now we are left to check the following.

\begin{lem}[Injectivity of compression]\label{inj_compr}
	If $A \in \L(V)$ and $\langle A v, v \rangle = 0$ for any $v \subset V$, then $A = 0$.
\end{lem} 
\begin{proof}
	The idea is that we can recover the inner product from norm. Indeed, if $v, w \in V$, then $\|v + w\|^2 = \|v\|^2 + \|w\|^2 + 2 \Re(\langle v, w \rangle)$, so knowing the norm, we at least know the real part of the inner product. Doing the same trick with $\|v + i w\|^2$ we can figure out the imaginary part.

	How does this help us? By a similar argument $\langle A(v + w), v + w \rangle = \langle A v, v \rangle + \langle A w, w \rangle + \langle A v, w\rangle + \langle A w, v \rangle$, so given that the quadratic form is always zero, we have $\langle A v, w \rangle + \langle A w, v \rangle = 0$ for any $v, w \in V$. Expanding $\langle A (v + i w), v + i w \rangle$ we see that $-i \langle A v, w \rangle + i \langle A v, w \rangle = 0$, which together with the previous observation implies that $\langle A v , w \rangle = 0$ for any $v, w \in V$. Now setting $w = A v$ this implies that $\|A v\|^{2} = 0$ for every $v \in V$ so $A = 0$.
\end{proof}

As one would hope, map $v \to \alpha v$, i.e. $\alpha I$ is positive, if and only if $\alpha \geq 0$. In particular in one-dimensional spaces the notion works as expected. Fortunately there are other examples, also. Indeed, any orthogonal projection is positive.

\begin{prop}
	If $A \in \L(V)$ is a orthogonal projection, then $A \geq 0$.
\end{prop}
\begin{proof}
	As any orthogonal projetion is sum of one-dimensional orthogonal projections, we can assume that the $A$ is one-dimensional in the first place. It follows that $A = \langle \cdot, v \rangle v/\|v\|^2$ for some $v \in V \setminus \{0\}$. Now for every $w \in V$ we have
	\begin{align*}
		\langle A w, w \rangle = \langle \langle w, v \rangle v, w \rangle/\|v, v\|^{2} = |\langle w, v \rangle|^{2}/\|v\|^{2} \geq 0,
	\end{align*}
	so $A$ is positive.
\end{proof}

We denote the one-dimensional orthogonal projection to the span of $v \in V \setminus \{0\}$, i.e. the map $ \langle \cdot, v \rangle v/\|v\|^2$ by $P_{v}$.

Taking positive linear combinations of orhogonal projections leads to large number of examples of positive maps.

\subsection{Real maps and adjoint}

Dual cone thinking lets us also lift other important notions.

\begin{maar}
	We say that a map $A \in \L(V)$ is \textit{real}, if
	\begin{align*}
		\langle A v, v \rangle \in \R
	\end{align*}
	for any $v \in V$.
\end{maar}

\begin{maar}
	We say that a map $A \in \L(V)$ is \textit{imaginary}, if
	\begin{align*}
		\langle A v, v \rangle \in i \R
	\end{align*}
	for any $v \in V$.
\end{maar}

The previous two families of maps are usually called Hermitian and Skew-Hermitian and as with positive maps, many of their properties are lifted form usual complex numbers. reals maps will have a special role in our discussion. They form a vector space over $\R$, which is denoted by $\H(V)$. Of course, every imaginary map is just $i$ times real map, and we won't preserve any special notation for such maps.

Interestingly enough, we can also lift the concept of complex conjugate.

\begin{lause}
	For any $A \in \L(V)$ there exists unique map $A^{*} \in \L(V)$, called the \textit{adjoint} of $A$, for which for any $v \in V$ we have
	\begin{align*}
	\langle A^{*} v, v\rangle = \overline{\langle A v, v \rangle}
	\end{align*}
\end{lause}
\begin{proof}
	The uniqueness of adjoint is immediate from the injectivity of compression. The map $(\cdot)^{*} : \L(V) \to \L(V)$ should evidently be conjugate linear, so for existence it suffices to find adjoint for suitable basis elements of $\L(V)$: the maps of the form $A = (x \mapsto \langle x, v \rangle w)$ for $v, w \in V$ will do.

	The quadratic form for such map is given by
	\[
		\langle A x, x \rangle = \langle x, v \rangle \langle w, x \rangle.
	\]
	But if we define $A^{*} = (x \mapsto \langle x, w \rangle v)$, we definitely have
	\[
		\langle A^{*} x, x \rangle = \langle x, w \rangle \langle v, x \rangle = \overline{\langle w, x \rangle \langle x, w \rangle} = \overline{\langle A v, v \rangle}.
	\]
\end{proof}

In more common terms: a adjoint of linear map $A \in \L(V)$ is the unique map $A^{*}$ such that

\begin{align*}
	\langle A v, w \rangle = \langle v, A^{*} w \rangle
\end{align*}
for any $v, w \in V$.

As real maps are their own adjoints, they are often called appropriately \textit{self-adjoint}.

The previous observation makes many of the basic properties of adjoint, which we collect below, evident.

\begin{lause}\label{basic_adjoint}
	For any linear maps $A$ and $B$, with appropriate domains and codomains, and $\lambda \in \C$ we have
	\begin{enumerate}[i)]
		\item Matrix of $A^{*}$ with respect to any orthonormal basis is conjugate transpose of matrix of $A$, i.e. $A^{*}_{i, j} = \overline{A_{j, i}}$.
		\item $(A^{*})^{*} = A$
		\item $(A + B)^{*} = A^{*} + B^{*}$
		\item $(\lambda I)^{*} = \overline{\lambda} I$
		\item $(AB)^{*} = B^{*}A^{*}$.
	\end{enumerate}
\end{lause}

\subsection{More convincing}

Positive maps have many other desirable properties. First of all, eigenvalues of a positive map are non-negative. This fact is a corollary of a more general property.

\begin{maar}
	Let $W \subset V$ be a subspace and $A \in \L(V)$. Then the \textit{compression} of $A$ to $W$, denoted by $\arestr{A}{W}$ is the linear map
	\begin{align*}
		P_{W} \circ A \circ \incl{V}{W} : W \to W
	\end{align*}
	where $\incl{V}{W}$ is the inclusion from $W$ to $V$ and $P_{W}$ is an orthogonal projection to $W$.
\end{maar}

\begin{lem}
	Let $W \subset V$ and $A \geq 0$. Then also $\arestr{A}{W} \geq 0$. In particular all the eigenvalues of $A$ are non-negative.
\end{lem}
\begin{proof}
	Note that quadratic form give essentially the one-dimensional compressions. Indeed, if $W = (v)$, then
	\begin{align*}
		\arestr{A}{W} x = \frac{\langle A x, v \rangle}{\langle v, v \rangle} v = \frac{\langle A v, v \rangle}{\langle v, v \rangle} x
	\end{align*}
	for any $x \in (v)$. This means that a map is positive, if and only if its compressions to one-dimensional subspaces are.

	Now the trick is that nested compressions work nicely: if $W' \subset W \subset V$ and $A \in \L(V)$, then $\arestr{(\arestr{A}{W})}{W'} = \arestr{A}{W'}$. Consequently, if every one-dimensional compression $A$ is positive, same is true for all its compressions.

	Now compressing to eigenspace we see that if $A$ is positive, all it's eigenvalues are non-negative.
\end{proof}

In addition, (categorical) sum of two positive map is positive.
\begin{lem}
	Let $A_{1} \in \L(V_{1})$ and $A_{2} \in \L(V_{2})$. Then $A_{1} \oplus A_{2} \in \H_{+}(V_{1} \oplus V_{2})$, if and only if $A_{1} \in \H_{+}(V_{1})$ and $A_{2} \in \H_{+}(V_{2})$.
\end{lem}
\begin{proof}
	Recall that one defines $\langle(v_{1}, v_{2}), (w_{1}, w_{2}) \rangle_{V_{1} \oplus V_{2}} = \langle v_{1}, w_{1} \rangle_{V_{1}} + \langle w_{2}, w_{2} \rangle_{V_{2}}$. Now clearly
	\begin{align*}
	\langle (A_{1} \oplus A_{2})(v_{1}, v_{2}), (v_{1}, v_{2}) \rangle_{V_{1} \oplus V_{2}} =  \langle A_{1} v_{1}, v_{1} \rangle_{V_{1}} + \langle A_{2} v_{2}, v_{2} \rangle_{V_{2}} \geq 0
	\end{align*}
	for every $(v_{1}, v_{2}) \in V_{1} \oplus V_{2}$, if and only if both $\langle A_{1} v_{1}, v_{1} \rangle_{V_{1}} \geq 0$ for every $v_{1} \in V_{1}$ and $\langle A_{2} v_{2}, v_{2} \rangle_{V_{2}} \geq 0$ for every $v_{2} \in V_{2}$.
\end{proof}

\section{Spectral theorem}

The most important result in the theory of positive and real maps is the Spectral theorem.

\begin{lause}\label{cheapSpectral}
	Let $n = \dim(V)$. Then $A \in \L(V)$ is real if and only there exists real numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$ and for pairwise orthogonal vectors $v_{1}, v_{2}, \ldots, v_{n} \in V$ such that
	\begin{align}\label{spectralrepr}
		A = \sum_{i = 1}^{n} \lambda_{i} P_{v_{i}}.
	\end{align}
\end{lause}
\begin{proof}
	We first prove the theorem for the positive maps.

	We already proved one direction: every map of the previous form is positive.

	The other direction is tricky. The idea is to somehow find the vectors $v_{i}$. The problem is that such representation is by no means unique. If $A$ is any projection for instance, we could let $v_{i}$'s by any orthonormal basis of the corresponding subspace and $\lambda_{i}$'s all equal to one. There's no vector one has to choose.

	But we can think in reverse: there could be many vectors we cannot choose, depending on the map $A$. If $A$ is any non-identity projection to subspace $W$, say, we can only choose $v_{i}$'s in $W$ itself. Indeed, if $x \in W^{\perp}$, we have $A x = 0$, and hence $\langle A x, x \rangle = 0$. By comparing the quadratic form it follows $\langle P_{v_{i}} x, x \rangle = |\langle v_{i}, x \rangle|^{2}$ for any $1 \leq i \leq m$. But this means that $v_{i} \perp W^{\perp}$ and hence $v_{i} \in W$.

	More generally, if it so happens that for some $v \in V$ we have $\langle A v, v \rangle = 0$, we must have $v_{i} \perp v$ for any $1 \leq i \leq m$. But this means that were there such representation, we should have the following.

	\begin{lem}\label{spectralZeroLemma}
		If $A \in \H_{+}(V)$ and $\langle A v, v \rangle = 0$ for some $v \in V$, then $A v = 0$ and $A w \perp v$ for any $w \in v$.
	\end{lem}

	Before proving the Lemma, we complete the proof given the Lemma.

	Proof is by induction on $n$, the dimension of the space. If $n = 0$, the claim is evident. For induction step assume first that there exists $v \in V$ such that $\langle A v, v \rangle = 0$. Then by the Lemma for any $w \in v^{\perp}$ we have $A w \in v^{\perp} =: W$. But this means that $A = \incl{V}{W} \circ \arestr{A}{W} \circ P_{W} = A$. Now $\arestr{A}{W}$ is also positive, and $\dim(W) < n$. By induction assumption we have numbers $\lambda_{i}$ and vectors $v_{i} \in V$ for the map $\arestr{A}{W}$, but such representation for $\arestr{A}{W}$ immediately gives representation for $A$ also.

	We just have to get rid of the extra assumption on the existence of such $v$. But for this, note that if $\lambda = \inf_{|v| = 1} \langle A v, v \rangle$, consider $B = A - \lambda I$. Now $\inf_{|v| = 1} \langle B v, v \rangle = 0$, and $B$ is hence positive. Also, by compactness, the infimum is attained at some point $v$, so $B$ satisfies our assumptions. Now cook up a representation for $B$ and add orthonormal basis of $V$ with $\lambda_{i}$'s equal to $\lambda$: this is required representation for $A$.

	It remains to prove the general case of real map. But there's a rather simple trick: for every real map $A$ the map $A + \|A\| I$ is positive. Indeed, by the Cauchy-Schwarz -inequality one has
	\begin{align*}
		\left|\langle A v, v \rangle\right| \geq -\|A v\| \|v\| \geq - \|A\| \|v\|^{2}.
	\end{align*}
	Now if we manage to the representation for $A + \|A\| I$, we can certainly cook it for $A$ simply subtracting $\|A\|$ from the $\lambda_{i}$'s.
\end{proof}

\begin{proof}[Proof of lemma \ref{spectralZeroLemma}]
	Take any $w \in V$. Now by assumption for any $c \in \C$ we have
	\[
		\langle A (c v + w), c v + w \rangle = |c|^{2} \langle A v, v \rangle + c \langle A v, w \rangle + \overline{c} \langle A w, v \rangle + \langle A w, w \rangle \geq 0
	\]
	But this easily implies that $\langle A v, w \rangle = 0 = \langle A w, v \rangle$ for any $w \in V$. The first equality implies that $A v = 0$ and the second that $A w \perp v$ for any $w \in V$.
\end{proof}

Again, as to why such result should be true is a story for another time. 

In the representation \ref{spectralrepr} the numbers $\lambda_{i}$ are evidently the eigenvalues of $A$ and vectors $v_{i}$ the corresponding eigenvectors; this is why we call it the \textit{Spectral representation}. As remarked, the representation is of course not unique, but there is a way to make the Spectral representation unique, however. For this we have to change $v_{i}$ to corresponding eigenspaces.

\begin{lause}[Spectral theorem]
	Let $A \in \H(V)$. Then there exists unique non-negative integer $m$, distinct real numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}$ and non-trivial orthogonal subspaces of $V$, $E_{\lambda_{1}}, E_{\lambda_{2}}, \ldots E_{\lambda_{m}}$ with $E_{\lambda_{1}} + E_{\lambda_{2}} + \ldots + E_{\lambda_{m}} = V$, such that
	\begin{align*}\label{spectralrepr2}
		A = \sum_{i = 1}^{m} \lambda_{i} P_{E_{\lambda_{i}}}.
	\end{align*}
	Moreover, this representation is unique.
\end{lause}
\begin{proof}
	Existence of such representation immediately follows from the previous form of Spectral theorem. For uniqueness, note that $\lambda_{i}$'s are necessarily the eigenvalues of $A$ and $E_{\lambda_{i}}$'s the corresponding eigenspaces.
\end{proof}

Although the latter version is definitely of theoretical importance, we will mostly stick the former as it only contains one-dimensional projections.

Spectral representation makes many of the properties of real maps obvious. For instance any power of real map is real: indeed, if $A = \sum_{1 \leq i \leq n} \lambda_{i} P_{v_{i}}$, then
\[
	A^{2} = \left(\sum_{i = 1} \lambda_{i} P_{v_{i}}\right) \left(\sum_{j = 1} \lambda_{j} P_{v_{j}}\right) = \sum_{i = 1}^{n} \sum_{j = 1}^{n}\lambda_{i} \lambda_{j} P_{v_{i}} P_{v_{j}} = \sum_{i = 1}^{n} \lambda_{i}^2 P_{v_{i}},
\]
since $P_{v} P_{w} = 0$ for $v \perp w$. By induction one can extend the previous for higher powers. In other words: eigenspaces are preserved under compositional powers, and eigenvalues are ones to get powered up. From the original definition this is not all that clear. One could even extend to polynomials. If $p(x) = c_{n} x^{n} + c_{n- 1} x^{n - 1} + \ldots c_{1} x + c_{0}$, with $c_{i} \in \R$, we should write
\begin{align}\label{polynomial_matrix_function}
	p(A) = c_{n} A^{n} + c_{n - 1} A^{n - 1} + \ldots c_{1} A + c_{0} = \sum_{1 \leq i \leq n} p(\lambda_{i}) P_{v_{i}}.
\end{align}
This implies that if $p$ is the characteristic polynomial of $A$, then $p(A) = 0$: the special case of Cayley Hamilton theorem. Moreover, the minimal polynomial of $A$ is the polynomial with the eigenvalues of $A$ as single roots.

But even better, if $p$ is polynomial with all except one, say $\lambda_{i}$, of the eigenvalues of $A$ as roots, then $p(A) = p(\lambda_{i}) P_{E_{\lambda_{i}}}$. In particular, the projections to eigenspaces of $A$ are actually polynomials of $A$.

Also, given $A \in \H(V)$, we may write any $x \in V$ in the form $v = \sum_{1 \leq i \leq n} x_{i} v_{i}$, where $(v_{i})_{i = 1}^{n}$ is a eigenbasis for $A$ and $x_{i} = \langle x, v_{i} \rangle$. Now $A x = \sum_{1 \leq i \leq n} \lambda_{i} x_{i} v_{i}$, so for instance

\begin{itemize}
\item $Q_{A}(x) = \langle A x, x \rangle = \sum_{1 \leq i \leq n} \lambda_{i} x_{i}^{2}$. Thus $Q_{A}$ is just a positive linear combination of eigenvalues, and $R(A, \cdot)$ convex combination.
\item $\|A x\|^{2} = \langle A x, A x \rangle = \sum_{1 \leq i \leq n} \lambda_{i}^{2} x_{i}^{2} \leq \left(\max_{1 \leq i \leq n} \lambda_{i}^2 \right) \sum_{1 \leq i \leq n} x_{i}^2 = \left(\max_{1 \leq i \leq n} \lambda_{i}^2 \right) \|x\|^{2}$. It follows that $\|A\| = \max_{1 \leq i \leq n} |\lambda_{i}|$.
\end{itemize}

Similarly, if $A \geq 0$, $A$ has a unique positive square root, which we denote by $A^{\frac{1}{2}}$: map such that $A^{\frac{1}{2}} A^{\frac{1}{2}} = A$. Given the spectral reprsentation $A = \sum_{1 \leq i \leq n} \lambda_{i} P_{v_{i}}$, we can simply set $A^{\frac{1}{2}} = \sum_{1 \leq i \leq n} \lambda_{i}^{\frac{1}{2}} P_{v_{i}}$. As for the uniqueness, note that if $B$ is a positive square root for $A$ and $B = \sum_{1 \leq i \leq n} \lambda_{i}' P_{v_{i}'}$, then $B^2 = \sum_{1 \leq i \leq n} \lambda_{i}'^{2} P_{v_{i}'}$. It follows that eigenvalues of $B$ are simply square roots of eigenvalues of $A$ and the corresponding eigenspaces are equal. Of course, the whole uniqueness argument floats more naturally with unique spectral representation.

Finally, one should note that the lemma \ref{spectralZeroLemma} enjoys following natural generalization.

\begin{prop}\label{posnegfact}
	If $A \geq 0$ and $\arestr{A}{W} = 0$ for some subspace $W \subset V$ then we may decompose $A = \arestr{A}{W^{\perp}} \oplus 0_{W}$.
\end{prop}
\begin{proof}
	We prove the statement by induction on the dimension of $W$. Lemma \ref{spectralZeroLemma} took care of the case $\dim(W) = 1$. When $\dim(W) > 1$ we may decompose $W = W' \oplus W''$ where $\dim(W') = 1$. Now $\arestr{A}{W'} = \arestr{(\arestr{A}{W})}{W'} = \arestr{0}{W'} = 0$, so we may decompose $A = \arestr{A}{W'^{\perp}} \oplus 0_{W'}$. But $\arestr{(\arestr{A}{W'^{\perp}})}{W''} = \arestr{A}{W''} = \arestr{(\arestr{A}{W})}{W''} = \arestr{0}{W''} = 0$, so by the induction hypothesis $\arestr{A}{W'^{\perp}} = \arestr{(\arestr{A}{W'^{\perp}})}{W''^{\perp}} \oplus 0_{W''} = \arestr{A}{W^{\perp}} \oplus 0_{W''}$. Consequently $A = \arestr{A}{W'^{\perp}} \oplus 0_{W'} = \arestr{A}{W^{\perp}} \oplus 0_{W''} \oplus 0_{W'} = \arestr{A}{W^{\perp}} \oplus 0_{W}$, as desired.
\end{proof}
It's easy to see that this property actually characterizes the set of positive and negative maps: we may find kernel of a positive or negative map by finding where the map compresses to zero.

The previous proposition has an useful corollary.

\begin{kor}\label{projection_compression}
	If $W, W' \subset V$, then $\arestr{(P_{W})}{W'} = 0$ if and only if $W \perp W'$.
\end{kor}
\begin{proof}
	Assume first that $\arestr{(P_{W})}{W'} = 0$. Then by the lemma \ref{posnegfact} we have $W = \image(P_{W}) \subset W'^{\perp}$. The other direction is evident.
\end{proof}

\subsection{Commuting real maps}

\textbf{Warning!} Composition of positive maps need not be positive!

If $A, B \in \H_{+}(V)$, then, as we noticed, $(A B)^{*} = B^{*} A^{*} = B A$, so for $A B$ to be even real, $A$ and $B$ would at least need to commute. Natural question follows: when do two positive maps commute? Since $(c_{1} I + A)$ and $(c_{2} I + B)$ commute if and only if $A$ and $B$ do, this is same as asking when do two real maps commute.

It turns out that real maps commute only if they ``trivially" commute, in the following sense. If there exists vectors $v_{1}, v_{2}, \ldots, v_{n}$ and numbers $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$ and $\lambda'_{1}, \lambda'_{2}, \ldots, \lambda'_{n}$ such that
\[
	A = \sum_{1 \leq i \leq n} \lambda_{i} P_{v_{i}} \; \text{ and } \; B = \sum_{1 \leq i \leq n} \lambda'_{i} P_{v_{i}},
\]
then $A$ and $B$ are said to be \textit{simultaneously diagonalizable}. Simultaneosly diagonalizable maps trivially commute, and it turns out that if two real maps commute, they are indeed simultenously diagonalizable.

To prove this statement, we start with a lemma, simplest non-trivial case of the statement.

\begin{lem}\label{projectionLemma}
	Let $W_{1}, W_{2} \subset V$ be two subspaces. Then $P_{W_{1}}$ and $P_{W_{2}}$ commute if and only if there exists orthogonal subspaces $U_{1}, U_{2}$ and $U_{0}$ such that
	\[
		W_{1} = U_{1} + U_{0}  \; \text{ and } \; W_{2} = U_{2} + U_{0}.
	\]
	We then have $P_{W_{1}} = P_{U_{1}} + P_{U_{0}}$ and $P_{W_{2}} = P_{U_{2}} + P_{U_{0}}$, and $U_{0} = W_{1} \cap W_{2}$.
\end{lem}
\begin{proof}
	Write $U_{0} := W_{1} \cap W_{2}$ and $W_{i} = U_{0} + U_{i}$ for some $U_{i} \perp U_{0}$ for $i \in \{1, 2\}$. Now $P_{W_{i}} = P_{U_{i}} + P_{U_{0}}$ for $i \in \{1, 2\}$ so it suffices to check that $U_{1} \perp U_{2}$. Equivalently, it suffices to prove that if $W_{1} \cap W_{2} = \{0\}$, and $P_{W_{1}}$ and $P_{W_{2}}$ commute, then $W_{1} \perp W_{2}$ or equivalently $P_{W_{1}}P_{W_{2}} = 0 = P_{W_{2}}P_{W_{1}}$. But for any $v \in V$ we have $W_{1} \ni P_{W_{1}}P_{W_{2}}v = P_{W_{2}}P_{W_{1}}v \in W_{2}$, so indeed $P_{W_{1}}P_{W_{2}} = 0 = P_{W_{2}}P_{W_{1}}$.
\end{proof}

\begin{maar}
	We say that two $W_{1}, W_{2} \subset V$ subspaces commute if the respective projections commute.
\end{maar}

\begin{lause}\label{commuting_real_maps}
	Let $\mathcal{A} = (A_{j})_{j \in J}$ by an arbitrary family of commuting real maps. Then there exists non-trivial orthogonal subspaces of $V$, $E_{1}, E_{2}, \ldots E_{m}$ with $E_{1} + E_{2} + \ldots + E_{m} = V$ and numbers $\lambda_{i, j}$ for $j \in J$ and $1 \leq i \leq n$ such that
	\[
		A_{j} = \sum_{1 \leq i \leq m} \lambda_{i, j} P_{E_{i}}
	\]
	for any $j \in J$.
\end{lause}
\begin{proof}
	The main idea is the following: like in the spectral theorem, we would like to somehow find the subspaces $E_{1}, E_{2}, \ldots E_{m}$. Also, at least for finite families, we could probably use induction, so we should get far just by proving the theorem for a family of only two maps. For two projections we have already proved the statement as lemma \ref{projectionLemma}.

	Now here's the trick: if two maps commute, so do all their polynomials. Hence if we have two commuting $A$ and $B$, we know that all the respective eigenspaces commute. Now if we could prove the statement at least for finite families of projections, we could conclude the case of two general maps. Indeed we could write any eigenprojection of $A$ or $B$ as a linear combination of sum finite family of orthogonal (orthogonal) projections, but those projections would then also span $A$ and $B$.

	More generally, if we could prove the statement for arbitrary families of projections, the same argument would yield it for any family of more general linear maps, so we can safely assume that all the maps $A_{j}$ are projections.

	Let's first deal with the finite case by induction. As mentioned, we already dealt with the case $|J| = 2$, but we can draw better conclusions. If we have two commuting projections $P_{W_{1}}$ and $P_{W_{2}}$ in $\mathcal{A}$. Now by the lemma we may write $P_{W_{1}} = P_{U_{1}} + P_{U_{0}}$ and $P_{W_{2}} = P_{U_{2}} + P_{U_{0}}$. The nice things is that any map in $\mathcal{A}$ also commutes with $P_{W_{1}} + P_{W_{2}} = P_{U_{1}} + P_{U_{2}} + 2 P_{U_{0}}$, so also with it's eigenprojections, $P_{U_{0}}$ and $P_{U_{1} + U_{2}}$. It follows that any map in $\mathcal{A}$ commutes with $U_{0}, U_{1}$ and $U_{2}$.

	We have split the subspaces $W_{1}$ and $W_{2}$ in pieces, and we could actually forget $W_{1}$ and $W_{2}$ altogether and replace them by $U_{0}, U_{1}$ and $U_{2}$: note that all the same assumption hold for this new family, and $U_{0}, U_{1}$ and $U_{2}$ span $W_{1}$ and $W_{2}$.

	Problem here is of course: it's not clear that the new family, say $\mathcal{A}'$ is any simpler than $\mathcal{A}$! It could well have more elements than $\mathcal{A}$ so we can't just do straightforward induction. What could happen also is that some of the subspaces $U_{0}, U_{1}, U_{2}$ coincide with the subspaces already present in the family, so the size of the family doesn't increase, and it could even decrease. This will indeed happen. One way to see this is to look at the sum of dimensions of all the projections of the family: if we change the family this sum cannot increase. Moreover, if we pick two subspaces $W_{1}$ and $W_{2}$ which are not orthogonal, this sum will decrease!

	The conclusion is: pick pairs projections with non-orthogonal subspaces and do the replacing procedure as explained before; this process will eventually stop since the sum of dimesions can't drop below zero. But the only reason this process could stop is that all subspaces are pairwise orthogonal in which case we are done. The proof of finite case is complete.

	There are many ways to bootstrap the previous argument for arbitrary families. For any finite subfamily we can form the set of generating projections. If add one more map, the set projections get refined: some of the subspaces get split to pieces. Now sizes of all these generating families are bounded by $n$ so we may pick one with most number of elements. Now if $A$ is any projection in $\mathcal{A}$, by maximality, adding it to the family does not refine the generating set. But this means that the generating set generates any element of $\mathcal{A}$ and we are done.

	We also see that there exists unique minimal family of generating projections TODO.

	Alternative approach to the theorem could be to look at the commutative $\R$-algebra of real maps generated by $\mathcal{A}$: generating projections will be in some sense minimal projections in this algebra.
\end{proof}

The previous theorem sends a very important message.

\begin{phil}
	Commutativity kills the exciting phenomena.
\end{phil}

One would naturally hope that product of positive maps is still positive, but as soon as we try to make such restriction, everything degenerates to $\R^{m}$, or to diagonal maps. Dealing with diagonal maps is then again just dealing with many real numbers at the same time: of course this makes sense and all, but doesn't lead to very interesting concept.

Conversely, if one wants exciting things to happen, one should make things very non-commutative.

As another corollary of theorem \ref{commuting_real_maps} we have

\begin{kor}
	If $A, B \geq 0$ and $A$ and $B$ commute, then $AB \geq 0$.
\end{kor}

Also in the general case we can say something positive:

\begin{prop}
	If $A, B \geq 0$, then $AB$ is diagonalizable and has non-negative eigenvalues. Conversely, if $C$ is diagonalizable and has non-negative eigenvalues, then it's of the form $AB$ for some positive $A$ and $B$.
\end{prop}
\begin{proof}
	TODO (Is this true? Probably not)
\end{proof}

TODO: independence of random variables.

\subsection{Symmetric product}

As normal product doesn't quite work with positivity, next attempt might be symmetrized product
\[
	S(A, B) = AB + BA,
\]
(or maybe with normalizing constant $\frac{1}{2}$ in the front), instead of the usual one. It turns out that even this doesn't fix positivity.

For one dimesional projections things go as badly as they possibly can.

\begin{prop}\label{symmetric_projection}
	If $v, w \in V \setminus \{0\}$, then
	\begin{align*}
		P_{v} P_{w} + P_{w} P_{v} \geq 0,
	\end{align*}
	if and only if $v$ and $w$ are parallel or orthogonal, i.e. if and only if $P_{v}$ and $P_{w}$ commute.
\end{prop}
\begin{proof}
	Since everything is happening in a (at most) two dimensional subspace of $V$, we may assume that $V$ is two dimensional in the first place. Note that
	\begin{align*}
		P_{v} P_{w} + P_{w} P_{v} = (P_{v} + P_{w})^2 - P_{v}^2 - P_{w}^2 = (P_{v} + P_{w})^2 - P_{v} - P_{w} = A^2 - A = A (A - I),
	\end{align*}
	where $A := P_{v} + P_{w}$. This is positive, if and only if the eigenvalues of $A$ are outside the interval $(0, 1)$. But since $\tr(A) = 2$ and $A \geq 0$, the only way this can happen is that either $A$ has double eigenvalues $1$ or $A$ has eigenvalues $0$ and $2$. To conclude the claim itself, we are left to do two reality checks:
	\begin{lem}
		If $A = P_{v} + P_{w} = I$, then $v$ and $w$ are orthogonal.
	\end{lem}
	\begin{proof}
		Note that $I_{(v)} = \arestr{A}{(v)} = \arestr{(P_{v})}{(v)} + \arestr{(P_{w})}{(v)} = I_{(v)} + \arestr{(P_{w})}{(v)}$, so $\arestr{(P_{w})}{(v)} = 0$. By lemma \ref{posnegfact} we have $(w) \perp (v)$.
	\end{proof}
	\begin{lem}
		If $A = P_{v} + P_{w} = 2 P_{u}$ for some $u \in V$, then $v, w$ and $u$ are all parallel.
	\end{lem}
	\begin{proof}
		Since $0 = \arestr{A}{(u)^{\perp}} = \arestr{(P_{v} + P_{w})}{(u)^{\perp}} = \arestr{(P_{v})}{(u)^{\perp}} + \arestr{(P_{w})}{(u)^{\perp}} \geq 0$, we have $\arestr{(P_{v})}{(u)^{\perp}} =  \arestr{(P_{w})}{(u)^{\perp}}$. Now by the lemma \ref{posnegfact} we have $(v), (w) \subset (u)$: hence the claim.
	\end{proof}
\end{proof}


Moreover, even if one adds positive buffer, things won't work in general.

\begin{prop}\label{symmetric_fail}
	Let $\alpha, \beta, \gamma \in \C$ and $n \geq 2$. Then the expression $\alpha A^{2} + \beta AB + \overline{\beta} BA + \gamma B^{2}$ is positive for any $A, B \geq 0$ if and only if $\alpha, \gamma \in [0, \infty)$ and $|\beta|^{2} \leq \alpha \gamma$.
\end{prop}
\begin{proof}
	By easy scaling arguments consideration we may reduce the considerations to the case $\alpha = \gamma = 1$. If $|\beta| \leq 1$, we may write
	\begin{align*}
		A^2 + \beta A B + \overline{\beta} B A + B^2 = (A + \beta B)^{*}(A + \beta B) + (1 - |\beta|^2) B^2 \geq 0.
	\end{align*}
	If $|\beta| > 1$, we need to find $A, B \geq 0$ such that $A^2 + \beta A B + \overline{\beta} B A + B^2 \not\geq 0$.

	TODO (is this true? probably)
\end{proof}

So in some sense, by taking non-commutative products, we really lose most of the structure.

\section{Congruence}

\subsection{$*$-conjugation}

There is one very important way to produce positive maps from others, called congruence. Given any two positive maps $A$ and $B$, their composition need not be positive, but the map $BAB$ is. First of all, it is real as $(BAB)^{*} = B^{*} A^{*} B^{*} = BAB$. Also $Q_{BAB}(v) = \langle BAB v, v \rangle = \langle A (B v), (B v) \rangle \geq 0$ for any $v \in V$. We didn't really need the assumption on the positivity of $B$, but realness was not that important either. Namely for arbitrary linear $B$ we could consider the product $B^{*}AB$ instead: this is positive whenever $A$ is. If $C = B^{*}AB$ for some $B \in \L(V)$, we say that $C$ is $*$-conjugate of $A$.

\begin{maar}
	Let $A, B \in \H$. We say that $B$ is \textit{$*$-conjugate} of $A$ if for some $C \in \L(V)$ we have $B = C^{*} A C$.
\end{maar}

We also see that $Q_{B^{*}AB} = Q_{A} \circ B$: conjugation is a change of basis in the quadratic form. This is the main motivation for the definition of the $*$-conjugation. We have already seen that the quadratic form of a map is a good way to characterize many of its good properties, so to some extent to understand maps, we just to need to understand structure of their quadratic forms. By change of basis of the quadratic form we have a good control of what happens. We might however lose some information: if $B = 0$, for instance, the quadratic form after $*$-conjugation by $B$ doesn't tell much about $A$. But if $B$ is invertible, or equivalently if $C$ and $B$ are $*$-conjugates of each other, we shouldn't lose any information.

\begin{maar}
	Let $A, B \in \H$. We say that $A$ and $B$ are \textit{congruent} if they are $*$-conjugates of each other.
\end{maar}

It is easily verified that congruence is a equivalence relation.

The construction of $*$-conjugation makes also sense for general linear map $A$, i.e. we could just as well $*$-conjugate non-positive, or even non-real maps. The result then need not be positive or real, and in general, $*$-conjugation loses its usefulness.

The previous construction can be also performed between two spaces $V$ and $W$: given any map $B \in \L(V, W)$ and $A \in \H_{+}(W)/\H(W)/\L(W)$, we note that $B^{*}AB \in \H_{+}(V)/\H(V)/\L(V)$. For real maps we can say a lot more: while congruence doesn't in general preserve eigenvalues, it preserves their signs.

\begin{lause}[Sylvester's Law of Inertia]
	$A, B \in \H(V)$ are congruent, if and only if $A$ and $B$ have equally many positive, negative and zero eigenvalues, counted with multiplicity.
\end{lause}
\begin{proof}
	Let's start with the ``if" part. Let's denote the eigenvalues of $A$ and $B$ by $\lambda_{1} \leq \lambda _{2} \leq \ldots \leq \lambda_{n}$ and $\lambda_{1}' \leq \lambda _{2}' \leq \ldots \leq \lambda_{n}'$, respectively, and the corresponding eigenvectors with $v_{1}, v_{2}, \ldots, v_{n}$ and $v_{1}', v_{2}', \ldots, v_{n}'$. By assumption $\lambda_{i}$ and $\lambda_{i}'$ have the same sign (or are both zero) for any $1 \leq i \leq n$, so we may find non-zero real numbers $t_{1}, t_{2}, \ldots, t_{n}$ such that $\lambda_{i} = \lambda_{i}' t_{i}^{2}$. Now consider a linear map $C$ with $C v_{i} = t_{i} v_{i}'$. $C$ is clearly a surjection and hence a bijection. Also if $v = \sum_{i = 1}^{n} x_{i} v_{i}$ $(Q_{B} \circ C)(v) = Q_{B}(\sum_{i = 1}^{n} x_{i} t_{i} v_{i}') = \sum_{i = 1}^{n} |x_{i}|^{2} t_{i}^2 \lambda_{i}' = \sum_{i = 1}^{n} |x_{i}|^{2} \lambda_{i} = Q_{A}(v)$ so $Q_{C^{*}BC} = Q_{B} \circ C = Q_{A}$. It follows that $C^{*}BC = A$ and hence $A$ and $B$ are congruent.

	The ``only if" - part is a bit trickier. The idea is to find a good description for the number of positive non-negative eigenvalues. We noticed before that we can write quadratic forms in the form $Q_{A}(v) = \sum_{i = 1}^{n} \lambda_{i} |x_{i}|^{2}$ if $v = \sum_{i = 1}^{n} x_{i}v_{i}$, and $v_{i}$ are the eigenvectos of $A$ with $\lambda_{i}'s$ as the corresponding eigenvectors. In particular if say first $k$ eigenvalues are negative, $Q_{A}$ will be negative on $\vspan\{v_{i} | 1 \leq i \leq k\}$, a $k$-dimensional subspace, minus zero. Similarly, now $n - k$ of the eigenvalues are non-negative, so the quadratic form is non-negative on a subspace of dimension of at least $n - k$. But the dimensions can't be any bigger: if $Q_{A}$ were for instance negative on some $k + 1$ dimesional subspace, this subspace would necessarily intersect a subspace where $Q_{A}$ is non-negative, which is non-sense.

	Congruence preserves the previous notion: if $Q_{B}$ is negative on a subspace of dimension $k$, so is $Q_{B} \circ C$ for any invertible $C$; namely in the inverse image. Same reasoning holds for the the subspace on which $Q_{B}$ is non-negative, so again, $Q_{B} \circ C$ has to have similar structure. We are done.
\end{proof}

In the proof we used the following useful linear algebra fact.

\begin{lem}
	Let $V$ be $n$-dimensional and $W_{1}, W_{2} \subset V$ subspaces such that $\dim(W_{1}) + \dim(W_{2}) > n$. Then $W_{1} \cap W_{2} \neq \{0\}$.
\end{lem}
\begin{proof}
	We find non-trivial element $v \in W_{1} \cap W_{2}$. Take bases for $W_{1}$ and $W_{2}$, say $(e_{i})_{i = 1}^{n_{1}}$ and $(f_{i})_{i = 1}^{n_{2}}$ with $n_{1} + n_{2} > n$. Since $(e_{i})_{i = 1}^{n_{1}} \cup (f_{i})_{i = 1}^{n_{2}}$ can't be linearly independent, as that would mean $\dim(V) \geq \dim(W_{1}) + \dim(W_{2}) > n$, we can find non-trivial pair of sequence $(a_{i})_{i = 1}^{n_{1}}$'s and $(b_{i})_{i = 1}^{n_{2}}$ such that $\sum_{i = 1}^{n_{1}} a_{i} e_{i} + \sum_{i = 1}^{n_{2}} b_{i} f_{i} = 0$. But $W_{1} \ni \sum_{i = 1}^{n_{1}} a_{i} e_{i} = v = -\sum_{i = 1}^{n_{2}} b_{i} f_{i} \in W_{2}$, and since sequences are non-trivial, $v$ is non-trivial element in the intersection.
\end{proof}

If $n_{0}, n_{-}$ and $n_{+}$ denote the number of zero, negative and positive eigenvalues of $A$, \textit{inertia} of $A$ is the triplet $\{n_{0}, n_{-}, n_{+} \} := \{n_{0}(A), n_{-}(A), n_{+}(A) \}$. The previous theorem can be hence restated, that inertia is invariant under congruence.

The proof also gives a useful characterization for the number of non-negative eigenvalues.

\begin{kor}\label{subspace_lemma}
	If $A \in \H(V)$, number of non-negative eigenvalues of $A$ equals largest non-negative integer $k$ such that for some subspace $W \subset V$ of dimension $k$ the quadratic form $Q_{A}$ is non-negative on $W$, or equivalently, $\arestr{A}{W} \geq 0$.
\end{kor}

Sylvester's Law of inertia gives another proof of the fact that strictly positive maps are exactly the maps congruent to the identity, and positive maps are the maps congruent to some projection. More precisely, the positive maps are partitioned to $n + 1$ congruence classes depending on their rank, $k$:th congruence class containing the projections to $k$-dimensional subspaces. $0$:th class contains only the zero map, the only rank $0$ positive map, and the $n$:th class is the class of strictly positive maps.

If one $*$-conjugates with non-invertible, the inertia may change, but in quite obvious way only: some eigenvalues may move to $0$. In particular, we have the following even a bit more general version of the law.

\begin{lause}[General Sylvester's Law of Inertia]
	For $A, B \in \normal(V)$ and $A$ is $*$-conjugate of $B$, if and only if $n_{\pm}(A) \leq n_{\pm}(B)$.
\end{lause}
\begin{proof}
	The proof is essentially the same.
\end{proof}

This extension draws a picture about the relation of previously mentioned congcruence classes. We can move to the congruence classes of lower indeces by $*$-conjugation, but cannot move up the ladder: the complexity of quadratic forms cannot increase. One could also think that $*$-congruence for linear maps corresponds to multiplication by non-negative real for real numbers.

\subsection{Block decomposition}

Congruence is a convenient to tool to investigate positivity. The idea is that with conguence we can perform sort of a Gaussian elimination. If $n = 2$ for instance, we can write any real map in the matrix form
\[
	M =
	\begin{bmatrix}
		a & b \\
		\overline{b} & c
	\end{bmatrix}
\]
for some $a, c \in \R$ and $b \in \C$. Now if $a \neq 0$, we could eliminate with
\[
	D =
	\begin{bmatrix}
		1 & -\frac{b}{a} \\
		0 & 1
	\end{bmatrix}
\]
to get
\[
	M D =
	\begin{bmatrix}
		a & b \\
		\overline{b} & c
	\end{bmatrix}
	\begin{bmatrix}
		1 & -\frac{b}{a} \\
		0 & 1
	\end{bmatrix}
	=
	\begin{bmatrix}
		a & 0 \\
		\overline{b} & \frac{a c - |b|^2}{a}.
	\end{bmatrix}
\]
The resulting map of course need not be real, but if we also eliminate from the other side by $D^{*}$, we get
\[
	D^{*} M D =
	\begin{bmatrix}
		a & 0 \\
		0 & \frac{a c - |b|^2}{a}.
	\end{bmatrix}
	=: M'
\]
Now $D$ is evidently invertible, it's determinant being $1$, so $M$ and $M'$ are congruent. Sylvester's law of inertia tell's us hence that that if $a > 0$ and $\det(M) \geq 0$, then $M \geq 0$.

We can generalize this thinking. For general $n$ if we have decomposition $V = W_{1} \oplus W_{2}$, then we can decompose any mapping $M$ as
\[
	M =
	\begin{bmatrix}
		A & B \\
		B^{*} & C
	\end{bmatrix},
\]
where $A, B$ and $C$ are the \textit{blocks} of $M$ given by $A = P_{W_{1}} \circ M \circ \incl{V}{W_{1}} = \arestr{M}{W_{1}}$, $B = P_{W_{1}} \circ M \circ \incl{V}{W_{2}}$ and $C = P_{W_{2}} \circ M \circ \incl{V}{W_{2}} = \arestr{M}{W_{2}}$. Now we can generalize the previous elimination: if $A$ happens to be invertible and we let
\[
	D =
	\begin{bmatrix}
		I & -A^{-1} B \\
		0 & I
	\end{bmatrix}
\]
then
\[
	D^{*} =
	\begin{bmatrix}
		I & 0 \\
		-B^{*} A^{-1}  & I
	\end{bmatrix}
\]
and
\begin{align}\label{schur_complement}
	D^{*} M D =
	\begin{bmatrix}
		I & 0 \\
		-B^{*} A^{-1}  & I
	\end{bmatrix}
	\begin{bmatrix}
		A & B \\
		B^{*} & C
	\end{bmatrix}
	\begin{bmatrix}
		I & -A^{-1} B \\
		0 & I
	\end{bmatrix}
	=
	\begin{bmatrix}
		A & 0 \\
		0 & C - B^{*} A^{-1} B
	\end{bmatrix}.
\end{align}
The map $(C - B^{*} A^{-1} B) : W_{2} \to W_{2}$ is called the \textit{Schur complement} of block $A$ of $M$, or maybe one should say Schur complement of $M$ with respect to $W_{1}$. We denote the Schur complement by $M/A$.

Now again if $A$ is invertible, $M \geq 0$ if and only if $A > 0$ and $M/A \geq 0$.

This observations leads to convenient characterization for strictly positivity, called Sylvester's criterion. If $W_{2}$ is $1$-dimensional, $M/A$ is just a real number and $M$ is stricly positive if and only if $A > 0$ and this real number is positive. On the other hand, by computing determinants we see that
\[
	\det(M) = \det \left(
	\begin{bmatrix}
		A & 0 \\
		0 & M/A
	\end{bmatrix}
	\right)
	=
	\det(A) \det(M/A),
\]
as $\det(D) = 1$. Hence $M$ is positive if and only if $\det(M)$ is positive and $A > 0$. Applying the same idea inductively we arrive at
\begin{lause}[Sylvester's criterion]
	$A \in \H(V)$ is stricly positive if and only for some (and then for any) sequence of subspaces $W_{1} \subset W_{2} \subset \ldots \subset W_{n - 1} \subset W_{n} = V$ with $\dim(W_{m}) = m$ we have $\det(A_{W_{m}}) > 0$ for any $1 \leq m \leq n$.
\end{lause}

TODO: Explain what happend with non-strict case.

One can solve $M$ from \ref{schur_complement} to arrive at so-called \textit{LDL-decomposition} of $M$:
\begin{align}\label{ldl_decomposition}
	M =
	\begin{bmatrix}
		A & B \\
		B^{*} & C
	\end{bmatrix}
	=
	\begin{bmatrix}
		I & 0 \\
		B^{*} A^{-1}  & I
	\end{bmatrix}
	\begin{bmatrix}
		A & 0 \\
		0 & C - B^{*} A^{-1} B
	\end{bmatrix}
	\begin{bmatrix}
		I & A^{-1} B \\
		0 & I
	\end{bmatrix}.
\end{align}

LDL-decomposition leads to many interesting identities. First of all, (given that $A$ is invertible), $M$ is invertible if and only if $C - B^{*} A^{-1} B$ is and its inverse is given by
\begin{align*}
	M^{-1} &=
	\begin{bmatrix}
		I & -A^{-1} B \\
		0 & I
	\end{bmatrix}
	\begin{bmatrix}
		A^{-1} & 0 \\
		0 & (C - B^{*} A^{-1} B)^{-1}
	\end{bmatrix}
	\begin{bmatrix}
		I & 0 \\
		-B^{*} A^{-1}  & I
	\end{bmatrix} \\
	&=
	\begin{bmatrix}
		A^{-1} + A^{-1} B (C - B^{*} A^{-1} B)^{-1} B^{*} A^{-1} & -A^{-1} B (C - B^{*} A^{-1} B)^{-1} \\
		-(C - B^{*} A^{-1} B)^{-1} B^{*} A^{-1} & (C - B^{*} A^{-1} B)^{-1}
	\end{bmatrix}.
\end{align*}

If one take Schur complement with respect to $C$ instead one arrives at
\begin{align*}
	M^{-1}
	&=
	\begin{bmatrix}
		(A - B C^{-1} B^{*})^{-1} & (A - B C^{-1} B^{*})^{-1} B C^{-1} \\
		- C^{-1} B^{*} (A - B C^{-1} B^{*})^{-1} & C^{-1} + C^{-1} B^{*} (A - B C^{-1} B^{*})^{-1} B C^{-1}
	\end{bmatrix},
\end{align*}
so by comparing blocks we see that for instance
\begin{align}\label{woodbury_identity}
	A^{-1} + A^{-1} B (C - B^{*} A^{-1} B)^{-1} B^{*} A^{-1} = (A - B C^{-1} B^{*})^{-1},
\end{align}
\textit{Woodbury matrix identity}. Why might such identity be useful? The idea is that if $\dim(W_{2}) \ll \dim(W_{1})$, the identity is way to connect inverse of $A - B C^{-1} B^{*}$, low rank update of $A$, and $A$. If $\dim(W_{2}) = 1$ for instance, by setting $C = -1$ for some $c > 0$ and $B = v$ for some $v \in V$ we get
\begin{align*}
	A^{-1} - \frac{A^{-1} v v^{*} A^{-1}}{1 + \langle A^{-1}v, v\rangle} = (A + v v^{*})^{-1}:
\end{align*}
inverse of rank 1 update can be easily calculated if one knows the inverse of the original map.

In a similar vein one obtains formulas for determinants. Starting with $\det(M) = \det(A) \det(C - B^{*} A^{-1} B)$, if we happen to know determinant of a map and need determinant of a compression, it is sufficient to find it for a schur complement. This is particularly useful when $W_{2}$ is low dimensional. If $\dim(W_{2}) = 1$ and $W_{2} = \vspan(v)$, then
\begin{align*}
	\det(M) &= \det(A) \left(C - B^{*} A B\right) \\
	&= \frac{\det(A) |v|^2}{\langle M^{-1} v, v \rangle}:
\end{align*}
Schur complement is inverse of compression $M$ to $W_{2}$. It follows that if $A$ is invertible, we have
\begin{align}\label{compression_determinant}
	\det(\arestr{A}{W}) = \det(A) \langle A^{-1} v, v \rangle.
\end{align}
By comparing determinants from two LDL-decompositions we arrive at
\begin{align}\label{matrix_determinant_lemma}
	\det(A) \det(C - B^{*} A^{-1} B) = \det(C) \det(A - B C^{-1} B^{*}),
\end{align}
\textit{matrix determinant lemma}. Again, by the choices for $B = v$ and $C = -1$ we arrive at
\begin{align*}
	\det(A) \left(1 + \langle A^{-1}v, v\rangle\right) = \det(A + v v^{*}):
\end{align*}
determinant of rank 1 update can be also easily calculated.

Of course, once one knows the statements, such identities could also be easily verified by multiplying everything out, for instance, but this is how one might stumble upon them.

\section{Loewner order}

\begin{maar}
	If $A, B \in \H(V)$, we write that $A \leq B$ ($A$ is smaller than $B$) if $B - A \geq 0$, $B - A$ is positive. If $B - A$ is strictly positive, we write $A < B$.
\end{maar}

We could of course have made such definition immediately after defining positive maps, but now we have proper tools to investigate such order. Proposition \ref{basic_positive} tells us that such order is indeed partial order on the $\R$-vector space of real maps. More explicitly, we have the following properties:


\begin{prop}
\begin{enumerate}[(i)]
		\item If $A \leq B$ then $\alpha A \leq \alpha B$ for any $\alpha \geq 0$.
		\item If $A \leq B$ and $B \leq C$ then $A \leq C$.
		\item If $A \leq B$ and $B \leq A$ then $A = B$.
		\item If $\lambda I \leq A$, then all the eigenvalues of $A$ are at least $\lambda$. Similarly if $A \leq \lambda I$, all the eigenvalues of $A$ are at most $\lambda$.
\end{enumerate}
\end{prop}

\begin{esim}\label{projection_order}
	If $W_{1}, W_{2} \subset V$ are two subspaces of $V$ we have $P_{W_{1}} \leq P_{W_{2}}$ if and only if $W_{1} \subset W_{2}$. Indeed if $W_{1} \subset W_{2}$ then $W_{2} = W_{1} + W_{3}$ for some $W_{3} \perp W_{1}$ and hence $P_{W_{2}} = P_{W_{1}} + P_{W_{3}} \geq P_{W_{1}}$. Conversely if $P_{W_{1}} \leq P_{W_{2}}$, then $0 \leq \arestr{(P_{W_{1}})}{W_{2}^{\perp}} \leq \arestr{(P_{W_{2}})}{W_{2}^{\perp}} = 0$, so $\arestr{(P_{W_{1}})}{W_{2}^{\perp}} = 0$. But now lemma \ref{projection_compression} implies that $W_{1} \subset W_{2}$.
\end{esim}

Key thing here is to note what is missing from the standard real ordering: multiplication by positive map doesn't preserve usual ordering. This is the reason many standard arguments don't work for general real maps.

For example if $0 < a \leq b$, with real numbers one could multiply the inequalities by the positive number $(a b)^{-1}$ to get $0 < b^{-1} \leq a^{-1}$. This doesn't quite work with linear maps anymore.

Congruence is way to at least partially fix this deficit: it's almost like multiplying by positive number. We have
\begin{prop}
	If $A \leq B$, then for any $C$ we have $C^{*} A C \leq C^{*} B C$.
\end{prop}

Using the previous we can mimic the previous proof to make it work.

\begin{lause}\label{inverse_decreasing}
	If $0 < A \leq B$, then $B^{-1} \leq A^{-1}$.
\end{lause}
\begin{proof}
	As mentioned, we can't really multiply by $(A B)^{-1}$, as it does not preserve the order and doesn't even need to be positive. If $A$ and $B$ commute, this would work though. We can almost multiply by $A^{-1}$: $*$-conjugate by $A^{-\frac{1}{2}}$. This preserves the order, and we get
	\[
		I \leq A^{-\frac{1}{2}} B A^{-\frac{1}{2}}.
	\]
	Now one would sort of want to multiply $B^{-1}$; so $*$-conjugate by $B^{-\frac{1}{2}}$, but $B$ is in the middle, so this doesn't quite work. But now we can follow the original strategy: since $I \leq X := A^{-\frac{1}{2}} B A^{-\frac{1}{2}}$ we have $X^{-1} \leq I$, that is
	\[
		A^{\frac{1}{2}} B^{-1} A^{\frac{1}{2}} \leq I.
	\]
	This is already almost what we wanted: simply $*$-conjugate by $A^{-\frac{1}{2}}$.
\end{proof}

There's one wee bit non-trivial part in the proof: if $I \leq X$ then $X^{-1} \leq I$. But if $I \leq X$, all the eigenvalues of $X$ are at least $1$, so all the eigenvalues of its inverse are at most $1$, so $X \leq I$.

\begin{huom}

Alternatively, we could conjugate both sides by $X^{-\frac{1}{2}}$ to arrive at the conclusion. Note that by doing this we have only used $*$-conjugation in the proof: actually we have $*$-conjugated altogether with 
\[
	A^{-\frac{1}{2}} (A^{-\frac{1}{2}}B A^{-\frac{1}{2}})^{-\frac{1}{2}} A^{-\frac{1}{2}} = (A^{\frac{1}{2}} (A^{-\frac{1}{2}}B A^{-\frac{1}{2}})^{\frac{1}{2}} A^{\frac{1}{2}})^{-1}.
\]
The map $A^{\frac{1}{2}} (A^{-\frac{1}{2}}B A^{-\frac{1}{2}})^{\frac{1}{2}} A^{\frac{1}{2}}$, which is real, is usually called the geometric mean of $A$ and $B$. It turns out that this mean, denoted by $G(A, B)$ satifies
\[
	G(A, B) = G(B, A) \;\;\; \text{ and } \;\;\; G(A, B)^{-1} = G(A^{-1}, B^{-1}),
\]
and if $A$ and $B$ commute we have $G(A, B) = (A B)^{\frac{1}{2}}$. The defining property of it we used it was that $G(A, B)$ is unique real map with
\[
	B = G(A, B) A^{-1} G(A, B).
\]

The point is: somewhat curiously we can almost do the original proof: just replace multiplication by congruence by square root, and replace square root of product by geometric mean.

\end{huom}

To further highlight the importance of congruence, we can use it to change map inequalities to usual real inequalities. For instance, one can generalize so called (two variable) arithmetic-harmonic mean inequality, which states that for any two positive real numbers $a$ and $b$ we have
\[
	\frac{a + b}{2} \geq \frac{2}{\frac{1}{a} + \frac{1}{b}}.
\]
This classic inequality, which can be seen as a restatement of the convexity of the map $x \mapsto \frac{1}{x}$, can be verified for instance by multiplying out the denominator and rewriting it as $\frac{(a - b)^{2}}{ab} \geq 0$.

To prove the matrix version, namely
\[
	\frac{A + B}{2} \geq 2 (A^{-1} + B^{-1})^{-1}
\]
for any $A, B > 0$, we can $*$-conjugate both sides by $A^{-\frac{1}{2}}$ to arrive at
\[
	\frac{I + A^{-\frac{1}{2}}B A^{-\frac{1}{2}}}{2} \geq 2 (I + A^{\frac{1}{2}}B^{-1} A^{\frac{1}{2}})^{-1}.
\]
If one writes $X = A^{-\frac{1}{2}}B A^{-\frac{1}{2}}$, this rewrites to
\[
	\frac{I + X}{2} \geq 2 (I + X^{-1})^{-1}.
\]
But now since $I$ and $X$ commute, the claim is evident form the scalar inequality. In a similar manner one could also prove that the geometric mean lies between arithmetic and harmonic.

\section{Eigenvalue inequalities}

There's great deal of things to be said about relationship between eigenvalues and Loewner order. Let's denote the eigenvalues of real map $A$ by $\lambda_{1}(A) \geq \lambda_{2} \geq \ldots \geq \lambda_{n}(A)$. One of the most basic result is the following.

\begin{prop}\label{loewner_eigenvalues}
	Assume that $A \leq B$. Then for any $1 \leq k \leq n$ we have $\lambda_{k}(A) \leq \lambda_{k}(B)$.
\end{prop}
\begin{proof}
	We first claim that $A$ has at most as many non-negative eigenvalues as $B$: if we manage to do this, we can apply the observation for the maps $A - \lambda I$ and $B - \lambda I$ and conclude that $B$ has at least $k$ eigenvalues in $[\lambda_{k}(A), \infty)$, which implies that $\lambda_{k}(A) \leq \lambda_{k}(B)$.

	To prove the claim note that if $A$ has $k$ non-negative eigenvalues, by lemma \ref{subspace_lemma} it's restriction to some $k$-dimensional subspace is positive. But then also the compression of $B$ to this subspace is positive, so also $B$ has at least $k$ non-negative eigenvalues.
\end{proof}

In general that's all one can say: if numbers $a_{1} \geq a_{2} \geq \ldots a_{n}$ and $b_{1} \geq b_{2} \geq \ldots \geq b_{n}$ satisfy $a_{k} \leq b_{k}$, then we can definitely find $A$ and $B$ with $A \leq B$ and $a_{i}$'s and $b_{i}$'s as eigenvalues: simply take $A = \sum_{i = 1}^{n} a_{i} P_{e_{i}}$ and $B = \sum_{i = 1}^{n} b_{i} P_{e_{i}}$ where $(e_{i})_{i = 1}^{n}$ is an orthonormal basis.

Eigenvalues work also desirably with compression.

\begin{prop}[Cauchy interlacing theorem]\label{compression_eigenvalues}
	If $A \in \H^{n}(V)$ and $W \subset V$ is of dimension $n - 1$, then we have
	\begin{align*}
		\lambda_{1}(A) \geq \lambda_{1}(\arestr{A}{W}) \geq \lambda_{2}(A) \geq \lambda_{2}(\arestr{A}{W}) \geq \ldots \geq \lambda_{n - 1}(A) \geq \lambda_{n - 1}(\arestr{A}{W}) \geq \lambda_{n}(A).
	\end{align*}
\end{prop}

\begin{proof}
	We use the same appoach and first prove that $A$ has at least as many non-negative eigenvalues as $\arestr{A}{W}$: again if we know this, we get inequalities of the form $\lambda_{k}(A) \geq \lambda_{k}(\arestr{A}{W})$. Then applying the idea for the $-A$, we get the reverse inequalities, and finally the complete chain.

	To prove the claim, note again that if $\arestr{A}{W}$ has $k$ non-negative eigenvalues, by lemma \ref{subspace_lemma} it's compression to some $k$-dimensional subspace is positive. But then also compression of $A$ to this same subspace is positive and hence it has $k$ non-negative eigenvalues.
\end{proof}

TODO picture of eigenvalues changing when compressed

Again one can prove that this result is strongest possible.

\begin{prop}\label{compression_eigenvalues_con}
	For any $a_{1} \geq b_{1} \geq a_{2} \geq \ldots \geq b_{n - 1} \geq a_{n}$ we may find $A \in \H^{n}(V)$ with $a_{i}$'s as spectra and $(n - 1)$-dimensional subspace $W$ of $V$ such that eigenvalues of $\arestr{A}{W}$ are the $b_{i}$'s.
\end{prop}

Before approaching the proof we note an interesting corollary.

Let us call pair $(A, B) \in \H(V)^{2}$ a \textit{projection pair} if $B - A = v v^{*}$ for some $v \in V$. Note that such $v$ is always unique up to phase. Let us say that a projection pair $(A, B)$ is strict, if whenever $B - A = v v^{*}$ then $v$ is not orthogonal to any eigenvector of $A$. 

\begin{kor}\label{projection_eigenvalues}
	Let $(A, B)$ be a projection pair. Then
	\begin{align*}
		\lambda_{1}(B) \geq \lambda_{1}(A) \geq \lambda_{2}(B) \geq \lambda_{2}(A) \geq \ldots \geq \lambda_{n}(B) \geq \lambda_{n}(A).
	\end{align*}
	$(A, B)$ is strict if and only if all the inequalities are strict. 
\end{kor}

\begin{proof}
	By proposition \ref{loewner_eigenvalues} $\lambda_{k}(A) \leq \lambda_{k}(B)$, so we just need to prove that $\lambda_{k + 1}(B) \leq \lambda_{k}(A)$. Let $W$ be orthocomplement of $\vspan\{v\}$. Then $\arestr{A}{W} = \arestr{B}{W}$ and $W$ is $(n - 1)$-dimensional. Hence by lemma \ref{compression_eigenvalues} we have $\lambda_{k + 1}(B) \leq \lambda_{k}(\arestr{B}{W}) = \lambda_{k}(\arestr{A}{W}) \leq \lambda_{k}(A)$, which is what we wanted. TODO
\end{proof}

One could now use induction to make similar but more complicated statements about inequalities when compression is to subspace of bigger codimension or when $B - A$ is or larger rank. One could also ask what happens $B - A$ multiple of projection to $k$-dimensional subspace (TODO: what happens?).

One also has a similar converse as in the compression case.

\begin{prop}\label{projection_eigenvalues_con}
	For any $b_{1} \geq a_{1} \geq b_{2} \geq a_{2} \ldots \geq b_{n} \geq a_{n}$ we may find projection pair $A, B \in \H^{n}(V)$ with $a_{i}$'s and $b_{i}$'s as spectra.
\end{prop}

We will first prove this converse. The idea is the following: the eigenvalues of roots of the characteristic polynomial, hence to control eigenvalues, we should control characteristic polyomials. It turns out that if two maps differ by map rank $1$, their characteristic polynomials are intimately related.

\begin{lem}\label{projection_characteristic_polynomial}
	Let $A, B \in \H$ be a projection pair. Then
	\begin{align*}
		\det(B - z I) = \det(A - z I) \left(1 + \langle (A - z I)^{-1}v, v\rangle\right).
	\end{align*}
\end{lem}
\begin{proof}
	This is just direct application of rank 1 version of matrix determinant lemma \ref{matrix_determinant_lemma}.
\end{proof}

\begin{proof}[Proof of propostion \ref{projection_eigenvalues_con}]
	If $a_{i} = b_{j}$ for some $1 \leq i, j \leq n$ we can forget $a_{i}$ and $b_{j}$, solve the remaining problem on smaller space to get $A'$ and $v'$ and take $A : V' \oplus \C \to V' \oplus \C$ to be $A' \oplus a_{i}$ and $v = v' \oplus 0$. We may hence assume that the numbers are distinct.

	First take $A$ with the given eigenvalues. By the previous lemma we just want to choose $v$ in such a way that
	\begin{align*}
		\frac{p_{B}(z)}{p_{A}(z)} = 1 + \langle (A - z I)^{-1}v, v\rangle= 1 + \sum_{i = 1}^{n} \frac{|\langle v, e_{i} \rangle|^2}{a_{i} - z},
	\end{align*}
	where $e_{i}$'s are the eigenvectors of $A$ and $p_{A}$ and $p_{B}$ are polynomials with $a_{i}$'s and $b_{i}$'s as roots. But this is easily achieveable if can show that the residues of $p_{B}(z)/p_{A}(z)$ are negative, which follows easily from the interlacing property.

	From the identity we can also easily deduce the other direction. If $\langle v, e_{i} \rangle \neq 0$ for any $1 \leq i \leq n$ the function
	\begin{align*}
		z \mapsto 1 + \sum_{i = 1}^{n} \frac{|\langle v, e_{i} \rangle|^2}{a_{i} - z}
	\end{align*}
	has $n$ poles of negative residue so it has a root between any two poles. Also it tends to $1$ at infinity so it has also root on $(a_{1}, \infty)$.
\end{proof}

The proof of \ref{compression_eigenvalues_con} is similar: the aim to first connect the characteristic polynomials of $A$ and its compression and then do similar observations.

\begin{lem}
	Let $A \in \H(V)$ and $W \subset V$ a subspace of codimension $1$, orthocomplement of subspace spanned by unit vector $v$. Then
	\begin{align*}
		\det(\arestr{A}{W} - z I) = \det(A - z I) \langle (A - z I)^{-1}v, v\rangle
	\end{align*}
\end{lem}
\begin{proof}
	This is direct application of \ref{compression_determinant}.
\end{proof}

\begin{proof}[Proof of proposition \ref{compression_eigenvalues_con}]
	Proof is just an easier version of the proof of \ref{projection_eigenvalues_con}
\end{proof}

These eigenvalue inequalities have interesting corollaries.

\begin{kor}\label{spectrum_stability}
	If $A, B \in \H^{n}(V)$, then $|\lambda_{i}(A) - \lambda_{i}(B)| \leq \sum_{i = 1}^{n} |\lambda_{i}(A - B)| \leq n \|A - B\|$ for any $1 \leq i \leq n$.
\end{kor}
\begin{proof}
	If $B - A = \sum_{i = 1}^{n} \lambda_{i}(B - A) P_{v_{i}}$, write $A_{j} = A + \sum_{i = 1}^{j} \lambda_{i}(B - A) P_{v_{i}}$. By using lemma \ref{projection_eigenvalues_con} we may trace how the eigenvalues of $A_{j}$ change when $j$ increases. We conclude the given bound $\ldots$ almost: this implies bound for terms $|\lambda_{i}(A) - \lambda_{\sigma(i)}(B)|$ for some permutation $\sigma$ of $\{1, 2, \ldots, n\}$. But $\max_{1 \leq i \leq n} |\lambda_{i}(A) - \lambda_{i}(B)| \leq \max_{1 \leq i \leq n}|\lambda_{i}(A) - \lambda_{\sigma(i)}(B)|$ for any permutation $\sigma$ (as can be seen by simple exchange argument, for instance). The last inequality is trivial, so we are done.
\end{proof}

TODO: change order of compression and projection eigenvalues converses.

\section{Notes and references}

\section{Ideas}

\begin{itemize}
	\item Normal maps
	\item Square root of a matrix
	\item Ellipses map to ellipses
	\item adjoints of vectors
	\item Moore-Penrose pseudoinverse
	\item (canonical, lwdin) orthogonalization, polar decomposition and orthogonal Procrustes problem
	\item projection matrices
	\item Hilbert-Schmidt norm ($\to$ matrix functions?) and inner product
	\item Hilbert spaces
	\item Real vs. complex
	\item Positive definite kernels
	\item Weakly positive matrices
	\item Hlawka inequality for determinant %http://mathoverflow.net/questions/182181/hlawka-inequality-for-determinants-of-positive-definite-matrices?rq=1
	\item Trace-characterization of positive maps.
	\item Splitting positive maps to pseudo square roots
	\item Product of maps
	\item Exponential formula for geometric mean?
	\item Maximum of matrices with powerlimit
	\item If $A, B$ are Hermitian, what eigenvalues $AB$ can have? What if the eigenvalues are known? What about $AB + BA$. What eigenvalues $A$ can have if eigenvalues of $\Re(A)$ are known.
	\item It seems to be the case that if $n = 2$, and $A$ is Hermitian with $\spec(A) = \{\lambda_{1}, \lambda_{2}\}$ ($\lambda_{1} \leq \lambda_{2}$), then there exists linear $B$ such that $\Re(B) = A$, and $\spec(B) = \{\mu_{1}, \mu_{2} \}$ if and only if $\Re(\mu_{1} + \mu_{2}) = \lambda_{1} + \lambda_{2}$ and $\lambda_{1} \leq \Re(\mu_{i}) \leq \lambda_{2}$. In general this is known as Ky-Fan theorem, according to \cite{Ando3}.
	\item Let's define $A \leq_{2} B$ if $\tr(A) = \tr(B)$ and for any $t \in \R$ we have $\tr(|A - t I|) \leq \tr(|B - t I|)$. Similarly we can define $A \leq_{k} B$. This easily (?) defines a partial order on matrices. But know we lose all the data about the eigenvectors? Is there a way to bring it back? Is there some nice interpretation.
	\item One would like to get such order with restrictions. Maybe this is related to sectional curvature.
	\item What happens if $n = 2$, $A, B \in \H$, and $\tr(B) = 0$ and $\tr(AB) \geq 0$. What can be said about the relation between $A$ and $A + B$.
	\item We have up to first order that if $\lambda_{1} \geq \lambda_{2} \geq \ldots \geq \lambda_{n}$ are the eigenvalues of $A$, with respective eigenvectors $v_{i}$, then we should have
	\[
		\sum_{i = 1}^{k} \langle \dot{A} v_{i}, v_{i}\rangle \geq 0,
	\]
	for any $1 \leq k \leq n$ with equality for $k = n$.
	\item Is the right condition something like: for any $t \in \R$ we should have $A \cdot \chi_{(t, \infty)} \leq B \cdot \chi_{(t, \infty)}$ or something like that.
	\item Does the following work? We say that $A \leq_{2} B$ if for any orthonormal basis $(e_{i})_{i = 1}^{n}$ we have
	\[
		(\langle A e_{i}, e_{i} \rangle)_{i = 1}^{n} \prec_{2} (\langle B e_{i}, e_{i} \rangle)_{i = 1}^{n}.
	\]
	Does this correspond to the case $n = 1$? This probably doesn't work: if $n = 2$, $\tr(A) = \tr(B) = 0$ and $e_{1}$ is in Kernel of $B$, then the right-hand sequence is zero sequence.
	\item Lorenz order?
	\item BMV-conjecture (theorem)
	\item Proof difficulties
	\item Proof ``sketch" (as in joke)
	\item Positive linear functions $\H \to \R$.
	\item What about positive linear functionals form $\H^{n} \to \H^{m}$?
	\item Power series for positivity of inverse function.
	\item Two notions of positivity: spectral and quadratic form. First works well with functional calculus and second with linear phenomena, but one shouldn't mix these two things.
\end{itemize}




