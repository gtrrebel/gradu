\chapter{Matrix monotone functions -- part 3}

\section{Loewner's theorem}

Aim of this chapter is to prove Theorem \ref{weak_loewner}. Before we start with the proof, we reinterpret Theorem \ref{weak_loewner} in terms of duals.

\begin{maar}
	We will denote
	\begin{align*}
		\pickclass(a, b) := (R_{+}(a, b))^{*}
	\end{align*}
	and call elements of $\pickclass(a, b)$ \textbf{weakly Pick functions} (on $(a, b)$).
\end{maar}

Let us first come back to Theorem \ref{better_loewner}. With the dual language it rewrites to (modulo analyticity) to
\begin{lause}\label{bester_loewner}
	The natural map $P_{\infty}(a, b)/\R \to \pickclass(a, b)$ is bijection.
\end{lause}
\begin{proof}
	We already observed in the end of the chapter $5$ that every function $f \in P_{\infty}(a, b)$ defines an element in $\pickclass(a, b)$. Using the arguments of the section \ref{step_1} it's easy to check that any element in $\pickclass(a, b)$ is induced by an function $\R^{(a, b)}$ up to a constant, and such function is in $P_{\infty}(a, b)$ by Theorem \ref{better_loewner}.
\end{proof}

Let us denote
\begin{align*}
	\tilde{\pickclass}(a, b) :=& \{\varphi \in \pickclass | \text{$\varphi$ extends analytically to $\Hp \cup \Hm \cup (a, b)$} \\
	&\text{in such a way that $\varphi(z) = \overline{\varphi(\overline{z})}$ for any $z \in \Hp$ and $\varphi[(a, b)] \subset \R.$}\}.
\end{align*}

We prove Theorem \ref{weak_loewner} in the following form.
\begin{lause}\label{better_weak_loewner}
	The restriction $\tilde{\pickclass}(a, b) \to P_{\infty}(a, b)$ is bijection.
\end{lause}
\begin{proof}
	\textbf{Well-definedness}: Pick any $\varphi \in \tilde{\pickclass}(a, b)$. Note that for any $q \in \C_{n - 1}[x]$ and $t \in (a, b)$ one has
	\begin{align*}
		\frac{(\varphi N(q))^{(2 n - 1)}(t)}{(2 n - 1)!} = \lim_{\atop{z_{1}, \ldots, z_{n} \in \Hp}{z_{1}, \ldots, z_{n} \to t}} [z_{1}, \overline{z_{1}}, \ldots, z_{n}, \overline{z_{n}}]_{\varphi N(q)} \geq 0.
	\end{align*}
	By Theorem \ref{main_theorem} $\restr{\varphi}{(a, b)} \in P_{\infty}(a, b)$.

	\textbf{Injectivity}: This follows immediately from the basic properties of analytic functions.

	\textbf{Surjectivity}: The main idea is to interpret the claim via duals. Recall that by Theorem \ref{bester_loewner} any $f \in P_{\infty}(a, b)$ corresponds to some $p^{*} \in \pickclass(a, b)$. The class $R(a, b) := \vspan_{\C}(R_{+}(a, b))$ can be given norm $\|r\|_{(a, b)} = \sup_{\lambda \in \R \setminus (a, b)} |r(\lambda)| (\lambda^{2} + 1)$.
	\begin{lem}\label{pick_norm_estimate_2}
		For every $p^{*} \in \pickclass(a, b)$ there exists constant $C(p^{*})$ such that
		\begin{align*}
			\left|p^{*} (r)\right| \leq C(p^{*}) \|r\|_{(a, b)}
		\end{align*}
		for any $r \in R(a, b)$.
	\end{lem}
	\begin{proof}
		Not much changes from the proof of Lemma \ref{pick_norm_estimate}. We can again assume that $r \in R_{\pm}(a, b) := \vspan_{\R}(R_{+}(a, b))$. Fix $a < c < d < b$. It's easy to see that there exists a constant $C_{c, d}$ such that
		\begin{align*}
			-\frac{C_{c, d}\|r\|_{(a, b)}}{(\lambda - c) (\lambda - d)} \leq r(\lambda) \leq \frac{C_{c, d}\|r\|_{(a, b)}}{(\lambda - c) (\lambda - d)}
		\end{align*}
		holds for $\lambda \in \R \setminus (c', d')$ for some $a < c' < c < d < d' < b$. But now just by the definition of $\pickclass(a, b)$ we have $|p^{*}(r)| \leq C_{c, d} [c, d]_{\varphi_{p^{*}}} \|r\|_{(a, b)}$.
	\end{proof}
	\begin{lem}\label{pick_dual_dense_2}
		$\pickclass(a, b)$ is dense in $\pickclass((a, b) \cup \Hp)$ with norm $\|\cdot\|_{(a, b)}$.
	\end{lem}
	\begin{proof}
		Again, not much changes from the proof of Lemma \ref{pick_dual_dense}. We take sequence $x_{0}, x_{1}, \ldots $ converging to $x_{\infty} \in (a, b)$. Note that
		\begin{align*}
			\left\|\frac{(w - x_{0}) \cdots (w - x_{n})}{(\lambda - w)(\lambda - x_{0}) \cdots (\lambda - x_{n})}\right\|_{(a, b)}
		\end{align*}
		tends to zero as $n \to \infty$ whenever $|w - x_{\infty}| < \min(x_{\infty} - a, b - x_{\infty})$. But this means that $\pickclass(a, b)$ is dense in $\pickclass((a, b) \cup \D(x_{\infty}, \min(x_{\infty} - a, b - x_{\infty})))$, which is dense in $\pickclass((a, b) \cup \Hp)$ by Lemma \ref{pick_dual_dense} (and the fact that $\|\cdot\|_{(a, b)} \leq \|\cdot\|_{R}$).
	\end{proof}
	By Lemmas \ref{pick_norm_estimate_2} and \ref{pick_dual_dense_2} we may extend $p^{*} \in \pickclass(a, b)$ to $R((a, b) \cup \Hp)^{*}$. It is easy to see that the extension satisfies the estimate of Lemma \ref{pick_norm_estimate_2} and $p^{*}(r) \geq 0$ for any $r \in R_{+}((a, b) \cup \Hp)$. The extension lies hence in $\pickclass((a, b) \cup \Hp)$ and so defines an extension $\varphi_{p^{*}}$ for $f$ on $\Hp \cup \Hm \cup (a, b)$. That such function lies in $\tilde{\pickclass}(a, b)$ follows immediately from the following estimate, which can be easily shown.
	\begin{lem}\label{pick_div_dif_estimate_2}
		For every compact $K \subset \Hp \cup \Hm \cup (a, b)$ and $p^{*} \in \pickclass((a, b) \cup \Hp)$ there exists a constant $C(K, p^{*})$ such that
		\begin{align*}
			\left|[z_{0}, z_{1}, \ldots, z_{k}]_{\varphi_{p^{*}}}\right| \leq \frac{C(K, p^{*})}{\dist(z_{0}, \R \setminus (a, b)) \dist(z_{1}, \R \setminus (a, b)) \cdots \dist(z_{k}, \R \setminus (a, b))}
		\end{align*}
		for any $k \geq 1$ and $z_{0}, z_{1}, \ldots, z_{k} \in K$. In particular $\varphi_{p^{*}}$ is analytic.
	\end{lem}
\end{proof}

Note that by the Schwarz reflection principle the class $\tilde{\pickclass}(a, b)$ coincides with the set of Pick functions that continuously extend to $(a, b)$ in such a way that the extension is real on $(a, b)$.

\section{Notes and references}

Proofs of this chapter are new but heavily inspired by the respective arguments on Pick functions.

\begin{comment}

TODO:

\begin{itemize}
	\item Why smoothness
	\item Examples
	\item Pick functions are monotone
	\item Heaviside function
	\item Trace inequalities: if $f$ is monotone/convex then $\tr f$ is monotone/convex. Proof idea: we may write $\tr f$ as a limit of finite sum of translations of Heaviside functions (monotone case) or absolute values (convex case), so its sufficient to prove the claim for these functions. For monotone case it hence suffices to prove that if $A \leq B$, $B$ has at least as many non-negative eigenvalues as $A$. But this is clear by subspace characterization of non-negative eigenvalues. For convex case, it suffices to prove that $\tr |A| + \tr |B| \geq \tr |A + B|$ for any $A, B \in \H^{n}(a, b)$. For this, note that if $(e_{i})_{i = 1}^{n}$ is eigenbasis of $A + B$, we have
	\begin{eqnarray*}
		\tr |A + B| &=& \sum_{i = 1}^{n} \langle |A + B| e_{i}, e_{i} \rangle \\
		= \sum_{i = 1}^{n} \left|\langle (A + B) e_{i}, e_{i} \rangle \right| &\leq& \sum_{i = 1}^{n} \left|\langle A e_{i}, e_{i} \rangle \right| + \sum_{i = 1}^{n} \left|\langle B e_{i}, e_{i} \rangle \right| \\
		\leq \sum_{i = 1}^{n} \langle |A| e_{i}, e_{i} \rangle + \sum_{i = 1}^{n} \langle |B| e_{i}, e_{i} \rangle &=& \tr |A| + \tr |B|
	\end{eqnarray*}
	\item What about trace inequalities for $k$-tone functions? Eigen-package seems to find a counterexample for $6$-tone functions and $n = 2$, but it's hard to see if there's enough numerical stability. At divided differences of polynomials vanish. First non-trivial question would be:
	If $A_{j} = A + j H$ for $0 \leq j \leq 3$ and $H \geq 0$. Then is it necessarily the case that
	\begin{align*}
		\tr \left(A_{3} |A_{3}| - 3 A_{2}|A_{2}| + 3 A_{1} |A_{1}| - A_{0} |A_{0}| \right) \geq 0?
	\end{align*}
	This would imply that $3$-tone functions would lift to trace $3$-tone functions. Maybe expressing this as a contour integral from $-i \infty \to i \infty$ a same tricks as in the paper. First projection case: $H$ is projection. Or: approximate by integrals of heat kernels. It should be sufficient to proof things for $k$-fold integrals or heat kernel, or by scaling just for gaussian function.
	\item How is the previous related to the $|\cdot|$ not being operator-convex: quadratic form inequality for eigenvectors is not enough.
	\item The previous also implies that
	\begin{align*}
		f(Q_{A}(v)) \leq Q_{f(A)}(v)
	\end{align*}
	for any convex $f$. Using this and Minkowski one sees that $p$-schatten norms are indeed norms.
	\item For $f, g$ generalization (Look at $h(X) = g (\tr f(X))$) we need that $f$ is convex. What else? $h$ is convex if it is convex for diagonalizable matrices and $f$ is convex and $g$ increasing. For the diagonalizable maps it is sufficient that $f$ is increasing and $g = f^{-1}$ and $\log \circ f \circ \exp$ is convex.
	\item Von Neumann trace inequality, more trace inequalities.
	\item On Generalizations of Minkowski's Inequality in the Form of a Triangle Inequality, Mulholland
	\item There should nice proof for Loewner theorem, like the blog post for Bernstein's big theorem.
\end{itemize}

\end{comment}
