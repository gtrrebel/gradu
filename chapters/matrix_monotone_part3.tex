\chapter{Matrix monotone functions -- part 3}

\section{Loewner's theorem}

Aim of this chapter is to prove Theorem \ref{weak_loewner}. Before we start with the proof, let us make brief return to section \ref{loewner_first_touch}.

\begin{maar}
	We say that $f : (a, b) \to \R$ is weakly Pick (on $(a, b)$), if for any $r \in R_{+}(a, b)$
	\begin{align*}
		\langle f, r\rangle_{L} \geq 0.
	\end{align*}
	We will denote the set of weakly Pick functions on $(a, b)$ by $\pickclass(a, b)$.
\end{maar}

In section \ref{loewner_first_touch} we observed that the classes $P_{\infty}(a, b)$ and  $\pickclass(a, b)$ coincide: any $\infty$-monotone function corresponds to a linear functional on $R(a, b) = R((a, b))$, positive on $R_{+}(a, b)$. Note that as we defined $R_{+}(a, b)$ to be the set of those $r \in R((a, b))$, which are non-negative outside $(c, d)$ for some $a < c < d < b$, $R_{+}(a, b) \neq R_{+}((a, b))$.

Let us denote
\begin{align*}
	\tilde{\pickclass}(a, b) :=& \{\varphi \in \pickclass | \text{$\varphi$ extends analytically to $(a, b) \cup \Hp \cup \Hm$} \\
	&\text{in such a way that $\varphi(z) = \overline{\varphi(\overline{z})}$ for any $z \in \Hp$ and $\varphi[(a, b)] \subset \R.$}\}.
\end{align*}

Now Theorem \ref{weak_loewner} can be rephrased as follows.
\begin{lause}\label{better_weak_loewner}
	Let $f : (a, b) \to \R$. Then $f \in P_{\infty}(a, b)$, if and only for some $f = \restr{\varphi}{(a, b)}$ for some $\varphi \in \tilde{\pickclass}(a, b)$.
\end{lause}

The proof is in several steps. ``$\Leftarrow$" is the easy direction.
\begin{lem}\label{loewner_easy}
	Let $\varphi \in \tilde{\pickclass}(a, b)$. Then $\restr{\varphi}{(a, b)} \in P_{\infty}(a, b)$.
\end{lem}
\begin{proof}
	Note that for any $q \in \C_{n - 1}[x]$ and $t \in (a, b)$ one has
	\begin{align*}
		\frac{(\varphi N(q))^{(2 n - 1)}(t)}{(2 n - 1)!} = \lim_{\atop{z_{1}, \ldots, z_{n} \in \Hp}{z_{1}, \ldots, z_{n} \to t}} [z_{1}, \overline{z_{1}}, \ldots, z_{n}, \overline{z_{n}}]_{\varphi N(q)} \geq 0.
	\end{align*}
	By Theorem \ref{main_theorem} $\restr{\varphi}{(a, b)} \in P_{\infty}(a, b)$.
\end{proof}

For ``$\Rightarrow$" we follow the ideas of proof of Theorem \ref{pick_interpolation}. We have functional on $R(a, b)$, and would like to extend it to a functional in $R((a, b) \cup \Hp \cup \Hm)$. For this we need two ingredients: ``norm" for $R(a, b)$, for which the elements of $\pickclass(a, b)$ give bounded functionals, and density of $R(a, b)$ in $R((a, b) \cup \Hp \cup \Hm)$ with this norm. Suitable norm in this setting is
\begin{align*}
	\|r\|_{(a, b)} = \sup_{\lambda \in \R \setminus (a, b)} |r(\lambda)| (\lambda^{2} + 1).
\end{align*}
\begin{lem}\label{pick_norm_estimate_2}
	For every $\varphi \in \pickclass(a, b)$ there exists constant $C(\varphi)$ such that
	\begin{align*}
		\left|\langle \varphi, r\rangle_{L}\right| \leq C(\varphi) \|r\|_{(a, b)}
	\end{align*}
	for any $r \in R(a, b)$.
\end{lem}
\begin{proof}
	Not much changes from the proof of Lemma \ref{pick_norm_estimate}. We can again assume that $r \in R_{\pm}(a, b)$. Fix $a < c < d < b$. It's easy to see that there exists a constant $C_{c, d}$ such that
	\begin{align*}
		-\frac{C_{c, d}\|r\|_{(a, b)}}{(\lambda - c) (\lambda - d)} \leq r(\lambda) \leq \frac{C_{c, d}\|r\|_{(a, b)}}{(\lambda - c) (\lambda - d)}
	\end{align*}
	holds for $\lambda \in \R \setminus (c', d')$ for some $a < c' < c < d < d' < b$. But now just by the definition of $\pickclass(a, b)$ we have $|\langle \varphi, r\rangle_{L}| \leq C_{c, d} [c, d]_{\varphi} \|r\|_{(a, b)}$.
\end{proof}

\begin{lem}\label{pick_dual_dense_2}
	$R(a, b)$ is dense in $R((a, b) \cup \Hp \cup \Hm)$ with respect to $\|\cdot\|_{(a, b)}$.
\end{lem}
\begin{proof}
	Again, not much changes from the proof of Lemma \ref{pick_dual_dense}. We take sequence $x_{0}, x_{1}, \ldots $ converging to $x_{\infty} \in (a, b)$. Note that
	\begin{align*}
		\left\|\frac{(w - x_{0}) \cdots (w - x_{n})}{(\lambda - w)(\lambda - x_{0}) \cdots (\lambda - x_{n})}\right\|_{(a, b)}
	\end{align*}
	tends to zero as $n \to \infty$ whenever $|w - x_{\infty}| < \min(x_{\infty} - a, b - x_{\infty})$. But this means that $R(a, b)$ is dense in $R((a, b) \cup \D(x_{\infty}, \min(x_{\infty} - a, b - x_{\infty})))$, which is dense in $R((a, b) \cup \Hp \cup \Hm)$ by Lemma \ref{pick_dual_dense} (and the fact that $\|\cdot\|_{(a, b)} \leq \|\cdot\|_{R}$).
\end{proof}

\begin{lem}\label{loewner_hard}
	Let $f \in P_{\infty}(a, b)$. Then there exists $\varphi \in \tilde{\pickclass}(a, b)$ with $\restr{\varphi}{(a, b)} = f$.
\end{lem}
\begin{proof}
	Using Lemmas \ref{pick_norm_estimate_2}, \ref{pick_dual_dense_2}, and the arguments of alternate proof for Lemma \ref{weak_analytic}, one sees that $\langle f, \cdot\rangle_{L}$ can be extended to $R((a, b) \cup \Hp \cup \Hm)$. With arguments of the proof of Lemma \ref{rat_multiple_approx} one sees that if $r \in R((a, b) \cup \Hp \cup \Hm)$ is non-negative on $\R \setminus (c, d)$ for some $a < c < d < b$, then $\langle f, r \rangle_{L} \geq 0$. Consequently also the extension satisfies Lemma \ref{pick_norm_estimate_2}.

	This extension defines a function $\varphi$ on $(a, b) \cup \Hp \cup \Hm$. By the positivity of the extension, this function is a Pick function on $\Hp$ and by its very definition, it agrees with $f$ on $(a, b)$. That $\varphi$ is analytic on $(a, b)$, follows from Lemma \ref{div_anal} and the following estimate, which can be easily shown with Lemma \ref{pick_norm_estimate_2}.
	\begin{lem}\label{pick_div_dif_estimate_2}
		For every compact $K \subset (a, b) \cup \Hp \cup \Hm$ and $\varphi \in \pickclass((a, b) \cup \Hp \cup \Hm)$ there exists a constant $C(K, \varphi)$ such that
		\begin{align*}
			\left|[z_{0}, z_{1}, \ldots, z_{k}]_{\varphi}\right| \leq \frac{C(K, \varphi)}{\dist(z_{0}, \R \setminus (a, b)) \dist(z_{1}, \R \setminus (a, b)) \cdots \dist(z_{k}, \R \setminus (a, b))}
		\end{align*}
		for any $k \geq 1$ and $z_{0}, z_{1}, \ldots, z_{k} \in K$.
	\end{lem}
\end{proof}

\begin{proof}[Proof of Theorem \ref{better_weak_loewner}]
	t.f.i.f. Lemmas \ref{loewner_easy} and \ref{loewner_hard}.
\end{proof}

Note that by the Schwarz reflection principle the class $\tilde{\pickclass}(a, b)$ coincides with the set of Pick functions that continuously extend to $(a, b)$ in such a way that the extension is real on $(a, b)$.

\section{Notes and references}

Proofs of this chapter are new but heavily inspired by the respective arguments on Pick functions.

\begin{comment}

TODO:

\begin{itemize}
	\item Why smoothness
	\item Examples
	\item Pick functions are monotone
	\item Heaviside function
	\item Trace inequalities: if $f$ is monotone/convex then $\tr f$ is monotone/convex. Proof idea: we may write $\tr f$ as a limit of finite sum of translations of Heaviside functions (monotone case) or absolute values (convex case), so its sufficient to prove the claim for these functions. For monotone case it hence suffices to prove that if $A \leq B$, $B$ has at least as many non-negative eigenvalues as $A$. But this is clear by subspace characterization of non-negative eigenvalues. For convex case, it suffices to prove that $\tr |A| + \tr |B| \geq \tr |A + B|$ for any $A, B \in \H^{n}(a, b)$. For this, note that if $(e_{i})_{i = 1}^{n}$ is eigenbasis of $A + B$, we have
	\begin{eqnarray*}
		\tr |A + B| &=& \sum_{i = 1}^{n} \langle |A + B| e_{i}, e_{i} \rangle \\
		= \sum_{i = 1}^{n} \left|\langle (A + B) e_{i}, e_{i} \rangle \right| &\leq& \sum_{i = 1}^{n} \left|\langle A e_{i}, e_{i} \rangle \right| + \sum_{i = 1}^{n} \left|\langle B e_{i}, e_{i} \rangle \right| \\
		\leq \sum_{i = 1}^{n} \langle |A| e_{i}, e_{i} \rangle + \sum_{i = 1}^{n} \langle |B| e_{i}, e_{i} \rangle &=& \tr |A| + \tr |B|
	\end{eqnarray*}
	\item What about trace inequalities for $k$-tone functions? Eigen-package seems to find a counterexample for $6$-tone functions and $n = 2$, but it's hard to see if there's enough numerical stability. At divided differences of polynomials vanish. First non-trivial question would be:
	If $A_{j} = A + j H$ for $0 \leq j \leq 3$ and $H \geq 0$. Then is it necessarily the case that
	\begin{align*}
		\tr \left(A_{3} |A_{3}| - 3 A_{2}|A_{2}| + 3 A_{1} |A_{1}| - A_{0} |A_{0}| \right) \geq 0?
	\end{align*}
	This would imply that $3$-tone functions would lift to trace $3$-tone functions. Maybe expressing this as a contour integral from $-i \infty \to i \infty$ a same tricks as in the paper. First projection case: $H$ is projection. Or: approximate by integrals of heat kernels. It should be sufficient to proof things for $k$-fold integrals or heat kernel, or by scaling just for gaussian function.
	\item How is the previous related to the $|\cdot|$ not being operator-convex: quadratic form inequality for eigenvectors is not enough.
	\item The previous also implies that
	\begin{align*}
		f(Q_{A}(v)) \leq Q_{f(A)}(v)
	\end{align*}
	for any convex $f$. Using this and Minkowski one sees that $p$-schatten norms are indeed norms.
	\item For $f, g$ generalization (Look at $h(X) = g (\tr f(X))$) we need that $f$ is convex. What else? $h$ is convex if it is convex for diagonalizable matrices and $f$ is convex and $g$ increasing. For the diagonalizable maps it is sufficient that $f$ is increasing and $g = f^{-1}$ and $\log \circ f \circ \exp$ is convex.
	\item Von Neumann trace inequality, more trace inequalities.
	\item On Generalizations of Minkowski's Inequality in the Form of a Triangle Inequality, Mulholland
	\item There should nice proof for Loewner theorem, like the blog post for Bernstein's big theorem.
\end{itemize}

\end{comment}
