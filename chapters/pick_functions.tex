\chapter{Pick-Nevanlinna functions}

\textit{Pick-Nevanlinna function} is an analytic function defined in upper half-plane with a non-negative imaginary part. Such functions are sometimes also called Herglotz or $\R$ functions; we will call them just \textit{Pick functions}. The class of Pick functions is denoted by $\pickclass$.

\section{Examples and basic properties}

Most obvious examples of Pick functions might be functions of the form $\alpha z + \beta$ where $\alpha, \beta \in \R$ and $\alpha \geq 0$. Of course one could also take $\beta \in \Hpc$. As non-constant analytic functions are open mappings, real constants are the only Pick functions failing to map $\Hp \to \Hp$.

Pick functions can be thought of a set of ``positive analytic functions".

\begin{lause}
	$\pickclass \subset \{\text{analytic maps on $\Hp$} \}$ is a closed convex cone.
\end{lause}
\begin{proof}
	Again, t.f.i.f \ref{positive_machine}.
\end{proof}

Also a composition of Pick functions is a Pick function.

The map $z \mapsto -\frac{1}{z}$ is evidently a Pick function. Hence are also all functions of the form
\begin{align*}
	\alpha z + \beta + \sum_{i = 1}^{N} \frac{m_{i}}{\lambda_{i}- z},
\end{align*}
where $N$ is non-negative integer, $\alpha, m_{1}, m_{2}, \ldots, m_{N} \geq 0$, $\beta \in \Hp$ and $\lambda_{1}, \ldots, \lambda_{N} \in \Hm$.

There are (luckily) more interesting examples. All the functions of the form $x^{p}$ (with natural branch) where $0 < p < 1$ are Pick functions; similarly for $\log$. Another classic example is $\tan$. Indeed, by the addition formula
\begin{eqnarray*}
	\tan(x + i y) &=& \frac{\tan(x) + \tan(i y)}{1 - \tan(x) \tan(i y)} = \frac{\tan(x) + i \tanh(y)}{1 - i \tan(x) \tanh(y)} \\
	&=& \frac{\tan(x)(1 + \tanh^2(y))}{1 + \tan^2(x) \tanh^2(y)} + i \frac{(1 + \tan^2(x))\tanh(y)}{1 + \tan^2(x) \tanh^2(y)},
\end{eqnarray*}
and $y$ and $\tanh(y)$ have the same sign.

$\pickclass$ is almost salient: if $\varphi$ is analytic and $\Im(\varphi) = 0$, then $\varphi$ is a real constant (by Cauchy-Riemann equations, for instance). And again, this suggests that one should think about Pick functions up to a real constant.

So far we have made no mention on the topology, as it's usually taken to be the topology of locally uniform convergence. This definitely works (as it makes the evaluation functionals continuous), but we can do much better. It namely turns out that we can consider the set of Pick functions as a proper cone of $\C^{\Hp}$, set of all functions, with the topology of pointwise convergence.

\begin{prop}\label{pick_convergence}
	If $(\varphi_{i})_{i = 1}^{\infty}$ is a sequence of Pick functions converging pointwise, the limit function is also a Pick function.
\end{prop}

This result is far from clear: pointwise limits of analytic functions need not analytic in general. We will not prove the result yet, but it strongly suggests that there is something more going on; Pick functions are very rigid. Note also that if Pick functions are thought of a subset of all functions, the definition of the cone doesn't really fit the general framework of theorem \ref{positive_machine}. This suggests that question one should ask is:

\begin{quest}\label{pick_predual}
	What is the ``correct" predual for $\pickclass$?
\end{quest}


\section{Rigidity}

\subsection{Boundary}

To understand the rigidity phenomena we take a brief look at a close relative to Pick functions, \textit{Schur functions}. Schur funtions are analytic maps from open unit disc to closed unit disc. Classic fact about these functions is the Schwarz lemma.

\begin{lause}[Schwarz lemma]
	Let $\psi : \D \to \D$ be analytic such that $\psi(0) = 0$. Then $|\psi(z)| \leq |z|$ for any $z \in \D$.
\end{lause}

The textbook proof is based on two observations about analytic functions.
\begin{itemize}
	\item If $\varphi$ is analytic at $a$ with $\varphi(a) = 0$, then $\varphi/(\cdot - a)$ is also analytic.
	\item If $\varphi$ is analytic on closed unit disc and $|\varphi| \leq 1$ on the boundary of the disc, then $|\varphi| \leq 1$ inside the disc.
\end{itemize}

The first observation might not be very surprising, and it holds for smooth functions also. The second, on the other hand, is a true manifestation of the nature of the analytic maps: we can bound analytic functions simply by bounding them on the boundary of the domain. More generally: one knows everything about an analytic function on a domain simply by knowing it on a boundary, by Cauchy's integral formula.

This suggests that we should be able to recognize also Pick functions looking only at their boundary values. Actually even more is true: it suffices to look at the imaginary parts.

\begin{prop}
	Let $\varphi : U \to \C$ be analytic, such that $\overline{\Hp} \subset U$, and $\varphi$ is continuous at $\infty$. Then if the imaginary part of $\varphi$ is non-negative on the real axis, $\varphi$ is Pick function.
\end{prop}
\begin{proof}
	t.f.i.f the minimum principle applied to the harmonic function $\Im(\varphi)$.
\end{proof}

\subsection{Integral representations}

Recall that imaginary part of an analytic function determines also its real part, up to a constant, so we can also recover the function itself. This can be also done explicitly.

\begin{lause}
	Let $\varphi : U \to \C$ be analytic, such that $\overline{\Hp} \subset U$, and $\varphi(z) = O(|z|^{-\varepsilon})$ for some $\varepsilon > 0$ at infinity. Then for any $z \in \Hp$ we have
	\begin{align*}
		\varphi(z) = \frac{1}{\pi}\int_{\R} \frac{\Im(\varphi)(\lambda)}{\lambda - z} d \lambda
	\end{align*}
\end{lause}
\begin{proof}
	Note that the integral defines an analytic function, imaginary part of which equals
	\begin{align*}
		\frac{\Im(z)}{\pi}\int_{\R} \frac{\Im(\varphi)(\lambda)}{(\lambda - z)(\lambda - \overline{z})} d \lambda.
	\end{align*}
	This expression however equals $\Im(\varphi(z))$ by Poisson integral formula. By letting $z \to \infty$ one sees that also the real constants match.

	Alternatively one could observe that for a closed counter clockwise oriented curves $\gamma$ on the upper half-plane, enclosing $z$, we have
	\begin{align*}
		\varphi(z) = \frac{1}{2\pi i}\int_{\gamma} \frac{\varphi(\lambda)}{\lambda - z} d \lambda.
	\end{align*}
	Now given the bound, we may deform the contour to real axis. By comparing this identity and our goal, we are left to prove that
	\begin{align*}
		\frac{1}{2\pi i}\int_{\gamma} \frac{\overline{\varphi(\lambda)}}{\lambda - z} d \lambda = \frac{1}{2\pi i} \overline{\int_{\gamma} \frac{\varphi(\lambda)}{\lambda - \overline{z}} d \lambda} = 0.
	\end{align*}
	But this is clear as $\varphi/(\cdot - \overline{z})$ is analytic in the upper half-plane.
\end{proof}

There's of course nothing really special about the decay assumption $\varphi(z) = O(|z|^{-\varepsilon})$; it's there just to make everything converge.

One can guarantee the convergence also by other means. Note that as the integrand behaves like $(\lambda - z)^{-1}$, if we subtract from it something (not depending on $z$) behaving the same way at the infinity, we ought to improve convergence, but only change the value of the function by a constant. As an example, consider the integral
\begin{align}\label{easy_pick_repr}
	\frac{1}{\pi}\int_{\R} \left(\frac{1}{\lambda - z} - \frac{[|\lambda| > 1]}{\lambda} \right) \Im(\varphi)(\lambda)d \lambda.
\end{align}
It converges to an analytic function as long as, say, $\Im(\varphi)$ is bounded. As before, its imaginary part coincides with $\varphi$'s so the functions are equal up to a real constant. Now, however, there's no reason for the real constants to match and indeed they need not.

Note that the previous idea could be used to construct Pick functions. Everything still makes sense if we replace $\Im(\varphi)$ by some other positive function, as long as the integral converges. Heck, we could replace it by any positive measure for which $\mu((\lambda^2 + 1)^{-1}) < \infty$.

(Almost) all the examples given before are actually just special cases of this construction. The rational functions $\frac{1}{\lambda - z}$, where $\lambda \in \R$ are obtained by setting $\mu = \delta_{\lambda}$. The power functions are obtained as
\begin{align*}
	z^{p} &= 1 + \frac{1}{\pi}\int_{-\infty}^{0} \left(\frac{1}{\lambda - z} - \frac{1}{\lambda - 1}\right) \Im(\lambda^{p}) d \lambda \\
	&=1 + \frac{1}{\pi}\int_{-\infty}^{0} \left(\frac{1}{\lambda - z} - \frac{1}{\lambda - 1}\right) |\lambda|^{p} \sin(\pi p) d \lambda,
\end{align*}
Logarithm is even simpler:
\begin{align*}
	\log(z) = \int_{-\infty}^{0} \left(\frac{1}{\lambda - z} - \frac{1}{\lambda - 1}\right) d \lambda.
\end{align*}
Tangent function could be obtained by putting $\delta$-measures to its poles, the points of the form $\frac{\pi}{2} + n \pi$, where $n \in \Z$.

The only exception is the function $z \mapsto \alpha z$ -- it can't be expressed as such integral. But even this failure is really more about poor point of view, as we will see in a minute. With these observations in mind it ought to be not too surprising that we have the following.

\begin{lause}\label{pick_nevanlinna_herglotz_representation_theorem}
	$\varphi \in \pickclass$, if and only
	\begin{align}\label{pick_representation}
		\varphi(z) = \alpha z + \beta + \int_{-\infty}^{\infty} \left(\frac{1}{\lambda - z} - \frac{\lambda}{\lambda^2 + 1}\right) d \mu(\lambda)
	\end{align}
	for some $\alpha \geq 0$ and $\beta \in \R$ and a Radon measure $\mu$ with $\int_{-\infty}^{\infty} (\lambda^2 + 1)^{-1} d \mu(\lambda) < \infty$.
\end{lause}

Choosing $\lambda \mapsto \frac{\lambda}{\lambda^2 + 1}$ is common choice in the literature and is convenient as
\begin{itemize}
	\item It's real, so the integrand is Pick function for any $\lambda \in \R$.
	\item We may recover the constant $\beta$ as $\Re(\varphi(i))$.
\end{itemize}

To better explain the appearance of the linear term, we can write the integral in a sligtly different form. Denoting $d \nu(\lambda) = \frac{d \mu(\lambda)}{\lambda^2 + 1}$, the formula reads
\begin{align*}
	\varphi(z) = \alpha z + \beta + \int_{-\infty}^{\infty} \frac{\lambda z + 1}{\lambda - z} d \nu(\lambda).
\end{align*}
Here $\nu$ is just a finite Borel measure. Now it kind of makes sense to extend the domain of this measure to infinity: the linear term merely corresponds to $\delta$-measure at infinity point. Of course, should one formalize this line of thought, the question on the type of extended real line had to be asked and one should address the topology. The answer is that one should glue the real line into a circle. One shouldn't worry about such issues, though, as these thoughts are here merely for intuition. The giveaway is that $\alpha$ should be really thought as a part of the measure $\mu$, even though this might not make perfect sense.

We will not prove theorem \ref{pick_nevanlinna_herglotz_representation_theorem}, but it shall work as a motivation. Instead, we prove somewhat weaker claim.
\begin{lem}\label{pick_dense}
	Under the topology of pointwise convergence one has
	\begin{align*}
		\pickclass \subset \overline{\cone}\{(\lambda - z)^{-1} | \lambda \in \R\}.
	\end{align*}
\end{lem}
\begin{proof}
	Denote the closure of the cone by $\pickclass_{e}$. As $\lim_{\lambda \to \pm \infty}|\lambda|(\lambda - z)^{-1} = \pm 1$, $\R \in \pickclass_{e}$ Now by \ref{easy_pick_repr} all the bounded Pick functions extending analytically over $\R$ are also in $\pickclass_{e}$. We finish the proof by showing that such functions are dense in the set of all Pick functions: we denote this class by $\pickclass_{b}$.

	It is straightforward to check that
	\begin{align*}
		g_{\varepsilon}(z) = \frac{z + i \varepsilon}{1 - i \varepsilon z} \in \pickclass_{b}
	\end{align*}
	for any $\varepsilon > 0$. But now for any $\varphi \in \pickclass$ and $\varepsilon > 0$ also $\varphi \circ g_{\varepsilon} \in \pickclass_{b}$. Finally $\varphi = \lim_{\varepsilon \to 0} \varphi \circ g_{\varepsilon} \in \pickclass_{b}$, as we wanted.
\end{proof}

\section{Dual thinking}
\subsection{Pick functionals}

Theorem \ref{pick_nevanlinna_herglotz_representation_theorem} sheds light to Question \ref{pick_predual}: linear functionals on $\pickclass$ should be thought of some kind of rational functions.
\begin{maar}
	Let $X \subset \Hp$. We will denote by $R(X)$ the $\C$-vector space of rational functions $r$ such that
	\begin{itemize}
		\item All poles of $r$ are simple and lie in $X \cup X^{*} = X \cup \{z \in \C |\, \overline{z} \in X\}$.
		\item $r(\lambda) = O(|\lambda|^{-2})$ at infinity.
	\end{itemize}
	We will also write $R_{+}(X)$ for the functions in $R(X)$, which are non-negative on $\R$.
\end{maar}
It is useful to note that
\begin{align}\label{pick_functional_span}
	\vspan_{\R}(R_{+}(X)) &= \{r \in R(X) | r(\R) \subset \R \} =: R_{\pm}(X) \nonumber\\
	&\text{and} \\
	\vspan_{\C}(R_{+}(X)) &= R(X). \nonumber
\end{align}
Similarly to the previous chapter we have the dual pairing $\langle f, r \rangle_{L}$ between $\C^{X}$ and $R(X)$. Indeed, we may set
\begin{align*}
	\langle f, (z - a)^{-1} \rangle_{L} =
	\begin{cases}
		f(a) & \text{ if $a \in X$} \\
		\overline{f(\overline{a})} & \text{ if $a \in X^{*}$}
	\end{cases}
\end{align*}
and extend linearly.

\begin{lause}\label{pick_functionals}
	$R_{+}(\Hp)$ is the dual cone of $\pickclass$ in the sense of $\langle \cdot, \cdot \rangle_{L}$. In other words: all continuous linear functionals $p^{*} \in \C^{\Hp}$ such that $p^{*}(\varphi) \geq 0$ for any $\varphi \in \pickclass$ are of the form $p^{*}(\varphi) = \langle \varphi, r \rangle_{L}$ for some $r \in R_{+}(\Hp)$.
\end{lause}
\begin{proof}
	$\pickclass^{*} \subset R_{+}(\Hp)$: It is well known that continuous dual of a product is direct sum of duals (see for instance TODO). In other words, the elements of $\pickclass$ are finitely supported (i.e. finite linear combinations of evaluation functionals)\footnote{This fact is not important to us and we could have restricted our attention to finitely supported functionals anyway.}. We may hence interpret $p^{*} \in \pickclass^{*}$ as an rational function, $r$, with poles on $\Hp \cup \Hm$ and $r(\infty) = 0$, acting as $\varphi \to \langle \varphi, r \rangle_{L}$. It is easy to see that $r \in R_{+}(\Hp)$ if we can verify that $r$ is non-negative. But as $(\lambda - z)^{-1} \in \pickclass$ for any $\lambda \in \R$, we have
	\begin{align*}
		p^{*}((\lambda - z)^{-1}) = \langle (\lambda - z)^{-1}, r \rangle_{L} = r(\lambda) \geq 0
	\end{align*}
	for any $\lambda \in \R$, as desired.

	$R_{+}(\Hp) \subset \pickclass^{*}$: As by the previous part $\langle (\lambda - z)^{-1}, r \rangle_{L}$ for any $\lambda \in \R$ and $r \in R_{+}(\Hp)$, we are done by Lemma \ref{pick_dense}.
\end{proof}

The rational functions $R_{+}(X)$ (identified with the respective linear maps $f \mapsto \langle f, r \rangle_{L}$) are called \textbf{Pick functionals} (on $X \subset \Hp$).

One can also interpret the Pick functionals also with divided differences. Indeed, if $r \in R_{+}(X)$, it is easy to see that
\begin{align*}
	r(z) = \frac{N(q)}{|z - z_{1}|^2 \cdots |z - z_{n}|^2}
\end{align*}
for some $n \geq 1$, $q \in \C_{n - 1}[x]$ and pairwise distinct $z_{1}, z_{2}, \ldots, z_{n} \in X$. But this means that the respective Pick functional is given by
\begin{align*}
	\varphi \mapsto [z_{1}, \overline{z_{1}}, \ldots, z_{n}, \overline{z_{n}}]_{\varphi N(q)}.
\end{align*}

\subsection{Weakly Pick functions}

While Theorem \ref{pick_functionals} implies that $R_{+}(\Hp)$ is the dual cone of $\pickclass$, it turns out that it is also the predual we were looking for.\footnote{While preduals are not unique in general, as $R_{+}(\Hp)$ is the dual cone of $\pickclass$, if $R_{+}(\Hp)$ is a predual of $\pickclass$, it is unique maximal predual.}

\begin{maar}
	We will denote
	\begin{align*}
		\pickclass(X) := (R_{+}(X))^{*}
	\end{align*}
	and call elements of $\pickclass(X)$ \textbf{weakly Pick functions} (on $X$).
\end{maar}

First off: why should the elements of $\pickclass(X)$ be called weakly Pick functions? Note that every $p^{*} \in R(X)^{*}$ can be represented as a function $\varphi_{p^{*}} : X \cup X^{*} \to \C$ by setting
\begin{align*}
	\varphi_{p^{*}}(z) = p^{*}((\lambda - z)^{-1}).
\end{align*}
Strictly speaking $(\lambda - z)^{-1} \notin R(X)$, so the above equality should be interpreted suitably and function $\varphi_{p^{*}}$ will be only defined up to a constant. Apart from that the correspondence is bijective and the resulting map
\begin{align*}
	R(X)^{*} \to \C^{X \cup X^{*}}/\C
\end{align*}
is isomorphism, as one easily checks. If one restricts the domain to $R_{+}(X)^{*}$, additional reflectional symmetry is obtained
\begin{lem}\label{weakly_pick_nonsense}
	There exists a unique injection
	\begin{align*}
		\pickclass(X) &\to \C^{X}/\R \\
		p^{*} &\mapsto \varphi_{p^{*}}
	\end{align*}
	satisfying
	\begin{align}\label{weakly_pick_function_condition}
		\varphi_{p^{*}}(z) - \overline{\varphi_{p^{*}}(w)} = p^{*}\left(\frac{1}{(\lambda - z)} -\frac{1}{(\lambda - \overline{w})}\right)
	\end{align}
	for any $z, w \in X$ and $p^{*} \in (R_{+}(X))^{*}$.
\end{lem}
\begin{proof}
	Fix $z_{0} \in X$. Condition \ref{weakly_pick_function_condition} implies that such map $\varphi_{p^{*}}$ should also satisfy
	\begin{align}\label{weakly_pick_condition_2}
		\Im(\varphi_{p^{*}}(z)) &= p^{*}\left(\frac{\Im(z)}{|\lambda - z|^{2}}\right) \nonumber\\
		&\text{ and } \\
		\Re(\varphi_{p^{*}}(z)) - \Re(\varphi_{p^{*}}(z_{0})) &= p^{*}\left(\frac{\lambda}{|\lambda - z|^2} - \frac{\lambda}{|\lambda - z_{0}|^2}\right) \nonumber
	\end{align}
	for any $z \in X$. By \ref{pick_functional_span} these equalities determine a unique function in $\C^{X}/\R$, and it's straighforward to check that such function also satisfies \ref{weakly_pick_function_condition}.

	For injectivity, note that if $p^{*} \in R(X)^{*}$ such that $\varphi_{p^{*}}$ is real constant, then $p^{*}$ kills functions of the form $(\lambda - z)^{-1} -(\lambda - \overline{w})^{-1}$. Since such functions span $R(X)$, $p^{*} = 0$. 
\end{proof}

Now the statement ``$R_{+}(\Hp)$ is the predual of $\pickclass$" makes sense.

\begin{lause}\label{pick_weakly_pick}
	The natural map $\pickclass/\R \to \pickclass(\Hp)$ is bijection.
\end{lause}

Why is this useful? The message is that Pick functions are completely determined by Pick functionals. If function on $\C$ looks like a Pick function, in the sense that it gives non-negative values on every Pick functional, then it is a Pick function. As set of such functions is certainly closed, so is $\pickclass$. In particular:

\begin{proof}[Proof of theorem \ref{pick_convergence}]
	t.f.i.f \ref{pick_weakly_pick}.
\end{proof}

Theorem \ref{pick_weakly_pick} is kind of a result that would definitely benefit from quotient point of view: Pick functions should be defined up to a constant in the first place. One shouldn't worry too much, however: whether one thinks that weakly Pick functions (elements of $R_{+}(X)$) are really functions (elements of $\C^{X}$) or functions up to a constant (elements of $\C^{X}/\R$) doesn't make much difference.

\begin{proof}[Proof of Theorem \ref{pick_weakly_pick}]
	Well-definedness: this follows immediately from \ref{pick_functionals}.

	Injectivity: Take any $\varphi \in \C^{X}/\R$ such that $\langle \varphi, r \rangle_{L} = 0$ for any $r \in R_{+}(X)$. As $R(X) = \vspan_{\C}(R_{+}(X))$, for any $z, w \in \Hp$ we have $0 = \langle \varphi, (\lambda - z)^{-1} - (\lambda - \overline{w})^{-1}\rangle_{L} = \varphi(z) - \overline{\varphi(w)}$, which immediately implies that $\varphi$ is constant.

	Surjectivity: This is the tricky part. We should prove that for any $p^{*} \in \pickclass(\Hp)$ the map $\varphi_{p^{*}}$ is a Pick function. Note that (\ref{weakly_pick_condition_2}) implies that $\varphi_{p^{*}}$ maps upper half-plane to its closure. So ``only" analyticity remains to be checked.

	Let us first prove something slightly easier: $\varphi_{p^{*}}$ is continuous. This amounts to showing that for any $w \in \Hp$
	\begin{align*}
		\lim_{z \to w} p^{*}\left(\frac{1}{\lambda - z} - \frac{1}{\lambda - w} \right) = \lim_{z \to w}\left(\varphi_{p^{*}}(z) - \varphi_{p^{*}}(w)\right) = 0.
	\end{align*}
	The function $r_{z, w}(\lambda) = (\lambda - z)^{-1} - (\lambda - w)^{-1}$ tends to zero (in some sense) as $z \to w$, so something like this should be true: $R(\Hp)$ should have topology for which this convergence happens and the map $r \to p^{*} r$ is continuous.

	And indeed, there is one:
	\begin{align*}
		\|r\|_{R} = \sup_{x \in \R} |r(x)| (x^2 + 1).
	\end{align*}
	\begin{lem}\label{pick_norm_estimate}
		For every $p^{*} \in \pickclass(\Hp)$ there exists constant $C(p^{*})$ such that
		\begin{align*}
			\left|p^{*} (r)\right| \leq C(p^{*}) \|r\|_{R}
		\end{align*}
		for any $r \in R(\Hp)$
	\end{lem}
	\begin{proof}
		Write $r = r_{1} + i r_{2}$ where $r_{1}, r_{2} \in R_{\pm}(\Hp)$. Since clearly
		\begin{align*}
			\max(\|r_{1}\|_{R}, \|r_{2}\|_{R}) \leq \|r\| \leq \|r_{1}\|_{R} + \|r_{2}\|_{R}
		\end{align*} it suffices cook up such bound for $R_{\pm}(\Hp)$. But such functions satisfy
		\begin{align*}
			-\frac{\|r\|_{R}}{\lambda^2 + 1} \leq r(\lambda) \leq \frac{\|r\|_{R}}{\lambda^2 + 1} 
		\end{align*}
		so $|p^{*}(r)| \leq \Im(\varphi_{p^{*}}(i)) \|r\|_{R}$.
	\end{proof}
	Now in order to prove the continuity is suffices to check that $\lim_{w \to z} \|r_{z, w}\|_{R} = 0$. But this follows from
	\begin{align*}
		\|r_{z, w}\|_{R} = & |z - w| \sup_{\lambda \in \R}\left|\frac{\lambda^2 + 1}{(\lambda - z) (\lambda - w)}\right| \\
		= & |z - w| \sup_{\lambda \in \R}\left|\left(1 + \frac{z - i}{\lambda - z}\right) \left(1 + \frac{w + i}{\lambda - w}\right)\right| \\
		\leq & |z - w| \left(1 + \frac{|z - i|}{\Im(z)}\right) \left(1 + \frac{|w + i|}{\Im(w)}\right).
	\end{align*}
	Proving analyticity is not much harder. Note that if we manage to show that the order $2$ divided differences of $\varphi_{p^{*}}$ are locally bounded, Theorem \ref{bounded_div} implies the claim. Strictly speaking we only proved \ref{bounded_div} on real line, but the proof would be almost identical in the complex case. Observe that for any $z_{0}, z_{1}, z_{2} \in \Hp$ we have
	\begin{align*}
		[z_{0}, z_{1}, z_{2}]_{\varphi_{p^{*}}} = p^{*}\left(\frac{1}{(\lambda - z_{0}) (\lambda - z_{1}) (\lambda - z_{2})} \right).
	\end{align*}
	Hence, by \ref{pick_norm_estimate} we can bound the divided difference just by estimating $\|(\lambda - z_{0})^{-1} (\lambda - z_{1})^{-1} (\lambda - z_{2})^{-1}\|_{R}$. But this is straighforward: one has, for instance,
	\begin{align*}
		\left\|\frac{1}{(\lambda - z_{0}) (\lambda - z_{1}) (\lambda - z_{2})}\right\|_{R} \leq \frac{1}{\Im(z_{0})} \left(1 + \frac{|z - i|}{\Im(z)}\right) \left(1 + \frac{|w + i|}{\Im(w)}\right).
	\end{align*}
\end{proof}

It can be easily checked that the estimates in the proof can be generalized and pushed to local setting. We for instance have:
\begin{lem}\label{general_pick_norm_estimate}
	Let $X \subset \Hp$ and $p^{*} \in \pickclass(X)$. Then there exists a constant $C(p^{*})$ such that
	\begin{align*}
		\left|p^{*} (r)\right| \leq C(p^{*}) \|r\|_{R}
	\end{align*}
	for any $r \in R_{+}(X)$.
\end{lem}
\begin{prop}\label{pick_div_dif_estimate}
	Let $X \subset \Hp$ and $p^{*} \in \pickclass(X)$. Then for every compact $K \subset \Hp$ there exists a constant $C(K, p^{*})$ such that
	\begin{align*}
		\left|[z_{0}, z_{1}, \ldots, z_{k}]_{\varphi_{p^{*}}}\right| \leq \frac{C(K, p^{*})}{\Im(z_{0}) \Im(z_{1}) \cdots \Im(z_{k})}
	\end{align*}
	for any $k \geq 1$ and $z_{0}, z_{1}, \ldots, z_{k} \in X \cap K$. In particular $\varphi_{p^{*}}$ is continuous and analytic at every interior point of $X$.
\end{prop}

\section{Pick-Nevanlinna interpolation theorem}

There's an interesting generalization to Theorem \ref{pick_weakly_pick}.

\begin{lause}[Pick-Nevanlinna interpolation theorem]\label{pick_interpolation}
	Let $X \subset \Hp$. Then the restriction map $\pickclass(\Hp) = \pickclass/\R \to \pickclass(X)$ is surjection. In other words: for any $\varphi \in \pickclass(X)$ there exists a Pick function $\tilde{\varphi}$ such that $\restr{\tilde{\varphi}}{X} = \varphi$.
\end{lause}

\begin{proof}
	So we have an element in $p^{*} \in (R(X))^{*}$ such that $p^{*}(r) \geq 0$ for any $r \in R_{+}(X)$ and should prove that there exists $\tilde{p^{*}} \in R(\Hp)^{*}$ such that $\restr{\tilde{p^{*}}}{R(X)} = p^{*}$ and $\tilde{p^{*}}(r) \geq 0$ for any $r \in R_{+}(\Hp)$: we should extend a linear functional preverving its positivity.

	Such extension is not unique in general. Nevertheless, if it so happens that $R(X)$ is dense in $R(\Hp)$ with respect to $\|\cdot\|_{L}$, then, as $p^{*}$ is Lipschitz by \ref{general_pick_norm_estimate}, $\tilde{p^{*}}$ is unique. In this case $R_{+}(X)$ is dense in $R_{+}(\Hp)$ so the extension is really a Pick function. This already covers large variety of sets $X$.
	\begin{lem}\label{pick_dual_dense}
		If $X \subset \Hp$ has an accumulation point in $\Hp$, then $R(X)$ is dense in $R(\Hp)$.
	\end{lem}
	\begin{proof}
		Take a sequence of distinct points $z_{0}, z_{1}, \ldots$ in $X$ converging to $z_{\infty} \in \Hp$. We may assume w.l.o.g. that $z_{\infty} = i$. By Newton expansion, for any $w \in \Hp$
		\begin{align*}
			\frac{1}{\lambda - w} - \sum_{i = 0}^{n} \frac{(w - z_{0}) \cdots (w - z_{i - 1})}{(\lambda - z_{0}) \cdots (\lambda - z_{i - 1}) (\lambda - z_{i})} = \frac{(w - z_{0}) \cdots (w - z_{n})}{(\lambda - w)(\lambda - z_{0}) \cdots (\lambda - z_{n})}.
		\end{align*}
		If $|w - i| < 1$, norm of the error term tends to zero. It follows that $R(X)$ is dense in $R(X \cup \D(i, 1))$. But as $2 i$ is an accumulation point of $X \cup \D(i, 1)$, we may repeat the previous argument: $R(X)$ is dense in $R(X \cup \D(2 i, 2))$. Bootstrapping along the sequence $(2^{n} i)_{n = 1}^{\infty}$ yields the claim.
	\end{proof}
	For the general case one has to work slighty harder. Observe first that one only needs to prove the following lemma.
	\begin{lem}\label{pick_one_point_extension}
		For any $X \in \Hp$ and $w \in \Hp \setminus X$ the restriction map $\pickclass(X \cup \{w\}) \to \pickclass(X)$ is surjective.
	\end{lem}
	Given this lemma one can, for instance, use transfinite induction (or Zorn's lemma if one so prefers) to extend any element of $\pickclass(X)$ to $\pickclass(\Hp)$. Or one may inductively extend $X$ by a countable set with an accumulation point, after which there exists a unique extension by the already proven dense case.

	\begin{proof}[Proof of Lemma \ref{pick_one_point_extension}]
		Fix $z_{0} \in X$. Note that any $r \in R_{+}(X \cup \{w\})$ is of the form
		\begin{align*}
		\Re\left(a \left(\frac{1}{\lambda - w} - \frac{1}{\lambda - z_{0}}\right)\right) + s,
		\end{align*}
		where $a \in \C$ and $s \in R(X)$. Writing $r_{w}(\lambda) = (\lambda - w)^{-1} - (\lambda - z_{0})^{-1}$, the extension should satisfy
		\begin{align}\label{pick_extension_ineq}
			\Re\left(a p^{*}(r_{w})\right) + p^{*}(s) \geq 0
		\end{align}
		for some family of $\mathcal{F}$ pairs $(a, s) \in \C \times R(X)$. Note that if we find a value $p^{*}(r_{w})$ which satisfies all such inequalities, it determines an extension for $p^{*}$ to $\pickclass(X \cup \{w\})$.

		Each of the inequalities of the form \ref{pick_extension_ineq} (where $a \neq 0$) force $p^{*}(r_{w})$ in some closed half-space of $\C$. It is easy to check that for any $a \in \C$ there exist $(a, s) \in \mathcal{F}$: $p^{*}(r_{w})$ is constrained by half-spaces of all directions. This implies that set of suitable $p^{*}(r_{w})$'s can be expressed as intersection of compact sets, certain finite intersections of half-spaces in $\mathcal{F}$. We hence just have to verify that all such finite intersections are non-empty.

		This, finally, follows almost immediately from the Farkas' lemma.
		\begin{lem}[Farkas' lemma]\label{farkas_lemma}
			Let $m \geq 1$, $a_{1}, a_{2}, \ldots, a_{m} \in \R^{n}$ and $c_{1}, c_{2}, \ldots, c_{m} \in \R$. Assume that whenever $t_{1}, t_{2}, \ldots, t_{m} \geq 0$ are such that
			\begin{align*}
				t_{1} a_{1} + t_{2} a_{2} + \ldots + t_{m} a_{m} = 0,
			\end{align*}
			then also
			\begin{align*}
				t_{1} c_{1} + t_{2} c_{2} + \ldots + t_{m} c_{m} \geq 0.
			\end{align*}
			Then there exists $b \in \R^{n}$ such that $a_{i} \cdot b + c_{i} \geq 0$ for any $1 \leq i \leq m$.
		\end{lem}
		Indeed: the inequalities in \ref{pick_extension_ineq} can be interpreted in $\R^{2}$, in the sense of Farkas' lemma. Let's then check the condition. Pick any finite set of pairs $(a_{1}, s_{1}), (a_{2}, s_{2}), \ldots, (a_{m}, s_{m}) \in \mathcal{F}$. Take also $t_{1}, t_{2}, \ldots, t_{m} \geq 0$ with $\sum_{i = 1}^{m} t_{i} a_{i} = 0$. Now
		\begin{align*}
			0 \leq \sum_{i = 1}^{m} t_{i} \left(\Re(a_{i} r_{w}) + s_{i}\right) = \Re\left(\sum_{i = 1}^{m} t_{i} a_{i} r_{w}\right) + \sum_{i = 1}^{m} t_{i} s_{i} = \sum_{i = 1}^{m} t_{i} s_{i}
		\end{align*}
		and hence $\sum_{i = 1}^{m} t_{i} p^{*}(s_{i}) \geq 0$. By Farkas' lemma we can find $p^{*}(r_{w}) \in \C$ such that
		\begin{align*}
			\Re(a_{i} p^{*}(r_{w})) + p^{*}(s_{i}) \geq 0
		\end{align*}
		for any $1 \leq i \leq m$. This is just what we wanted.
	\end{proof}
\end{proof}

For completeness, we also prove Farkas' lemma.

\begin{proof}[Proof of Lemma \ref{farkas_lemma}]
	Proof is by induction on $n$.

	The case $n = 1$: We may clearly ignore all the zeros from the $a_{i}$'s. If all $a_{i}$'s are positive or negative, claim is trivial. Assume then that there are both positive and negative $a_{i}$'s. We may scale $a_{i}$'s (with $c_{i}$'s) such that they are all $1$ or $-1$: w.l.o.g. first $k$ $a_{i}$'s are ones. We should prove that one may choose $b$ such that $b + c_{i} \geq 0$ for any $1 \leq i \leq k$ and $-b + c_{j} \geq 0$ for any $k + 1 \leq j \leq m$. This is clearly possible if $c_{i} + c_{j} \geq 0$ for any $1 \leq i \leq k < j \leq m$. But if one sets $t_{i} = 1$ and $t_{j} = 1$ and other $t_{l}$'s to zero in the main condition, this is exactly the inequality one gets.

	Induction step: We split into two cases.
	\begin{itemize}
		\item Case 1: There exists $t_{1}, t_{2}, \ldots, t_{m} \geq 0$, not all zero, such that $\sum_{i = 1}^{m} t_{i} a_{i} = 0$.
		\item Case 2: The complement of case 1.
	\end{itemize}
	Let us also denote
	\begin{align*}
		C = \cone \{a_{i} | 1 \leq i \leq m\}.
	\end{align*}
	Case 1: it follows that there exist nonzero $v \in \R^{n}$ such that $v, -v \in C$. Decompose $\R^{n} = \vspan(v) \oplus W$, $a_{i} = a_{i}^{(v)} + a_{i}^{W}$ and $b = b^{(v)} + b^{W}$. The idea is to interpret the problem of finding $b$ as parametrized problem of finding $b^{W}$. We should find $b^{(v)}$ such that the following condition holds: whenever $t_{1}, t_{2}, \ldots, t_{m}$ are such that
	\begin{align*}
		\sum_{i = 1}^{m} t_{i} a_{i}^{W} = 0
	\end{align*}
	then
	\begin{align*}
		0 \leq \sum_{i = 1}^{m} t_{i} (c_{i} + a_{i}^{(v)} b^{(v)}) = \sum_{i = 1}^{m} t_{i} c_{i} + b^{(v)}\sum_{i = 1}^{m} t_{i} a_{i}^{(v)}.
	\end{align*}
	Then by $n - 1$ dimensional case there exists $b^{W}$ such that
	\begin{align*}
		0 \leq c_{i} + a_{i}^{(v)} b^{(v)} + a_{i}^{W} \cdot b^{W} = a_{i} \cdot b + c_{i}.
	\end{align*}
	This can be interpreted as one-dimensional problem where $a_{i}$'s are sums $\sum_{i = 1}^{m} t_{i} a_{i}^{(v)}$ and $c_{i}$'s sums $\sum_{i = 1}^{m} t_{i} c_{i}$ where $t_{i}$'s range over all tuples of non-negative numbers with $\sum_{i = 1}^{m} t_{i} a_{i}^{W} = 0$. But now there are infinitely many conditions so we can't immediately use the case $n = 1$. However, by the extra assumption $v, -v \in C$, so one can find $t_{i}$'s such that $\sum_{i = 1}^{m} t_{i} a_{i}^{W} = 0$ and $\sum_{i = 1}^{m} t_{i} a_{i}^{(v)}$ is both positive and negative. This means that it suffices to check that the condition holds for all pairs of such tuples. So take $s_{1}, s_{2} \geq 0$ and numbers $t_{i, 1}, t_{i, 2} \geq 0$ for $1 \leq i \leq m$ such that $\sum_{i = 1}^{m} t_{i, 1} a_{i}^{W} = \sum_{i = 1}^{m} t_{i, 2} a_{i}^{W} = 0$ and
	\begin{align*}
		s_{1} \sum_{i = 1}^{m} t_{i, 1} a_{i}^{(v)} + s_{2} \sum_{i = 1}^{m} t_{i, 2} a_{i}^{(v)} = 0.
	\end{align*}
	Then $\sum_{i = 1}^{m} \left(s_{1} t_{i, 1} + s_{2} t_{i, 2} \right) a_{i} = 0$ so by the main condition
	\begin{align*}
		0 \leq \sum_{i = 1}^{m} \left(s_{1} t_{i, 1} + s_{2} t_{i, 2} \right) c_{i} = s_{1} \sum_{i = 1}^{m} t_{i, 1} c_{i} + s_{2}\sum_{i = 1}^{m} t_{i, 2} c_{i},
	\end{align*}
	as desired.

	Case 2: We first claim that there exists $v \in \R^{n}$ such that $v, -v \notin C$. Assuming otherwise, take $1 \leq j < l \leq m$ (the case $m = 1$ is clear as $n > 1$). Now as $a_{j} - a_{l} \in C$ or $a_{j} - a_{l} \in C$, w.l.o.g. $a_{j} = a_{l} + \sum_{i = 1}^{m} t_{i} a_{i}$ for some $t_{1}, \ldots, t_{m} \geq 0$. If $t_{j} \geq 1$, we have found non-trivial decomposition for $0$, a contradiction. If on the other hand $t_{j} < 1$, we see that $a_{j} \in \cone\{a_{i} | i \neq j\}$, so we may forget $a_{j}$. Inducting on $m$ finishes the claim.

	Again, decompose $\R^{n} = \vspan(v) \oplus W$, $a_{i} = a_{i}^{(v)} + a_{i}^{W}$ and $b = b^{(v)} + b^{W}$. We prove that we may set $b^{(v)} = 0$ and also the reduced problem falls in the case 2. Indeed, if $t_{1}, t_{2}, \ldots, t_{m} \geq 0$ are such that $\sum_{i = 1}^{m} t_{i} a_{i}^{W} = 0$ then $\sum_{i = 1}^{m} t_{i} a_{i}^{(v)} \in C \cap \vspan(v) \subset \{0\}$. Consequently $\sum_{i = 1}^{m} t_{i} a_{i} = 0$ and hence $t_{i} = 0$ for any $1 \leq i \leq m$.
\end{proof}

\begin{huom}
	Pick Nevanlinna interpolation can be extended in many ways. One such extension concerns Taylor sections.
	\begin{maar}
		Let $d : \Hp \to \N \cup \{\infty\}$. Extend $d$ to $\C$ with $d(z) = d(\overline{z})$ for any $z \in \Hp$ and $d(\lambda) = 0$ for any $\lambda \in \R$. We denote by $R(d)$ the set of rational functions $r$ with the properties
		\begin{itemize}
			\item Pole of $r$ at $z \in \C$ (if any) has order at most $d(z)$.
			\item $r(\lambda) = O(|\lambda|^{-2})$ at infinity.
		\end{itemize}
	\end{maar}
	Note that $R(X) = R(\chi_{X})$.
	\begin{lause}\label{pick_taylor_sections}
		Let $d : \Hp \to \N \cup \{\infty\}$. Then the natural map $\pickclass/\R \to \pickclass(d) =: (R(d))^{*}$ is surjection.
	\end{lause}
	What does this result has to do with Taylor sections? Fix any $d : \Hp \to \N \cup \{\infty\}$. Now Theorem \ref{pick_taylor_sections} is just saying that for any $p^{*} \in \pickclass(d)$, there exists a Pick function $\varphi_{p^{*}}$ for which
	\begin{align*}
		\frac{\varphi_{p^{*}}^{(m)}(z)}{m!} = p^{*}\left(\frac{1}{(\lambda - z)^{m + 1}}\right)
	\end{align*}
	for any $z \in \Hp$ and $1 \leq m \leq d(z) - 1$.

	Theorem \ref{pick_taylor_sections} could be proved with pretty much same ideas as Theorem \ref{pick_interpolation}; we're not going to do that, however.
\end{huom}

\section{Notes and references}

Pick functions and representation theorem \ref{pick_nevanlinna_herglotz_representation_theorem} are discussed in numerous sources; see for example (?). Pick-Nevanlinna interpolation theorem \ref{pick_interpolation} was first observed and proved independently by Pick \cite{Pick} and Nevanlinna \cite{Nevan}. Since then, many different approaches exist, see (?) for a survey. Approach taken in this text is similar to (?).
\begin{comment}

TODO:
\begin{itemize}
	\item Examples of representing measures behind functions and functions behind representing measures
	\item Spectral commutant lifting theorem
	\item Use Morera's theorem to prove weak Hindmarsh's theorem
	\item Nice formula for finite Pick extension (rational function case)
\end{itemize}

\end{comment}






